{
 "metadata": {
  "name": "",
  "signature": "sha256:e176cfa99efdb9ab47564f618a7dcb118be0e773f580fcfe528ad0e1beb59a45"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Downloader"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To speed up the process we will use an IPython cluster already setup. For future reference, and even knowing that there are better ways to do it, I will leave here the setup process."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Mini Guide to IPython Parallel"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "IPython Parallalel is able to run in a single machine to maximize the use of the cores of the processor. If that's the case, the only thing you need is `ipcluster`, but what we want to do is to use 4 cores in 2 different machines. Those machines will be called the *engines*, and the one that connects to them, the *controller*.\n",
      "\n",
      "1. In the controller, we first create a profile named, let's say `cluster`\n",
      "  ```\n",
      "  $ ipython profile create --parallel --profile=cluster\n",
      "  ```\n",
      "  \n",
      "2. The step is to create or edit a file named `ipcontroller_config.py`, where `c.HubFactory.ip` is the host to listen to, in our case all the interfaces, and `c.HubFactory.location` the external IP of the controller.\n",
      "  ```python\n",
      "  # ipcontroller_config.py\n",
      "  c.HubFactory.ip = '*'\n",
      "  c.HubFactory.location = 'controller.host'\n",
      "  ```\n",
      "  \n",
      "3. Then, in each engine, we copy the file `~/.ipython/profile_cluster/security/ipcontroller-engine.json`.\n",
      "  ```\n",
      "  $ scp username@controller.host:/home/username/.ipython/profile_cluster/security/ipcontroller-engine.json ./\n",
      "  ```\n",
      "  \n",
      "4. In each engine, we just run `ipengine` with the file we just downloaded from the controller.\n",
      "  ```\n",
      "  $ ipengine --file=./ipcontroller-engine.json\n",
      "  ```\n",
      "  \n",
      "  So far, the easiest way to take advantage of all the cores in a single engine, is by running `ipengine` as many times as cores the machine has.\n",
      "5. Finally, from the controller, we execute an IPython Notebook with password and using the profile.\n",
      "  ```\n",
      "  $ ipython notebook --profile=cluster --no-browser --pprint --NotebookApp.password=`python -c \"from IPython.lib import passwd; print passwd()\"\n",
      "  \n",
      "  ```\n",
      "  We can add `--ip=0.0.0.0` for the Notebook to listen in all the interfaces.\n",
      "  \n",
      "Once everything is working, we just need an `IPython.parallel.Client` instance in our Notebook."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.parallel import Client"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Accessing the `ids` of the engines."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clients = Client()\n",
      "clients.ids"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "[0, 1, 2, 3, 4, 5, 6, 7]"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To execute functions in parallel, we access to a `View` and then call `map_sync` or `map_async`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "view = Client()[:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "view.map_sync(lambda x: 5**x, range(10))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "[1, 5, 25, 125, 625, 3125, 15625, 78125, 390625, 1953125]"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can execute some commands in all the engines using the cell and line magic `%px`,"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%px import socket; print(socket.gethostname())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[stdout:0] cultureplex-desktop\n",
        "[stdout:1] cultureplex-desktop\n",
        "[stdout:2] cultureplex-desktop\n",
        "[stdout:3] cultureplex-desktop\n",
        "[stdout:4] natalia-desktop\n",
        "[stdout:5] natalia-desktop\n",
        "[stdout:6] natalia-desktop\n",
        "[stdout:7] natalia-desktop\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Tested that everything works as expected, we can now run our actual code."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Accessing NYT API"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We first need API keys and list of tags to search for."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# One API key for each of the cores\n",
      "api_keys = [\n",
      "    \"3439a9084efa80c4f5fb1d290dfc1b44:11:70233981\",\n",
      "    \"a5c709f3168b829711241b243457e9d6:13:70235641\",\n",
      "    \"ba47374fd391c9bc5fd3ca51ff953a44:14:70229228\",\n",
      "    \"4557e02788189abb3642a33bca7469ff:11:69136863\",\n",
      "    \"87d7b22c0feec4f3112d80b71d0b500a:1:69642501\",\n",
      "    \"2b3d39fd4c7836168a2a370c25ad6232:16:70235576\",\n",
      "    \"d7655429355ab2df4621a10c01d04865:8:69135199\",\n",
      "#     \"730e30f5220059551e666430644fbf87:11:69642501\",  # Inactive\n",
      "    \"1944df13b86dd83e4a8c4ea82e767975:2:65092848\",\n",
      "]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "search_terms = open(\"search_terms.txt\", \"r\").read()[:-1].split(\"\\n\")  # last line is blank space"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We split the search terms in as many parts as cores we have, and create a dictionary with keys the API keys, and values a list of the terms to search using that key."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "terms = {}\n",
      "count = len(search_terms) / len(api_keys)\n",
      "for i, api_key in enumerate(api_keys):\n",
      "    terms[api_key] = search_terms[i * count: (i + 1) * count]\n",
      "# We add the remainder if any\n",
      "terms[api_key] += search_terms[(i + 1) * count:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's define the main `downloader` function and an imports for all the engines."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%px --local\n",
      "\n",
      "import io\n",
      "import json\n",
      "import os\n",
      "import sys\n",
      "import time\n",
      "import urllib\n",
      "import urllib2\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "from dateutil import parser\n",
      "from pandas.io.json import json_normalize\n",
      "from time import sleep"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def downloader(args):\n",
      "    api_key, search_terms = args\n",
      "    begin_date = '20130101'\n",
      "    end_date = '20131231'\n",
      "    entries_url = 'http://api.nytimes.com/svc/search/v2/articlesearch.json?q={0}&sort=oldest&begin_date={1}&end_date={2}&api-key={3}&page={4}'\n",
      "    data_path = 'data/files_new_york_times/json_original/'\n",
      "    not_companies = [\"entrepreneur\", \"startup\", \"new venture\", \"manager\", \"executive\", \"founder\"]\n",
      "    tags_rows = []\n",
      "    for tag in search_terms:\n",
      "        print(\"Processing '{0}'.\".format(tag)),\n",
      "        # Initialization variables     \n",
      "        perform_requests = True\n",
      "        first_request = True\n",
      "        iteration = 0\n",
      "        page = 0\n",
      "        offset = 0\n",
      "        hits = 0\n",
      "        begin_date_aux = begin_date\n",
      "        # We prepare the necessary paths\n",
      "        full_data_path = data_path + tag\n",
      "        # print full_data_path\n",
      "        if not os.path.exists(full_data_path):\n",
      "            os.makedirs(full_data_path)\n",
      "        # And now, we start with the requests\n",
      "        while perform_requests:\n",
      "            if \" & \" in tag:\n",
      "                tag = '\" OR \"'.join([tag, tag.replace(\" & \", \" and \")])\n",
      "            tag = '\"{}\"'.format(tag)\n",
      "            q_tag = urllib.quote_plus(tag)\n",
      "            request_url = entries_url.format(q_tag, begin_date_aux, end_date, api_key, page)\n",
      "            try:\n",
      "                response = urllib2.urlopen(request_url)\n",
      "                retries = 5\n",
      "            except Exception as e:\n",
      "                retries -= 1\n",
      "                print(request_url)\n",
      "                print(e)\n",
      "                if retries < 0:\n",
      "                    raise e\n",
      "                else:\n",
      "                    sleep(1)\n",
      "                    continue\n",
      "            sleep(1)  # Max. 10 request per second\n",
      "            # Right now, we load the reponse in the data variable. This variable contains the JSON result.\n",
      "            data = json.load(response)\n",
      "            if (len(data['response']['docs']) == 0):\n",
      "                perform_requests = False\n",
      "                print('Done.')\n",
      "                break\n",
      "            elif (data['status'] != 'OK'):\n",
      "                perform_requests = False\n",
      "                print('Finishing with errors, printing the response')\n",
      "                print('*' * 44)\n",
      "                print(response)\n",
      "                print('*' * 44)\n",
      "                break\n",
      "            if first_request:\n",
      "                hits = data['response']['meta']['hits']\n",
      "                first_request = False\n",
      "                # print 'Total hits for ' + tag + ': ' +  str(hits)\n",
      "            # We need to control the page, because the pagination ends when reach to 100.\n",
      "            # So, we need to change the begin date to start a new pagination from 0.\n",
      "            page += 1\n",
      "            if page > 100:\n",
      "                last_index = len(data['response']['docs']) - 1\n",
      "                last_date = parser.parse(data['response']['docs'][last_index]['pub_date'])\n",
      "                begin_date_aux = str(last_date.year) + '%02d' % last_date.month + '%02d' % last_date.day\n",
      "                page = 0\n",
      "                iteration += 1\n",
      "            if page % 10 == 0:\n",
      "                print('.'),\n",
      "            # We save the content in the JSON file.\n",
      "            filename = tag + '_' + str((iteration * 100) + page + iteration) + '.json'\n",
      "            # print '\\tSaving file', filename\n",
      "            with open(full_data_path + '/' + filename, 'w') as outfile:            \n",
      "               json.dump(data, outfile)\n",
      "            # We append the data to the total_data DataFrame\n",
      "            docs = []\n",
      "            for doc in data['response']['docs']:\n",
      "                doc.update(data['response']['meta'])\n",
      "                doc.update({\n",
      "                    \"search_term\": tag,\n",
      "                    \"is_company\": tag in not_companies,\n",
      "                })\n",
      "                docs.append(doc)\n",
      "            tags_rows += docs\n",
      "    return pd.DataFrame(tags_rows)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For testing purposes we can always run `downloader` with one of the elements of our `terms` dictionary."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = (terms.items()[0][0], terms.items()[0][1][:2])  # first API key, first 2 search terms\n",
      "# test = (terms.items()[0][0], [\"Rivet & Sway\"])  # first API key, example search term\n",
      "dw = downloader(test)\n",
      "dw.count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processing 'Duda'. "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done.\n",
        "Processing 'Electric Cloud'. "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done.\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "_id                 10\n",
        "abstract             6\n",
        "blog                10\n",
        "byline              10\n",
        "document_type       10\n",
        "headline            10\n",
        "hits                10\n",
        "is_company          10\n",
        "keywords            10\n",
        "lead_paragraph       8\n",
        "multimedia          10\n",
        "news_desk            4\n",
        "offset              10\n",
        "print_page           4\n",
        "pub_date            10\n",
        "search_term         10\n",
        "section_name        10\n",
        "snippet             10\n",
        "source              10\n",
        "subsection_name      4\n",
        "time                10\n",
        "type_of_material    10\n",
        "web_url             10\n",
        "word_count          10\n",
        "dtype: int64"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, when the code is ready, we just run the code in parallel using all the engines. But first, we prepare a function to monitor `stdout` of the engines' processes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import clear_output\n",
      "\n",
      "def wait_watching_stdout(ar, dt=1):\n",
      "    while not ar.ready():\n",
      "        clear_output()\n",
      "        print(\"\".join(ar.stdout) + '\\n')\n",
      "        sys.stdout.flush()\n",
      "        time.sleep(dt)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "result = view.map_async(downloader, terms.items())  # terms.items()\n",
      "wait_watching_stdout(result, dt=60)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processing 'Duda'. Done.\n",
        "Processing 'Electric Cloud'. Done.\n",
        "Processing 'Flybits'. Done.\n",
        "Processing 'Fruition Partners'. Done.\n",
        "Processing 'GENBAND'. Done.\n",
        "Processing 'globalVCard'. Done.\n",
        "Processing 'CSI Enterprise'. Done.\n",
        "Processing 'GuideSpark'. Done.\n",
        "Processing 'HasOffers'. Done.\n",
        "Processing 'Health Catalyst'. Done.\n",
        "Processing 'Hortonworks'. Done.\n",
        "Processing 'iBuildApp'. Done.\n",
        "Processing 'iCIMS Inc'. Done.\n",
        "Processing 'ImaginAb Inc'. Done.\n",
        "Processing 'iPipeline'. Done.\n",
        "Processing 'JAB Broadband'. Done.\n",
        "Processing 'Kahuna Inc'. Done.\n",
        "Processing 'Kareo'. Done.\n",
        "Processing 'Klocwork Inc.'. Done.\n",
        "Processing 'Rogue Wave'. Done.\n",
        "Processing 'Lancope'. Done.\n",
        "Processing 'Load DynamiX'. Done.\n",
        "Processing 'LocalResponse'. Done.\n",
        "Processing 'Looker'. Done.\n",
        "Processing 'Malcovery Security'. Done.\n",
        "Processing 'Maxta Inc'. Done.\n",
        "Processing 'MicroGREEN Polymers Inc'. Done.\n",
        "Processing 'Modernizing Medicine Inc'. Done.\n",
        "Processing 'myoscience'. Done.\n",
        "Processing 'NearWoo'. Done.\n",
        "Processing 'Nomis Solutions Inc'. Done.\n",
        "Processing 'OxiCool Inc'. Done.\n",
        "Processing 'Packsize LLC'. Done.\n",
        "Processing 'Panaya'. Done.\n",
        "Processing 'Paxata'. Done.\n",
        "Processing 'Perfecto Mobile'. Done.\n",
        "Processing 'Perseus Telecom'. Done.\n",
        "Processing 'Phunware'. Done.\n",
        "Processing 'Pluribus Networks'. Done.\n",
        "Processing 'PowerPlan Inc'. Done.\n",
        "Processing 'Quanterix'. Done.\n",
        "Processing 'RealMatch'. Done.\n",
        "Processing 'Red Lambda Inc'. Done.\n",
        "Processing 'Rise Interactive'. Done.\n",
        "Processing 'RootMetrics'. Done.\n",
        "Processing 'RxWiki Inc'. Done.\n",
        "Processing 'Scaled Agile Inc'. Done.\n",
        "Processing 'Scribe Software'. Done.\n",
        "Processing 'Seamless Medical Systems Inc'. Done.\n",
        "Processing 'SecureAuth'. Done.\n",
        "Processing 'SeeChange Health'. Done.\n",
        "Processing 'Shape Security'. Done.\n",
        "Processing 'microsoft'. Done.\n",
        "Processing 'Google'. Done.\n",
        "Processing 'Salesforce'. Done.\n",
        "Processing 'SAS'. Done.\n",
        "Processing 'Quicken'. Done.\n",
        "Processing 'Intuit'. Done.\n",
        "Processing 'las vegas sands'. Done.\n",
        "Processing 'autonation'. Done.\n",
        "Processing '3m company'. Done.\n",
        "Processing 'danaher corporation'. Done.\n",
        "Processing 'new york life insurance company'. Done.\n",
        "Processing 'mcdonalds corporation'. Done.\n",
        "Processing 'caterpillar'. Done.\n",
        "Processing 'ryder systems'. Done.\n",
        "Processing 'd.r. horton'. Done.\n",
        "Processing 'stanley black & decker'. Done.\n",
        "Processing 'archer daniels midland'. Done.\n",
        "Processing 'coca-cola company'. Done.\n",
        "Processing 'united continental holdings'. Done.\n",
        "Processing 'ingram micro'. Done.\n",
        "Processing 'chs'. Done.\n",
        "Processing 'international paper company'. Done.\n",
        "Processing 'mckesson'. Done.\n",
        "Processing 'phillip morris international'. Done.\n",
        "Processing 'state farm insurance'. Done.\n",
        "Processing 'owens corning'. Done.\n",
        "Processing 'waste management'. Done.\n",
        "Processing 'c.h. robinson worldwide'. Done.\n",
        "Processing 'harley-davidson'. Done.\n",
        "Processing 'manpower group'. Done.\n",
        "Processing 'union pacific'. Done.\n",
        "Processing 'r.r donnelley & sons company'. Done.\n",
        "Processing 'graham holdings company'. Done.\n",
        "Processing 'emc corporation'. Done.\n",
        "Processing 'mattel'. Done.\n",
        "Processing 'united parcel service'. Done.\n",
        "Processing 'omnicom group'. Done.\n",
        "Processing 'mckinsey & company'. Done.\n",
        "Processing 'pricewaterhousecoopers'. Done.\n",
        "Processing 'intuitive research and tech'. Done.\n",
        "Processing 'international bussiness machines'. Done.\n",
        "Processing 'formerly BrightTag'. Done.\n",
        "Processing 'aka Wright Foods'. Done.\n",
        "Processing 'turospree.com'. Done.\n",
        "Processing 'nirvanix.com'. Done.\n",
        "Processing 'getpostrocket.com'. Done.\n",
        "Processing 'votertide.com'. Done.\n",
        "Processing 'gamelayers.com'. Done.\n",
        "Processing 'unifyo.com'. Done.\n",
        "Processing 'lookery.com'. Done.\n",
        "Processing 'canv.as'. Done.\n",
        "Processing 'blurtt.com'. Done.\n",
        "Processing 'manilla.com'. Done.\n",
        "Processing 'admaze.ly.com'. Done.\n",
        "Processing 'springpad.com'. Done.\n",
        "Processing 'ex.fm'. Done.\n",
        "Processing 'sambamobile.com'. Done.\n",
        "Processing 'qik.com'. Done.\n",
        "Processing 'inbloom.org'. Done.\n",
        "Processing 'intonow.com'. Done.\n",
        "Processing 'getfindit.com'. Done.\n",
        "Processing 'inqmobile.com'. Done.\n",
        "Processing 'outboxmail.com'. Done.\n",
        "Processing 'argylesocial.com'. Done.\n",
        "Processing 'iamexec.com'. Done.\n",
        "Processing 'stippleit.com'. Done.\n",
        "Processing '500px Inc'. Done.\n",
        "Processing 'Wajam'. Done.\n",
        "Processing 'Keek Inc'. Done.\n",
        "Processing 'GuySpy'. Done.\n",
        "Processing 'Woozworld'. Done.\n",
        "Processing 'TinEye'. Done.\n",
        "Processing 'Idee Inc'. Done.\n",
        "Processing 'Jigsy'. Done.\n",
        "Processing 'Voices.com'. Done.\n",
        "Processing 'Snapsort'. Done.\n",
        "Processing 'EmpireAvenue'. Done.\n",
        "Processing 'Frank and Oak'. Done.\n",
        "Processing 'LoginRadius'. Done.\n",
        "Processing 'Chango Inc'. Done.\n",
        "Processing 'Pure Energies Group Inc'. Done.\n",
        "Processing 'Buytopia.ca'. Done.\n",
        "Processing 'Smoke NV Inc'. Done.\n",
        "Processing 'Just Quality International Inc'. Done.\n",
        "Processing 'Sterling Cross Defense Systems Corp'. Done.\n",
        "Processing 'Magnet Forensics Inc'. Done.\n",
        "Processing 'Greenlife Water Corp'. Done.\n",
        "Processing 'Secure Sense Solutions'. Done.\n",
        "Processing 'Sandstorm Gold Ltd'. Done.\n",
        "Processing 'ScribbleLive Technologies Inc'. Done.\n",
        "Processing 'TDot Performance Ltd'. Done.\n",
        "Processing 'Redwood Strategic Inc'. Done.\n",
        "Processing 'Emcision International Inc'. Done.\n",
        "Processing 'Henkaa Inc'. Done.\n",
        "Processing 'Whirlscape Inc'. Done.\n",
        "Processing 'Bionym'. Done.\n",
        "Processing 'Thalmic Labs'. Done.\n",
        "Processing 'Recon Instruments'. Done.\n",
        "Processing 'Om Malik Technology'. Done.\n",
        "Processing 'Affino'. Done.\n",
        "Processing 'AnalyzeRE'. Done.\n",
        "Processing 'Atomic Reach'. Done.\n",
        "Processing 'Cloud Dynamics'. Done.\n",
        "Processing 'Exo U'. Done.\n",
        "Processing 'PackIt'. Done.\n",
        "Processing 'American Retirement Advisors'. Done.\n",
        "Processing 'Highpoint Global AdKarma'. Done.\n",
        "Processing 'Axtria'. Done.\n",
        "Processing 'Dolls Kill'. Done.\n",
        "Processing 'Centric Digital'. Done.\n",
        "Processing 'Retail Capital'. Done.\n",
        "Processing 'Restore Health'. Done.\n",
        "Processing 'OpenRoad Lending'. Done.\n",
        "Processing 'Pontchartrain Partners'. Done.\n",
        "Processing 'Team Extreme Marketing International'. Done.\n",
        "Processing 'Choice Energy'. Done.\n",
        "Processing 'ReviMedia'. Done.\n",
        "Processing 'SmartZip Analytics'. Done.\n",
        "Processing 'Thompson Gray'. Done.\n",
        "Processing 'MCSG Technologies'. Done.\n",
        "Processing 'Innovative Surveillance Solutions'. Done.\n",
        "Processing 'Nordic'. Done.\n",
        "Processing 'Michigan Realty Solutions'. Done.\n",
        "Processing 'Ohio Investments'. Done.\n",
        "Processing 'Adore Me'. Done.\n",
        "Processing 'IQ Formulations'. Done.\n",
        "Processing 'Alcohol by Volume'. Done.\n",
        "Processing 'H.Bloom'. Done.\n",
        "Processing 'Tealium'. Done.\n",
        "Processing 'Boostability'. Done.\n",
        "Processing 'Solve Media'. Done.\n",
        "Processing 'Heartland Energy Partners'. Done.\n",
        "Processing 'Bridger'. Done.\n",
        "Processing 'Jeunesse Global'. Done.\n",
        "Processing 'Touchsuite'. Done.\n",
        "Processing 'Accleration Partners'. Done.\n",
        "Processing 'Moore Heating & Air Conditioning'. Done.\n",
        "Processing 'Optimatic Media'. Done.\n",
        "Processing 'EPI Engineering'. Done.\n",
        "Processing 'Simplicity Laser'. Done.\n",
        "Processing 'Cinium Financial Services'. Done.\n",
        "Processing 'TNH Advanced Specialty Pharmacy'. Done.\n",
        "Processing 'Human Movement Management'. Done.\n",
        "Processing 'Allegheny Science & Technology'. Done.\n",
        "Processing 'Hudl'. Done.\n",
        "Processing 'GuideSpark'. Done.\n",
        "Processing 'Optimus Technology'. Done.\n",
        "Processing 'Luxe Royale'. Done.\n",
        "Processing 'SoluComp'. Done.\n",
        "Processing 'RPM Freight Systems'. Done.\n",
        "Processing 'Motivational Press'. Done.\n",
        "Processing '99dresses'. Done.\n",
        "Processing 'Dinnr'. Done.\n",
        "Processing 'Seesmic Video'. Done.\n",
        "Processing 'Treehouse Logic'. Done.\n",
        "Processing 'Backchat'. Done.\n",
        "Processing 'Fabric Engine'. Done.\n",
        "Processing 'Finmaven'. Done.\n",
        "Processing 'Granify'. Done.\n",
        "Processing 'InfraDog'. Done.\n",
        "Processing 'Introhive'. Done.\n",
        "Processing 'Invici Technologies'. Done.\n",
        "Processing 'Login Radius'. Done.\n",
        "Processing 'mnubo'. Done.\n",
        "Processing 'Obero Solutions'. Done.\n",
        "Processing 'PeopleInsight'. Done.\n",
        "Processing 'Procima Experts'. Done.\n",
        "Processing 'reelyActive'. Done.\n",
        "Processing 'RtTech Software'. Done.\n",
        "Processing 'Seamless Mobile Health'. Done.\n",
        "Processing 'Slyce'. Done.\n",
        "Processing 'Smart Employee Benefits'. Done.\n",
        "Processing 'SweetIQ'. Done.\n",
        "Processing 'Thalmic Labs'. Done.\n",
        "Processing 'Vena Solutions'. Done.\n",
        "Processing 'WestonExprerssions'. Done.\n",
        "Processing 'Linkett'. Done.\n",
        "Processing '360pi'. Done.\n",
        "Processing 'AcademixDirect Inc'. Done.\n",
        "Processing 'Adconion Direct'. Done.\n",
        "Processing 'Adesto Technologies Corporation'. Done.\n",
        "Processing 'Alfresco'. Done.\n",
        "Processing 'Aitierre Corporation'. Done.\n",
        "Processing 'Altitude Digital'. Done.\n",
        "Processing 'Anametrix'. Done.\n",
        "Processing 'Aphios Corporation'. Done.\n",
        "Processing 'AppDirect'. Done.\n",
        "Processing 'Applied Predictive Technologies'. Done.\n",
        "Processing 'appsFreedom'. Done.\n",
        "Processing 'Apttus'. Done.\n",
        "Processing 'Argos Therapeutics Inc'. Done.\n",
        "Processing 'Aseptia/Wright Foods'. Done.\n",
        "Processing 'Atlantis Computing'. Done.\n",
        "Processing 'Avere Systems'. Done.\n",
        "Processing 'Become Inc'. Done.\n",
        "Processing 'BeyondTrust'. Done.\n",
        "Processing 'Bluebeam Software Inc'. Done.\n",
        "Processing 'Signal'. Done.\n",
        "Processing 'Cinsay Inc'. Done.\n",
        "Processing 'Cognitive Networks'. Done.\n",
        "Processing 'Compass-EOS'. Done.\n",
        "Processing 'Credit Sesame'. Done.\n",
        "Processing 'Curemark LLC'. Done.\n",
        "Processing 'Daintree Networks'. Done.\n",
        "Processing 'Data Dynamics Inc'. Done.\n",
        "Processing 'Defense.Net'. Done.\n",
        "Processing 'DigiCert Inc'. Done.\n",
        "Processing 'DiscGenics'. Done.\n",
        "Processing 'Patient Communicator'. Done.\n",
        "Processing 'Twitpic'. Done.\n",
        "Processing 'BERG'. Done.\n",
        "Processing 'Wishberg'. Done.\n",
        "Processing 'GreenGar Studios'. Done.\n",
        "Processing 'Rivet & Sway'. Done.\n",
        "Processing 'Dijiwan'. Done.\n",
        "Processing 'Wantful'. Done.\n",
        "Processing 'Disruptive Media'. Done.\n",
        "Processing 'Calexda'. Done.\n",
        "Processing 'TurnTable.fm'. Done.\n",
        "Processing 'Tutorspree'. Done.\n",
        "Processing 'Nirvanix'. Done.\n",
        "Processing 'PostRocket'. Done.\n",
        "Processing 'VoterTide'. Done.\n",
        "Processing 'SkyRocket'. Done.\n",
        "Processing 'GameLayers'. Done.\n",
        "Processing 'Serendip'. Done.\n",
        "Processing 'Unifyo'. Done.\n",
        "Processing 'Lookery'. Done.\n",
        "Processing 'Canvas Networks'. Done.\n",
        "Processing 'Blurtt'. Done.\n",
        "Processing 'Manilla'. Done.\n",
        "Processing 'Pumodo'. Done.\n",
        "Processing 'Admazely'. Done.\n",
        "Processing 'Springpad'. Done.\n",
        "Processing 'ExFM'. Done.\n",
        "Processing 'Samba Mobile'. Done.\n",
        "Processing 'Qik'. Done.\n",
        "Processing 'inBloom'. Done.\n",
        "Processing 'IntoNow'. Done.\n",
        "Processing 'FindIt'. Done.\n",
        "Processing 'Inq Mobile'. Done.\n",
        "Processing 'Outbox'. Done.\n",
        "Processing 'Argyle Company'. Done.\n",
        "Processing 'Exec'. Done.\n",
        "Processing 'Bloom.fm'. Done.\n",
        "Processing 'Stipple'. Done.\n",
        "Processing 'Zumbox'. Done.\n",
        "Processing 'Behance'. Done.\n",
        "Processing 'Eventbrite'. Done.\n",
        "Processing 'MailChimp'. Done.\n",
        "Processing 'Twitch'. Done.\n",
        "Processing 'Moz'. Done.\n",
        "Processing 'HubSpot'. Done.\n",
        "Processing 'Indeed'. Done.\n",
        "Processing 'BufferApp'. Done.\n",
        "Processing 'Airbnb'. Done.\n",
        "Processing 'ThinkGeek'. Done.\n",
        "Processing 'Quora'. Done.\n",
        "Processing 'Tesla Motors'. Done.\n",
        "Processing 'Grooveshark'. Done.\n",
        "Processing 'Klout'. Done.\n",
        "Processing 'Zendesk'. Done.\n",
        "Processing 'About.me'. Done.\n",
        "Processing 'Uber'. Done.\n",
        "Processing 'Lumosity'. Done.\n",
        "Processing 'entrepreneur'. Done.\n",
        "Processing 'startup'. Done.\n",
        "Processing 'new venture'. Done.\n",
        "Processing 'manager'. Done.\n",
        "Processing 'executive'. Done.\n",
        "Processing 'founder'. Done.\n",
        "Processing 'wal-mart'. Done.\n",
        "Processing 'exxon'. Done.\n",
        "Processing 'chevron'. Done.\n",
        "Processing 'berkshire hathaway'. Done.\n",
        "Processing 'apple'. Done.\n",
        "Processing 'samsung'. Done.\n",
        "Processing 'phillips 66'. Done.\n",
        "Processing 'general motors'. Done.\n",
        "Processing 'ford'. Done.\n",
        "Processing 'valero'. Done.\n",
        "Processing 'exelon corporation'. Done.\n",
        "Processing 'dow chemical'. Done.\n",
        "Processing 'conocophillips'. Done.\n",
        "Processing 'morgan chase'. Done.\n",
        "Processing 'world fuel services'. Done.\n",
        "Processing 'illinois tool works'. Done.\n",
        "Processing 'boeing'. Done.\n",
        "Processing 'intel corporation'. Done.\n",
        "Processing 'pepsico'. Done.\n",
        "Processing 'tjx companies'. Done.\n",
        "Processing 'costco wholesale'. Done.\n",
        "Processing 'metlife'. Done.\n",
        "Processing 'blackrock'. Done.\n",
        "Processing 'at&t'. Done.\n",
        "Processing 'johnson & johnson'. Done.\n",
        "Processing 'abbott laboratories'. Done.\n",
        "Processing 'cvs caremark'. Done.\n",
        "Processing 'visa'. Done.\n",
        "Processing 'cisco systems'. Done.\n",
        "Processing 'walt disney company'. Done.\n",
        "Processing 'rock tenn company'. Done.\n",
        "Processing 'halliburton company'. Done.\n",
        "Processing 'alcoa'. Done.\n",
        "Processing 'fluor corporation'. Done.\n",
        "Processing 'general electric'. Done.\n",
        "Processing 'cbre group'. Done.\n",
        "Processing 'energy transfer equity'. Done.\n",
        "Processing 'express scripts holding'. Done.\n",
        "Processing 'hca holdings'. Done.\n",
        "Processing 'nrg energy'. Done.\n",
        "Processing 'aramark holdings corporation'. Done.\n",
        "Processing 'honeywell international'. Done.\n",
        "Processing 'nike'. Processing 'SkyCross'. Done.\n",
        "Processing 'Skyhigh Networks'. Done.\n",
        "Processing 'Skyonic Corporation'. Done.\n",
        "Processing 'SmartZip Analytics Inc'. Done.\n",
        "Processing 'Solace Systems'. Done.\n",
        "Processing 'Solavei'. Done.\n",
        "Processing 'SpiderCloud Wireless Inc'. Done.\n",
        "Processing 'Splice Machine'. Done.\n",
        "Processing 'SpotXchange Inc'. Done.\n",
        "Processing 'SPR Therapeutics LLC'. Done.\n",
        "Processing 'Stoke Inc'. Done.\n",
        "Processing 'StreetLight Data'. Done.\n",
        "Processing 'Sureline Systems Inc'. Done.\n",
        "Processing 'T3 Motion Inc'. Done.\n",
        "Processing 'TechLeads Online'. Done.\n",
        "Processing 'Tenable Network Security Inc'. Done.\n",
        "Processing 'ThreatTrack Security'. Done.\n",
        "Processing 'Trueffect'. Done.\n",
        "Processing 'TRX Systems Inc'. Done.\n",
        "Processing 'Ubiquity Global Services Inc'. Done.\n",
        "Processing 'Welltok'. Done.\n",
        "Processing 'Xactly Corporation'. Done.\n",
        "Processing 'XCOR Aerospace Inc'. Done.\n",
        "Processing 'Yashi'. Done.\n",
        "Processing 'Zeta Interactive'. Done.\n",
        "Processing 'Fuh'. Done.\n",
        "Processing 'Quest Nutrition'. Done.\n",
        "Processing 'Reliant Asset Management'. Done.\n",
        "Processing 'Superfish'. Done.\n",
        "Processing 'Acacia Communications'. Done.\n",
        "Processing 'Provider Power'. Done.\n",
        "Processing 'Crescendo Bioscience'. Done.\n",
        "Processing 'Plexus Worldwide'. Done.\n",
        "Processing 'Vacasa'. Done.\n",
        "Processing 'Go Energies'. Done.\n",
        "Processing 'Minute Key'. Done.\n",
        "Processing 'Sainstore'. Done.\n",
        "Processing 'The HCI Group'. Done.\n",
        "Processing 'Dynamic Dental Partners Group'. Done.\n",
        "Processing 'Aseptia'. Done.\n",
        "Processing 'Asentra Health'. Done.\n",
        "Processing 'American Solar Direct'. Done.\n",
        "Processing 'Prescient Edge'. Done.\n",
        "Processing 'BlueSquare Resolutions'. Done.\n",
        "Processing 'BES Design Build'. Done.\n",
        "Processing 'Simpler Trading'. Done.\n",
        "Processing 'sweetFrog Premium Frozen Yogurt'. Done.\n",
        "Processing 'Base Commerce'. Done.\n",
        "Processing 'CPSG Partners'. Done.\n",
        "Processing 'MedHOK'. Done.\n",
        "Processing 'Showroom Logic'. Done.\n",
        "Processing 'WeVeel'. Done.\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dfs = result.result\n",
      "sum([d._id.count() for d in dfs])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "1047"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_result = pd.concat(dfs)\n",
      "df_result._id.count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "1047"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df_result.to_csv(\"raw_parallel_execution_extended.csv\", encoding=\"utf8\", index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!du -h raw_parallel_execution_extended.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1.4M\traw_parallel_execution_extended.csv\r\n"
       ]
      }
     ],
     "prompt_number": 18
    }
   ],
   "metadata": {}
  }
 ]
}