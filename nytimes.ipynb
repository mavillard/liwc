{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Sentence extractor for The New York Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from datetime import timedelta, date, datetime\n",
    "from dateutil import parser\n",
    "from time import sleep, time\n",
    "\n",
    "import requests\n",
    "import yaml\n",
    "from joblib import Parallel, delayed\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import BulkWriteError, DuplicateKeyError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.remove('search.log')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logging.getLogger().handlers = []\n",
    "logging.getLogger('requests.packages.urllib3').setLevel(logging.WARNING)\n",
    "logging.basicConfig(filename='search.log', level=logging.INFO, format='%(asctime)s %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_log(*args, status=None):\n",
    "    record = '{} ==> {}'.format(args, status)\n",
    "    logging.info(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "client.drop_database('nytimes')\n",
    "db = client.nytimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def insert_documents(docs, q):\n",
    "    try:\n",
    "        inserted = db.articles.insert_many(docs, ordered=False)\n",
    "        total_inserted = len(inserted.inserted_ids)\n",
    "        write_log(q, status='INSERTION OK {}'.format(total_inserted))\n",
    "    except BulkWriteError as e:\n",
    "        for err in e.details['writeErrors']:\n",
    "            write_log(q, status='INSERTION EXCEPTION {}'.format(err['errmsg']))\n",
    "        write_log(q, status='INSERTION OK {}'.format(e.details['nInserted']))\n",
    "    except Exception as e:\n",
    "        write_log(q, status='INSERTION EXCEPTION {}'.format(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Search terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_term(term):\n",
    "    terms = []\n",
    "    \n",
    "    curated_term = term.lower()\n",
    "    curated_term = curated_term.replace(' & ', ' ')\n",
    "    curated_term = curated_term.replace(' and ', ' ')\n",
    "    terms.append(curated_term)\n",
    "    if '-' in curated_term:\n",
    "        terms.append(curated_term.replace('-', ''))\n",
    "#         terms.append(curated_term.replace('-', ' ')) # same result\n",
    "\n",
    "    for term in terms:\n",
    "        if term.endswith('corporation'):\n",
    "            terms.append(term[:-12])\n",
    "        elif term.endswith('corp'):\n",
    "            terms.append(term[:-5])\n",
    "        elif term.endswith('company'):\n",
    "            terms.append(term[:-8])\n",
    "        elif term.endswith('inc.'):\n",
    "            terms.append(term[:-5])\n",
    "        elif term.endswith('inc'):\n",
    "            terms.append(term[:-4])\n",
    "        elif term.endswith('.com'):\n",
    "            terms.append(term[:-4])\n",
    "    \n",
    "#     terms = list(map(lambda x: '\"{}\"'.format(x), terms))\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "term_dict = {}\n",
    "for i in range(1, 4):\n",
    "    term_dict[i] = {}\n",
    "    st_file = open('search_terms_{}.txt'.format(i))\n",
    "    term_list = map(lambda x: x.strip(), st_file.readlines())\n",
    "    for term in term_list:\n",
    "        term_dict[i][term] = preprocess_term(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('search_terms.yml', 'w') as search_term_file:\n",
    "    search_term_file.write(yaml.dump(term_dict, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('search_terms.yml') as search_term_file:\n",
    "    term_yaml = yaml.load(search_term_file.read())\n",
    "    #\n",
    "    for key in term_yaml:\n",
    "        d = term_yaml[key]\n",
    "        for k in d:\n",
    "            d[k] = list(map(lambda x: '\"{}\"'.format(x), d[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##NYTimes API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One API key for each of the cores\n",
    "api_keys = [\n",
    "#     \"3439a9084efa80c4f5fb1d290dfc1b44:11:70233981\", # my api key\n",
    "#     \"a5c709f3168b829711241b243457e9d6:13:70235641\", # the other api key\n",
    "    \"c7ba2eac72924572152e63f4516210d7:14:72380734\", # my second api key\n",
    "    \"7e692d35c7bd20618395859a3c4cbef6:15:72380785\", # my third api key\n",
    "    \"ba47374fd391c9bc5fd3ca51ff953a44:14:70229228\",\n",
    "    \"4557e02788189abb3642a33bca7469ff:11:69136863\",\n",
    "    \"2b3d39fd4c7836168a2a370c25ad6232:16:70235576\",\n",
    "    \"87d7b22c0feec4f3112d80b71d0b500a:1:69642501\",\n",
    "    \"d7655429355ab2df4621a10c01d04865:8:69135199\",\n",
    "    \"1944df13b86dd83e4a8c4ea82e767975:2:65092848\",\n",
    "#     \"730e30f5220059551e666430644fbf87:11:69642501\", # developer inactive\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_multiple(n, m):\n",
    "    # 4, 17 ==> 20\n",
    "    rest = m % n\n",
    "    return m if rest == 0 else m + n - rest\n",
    "\n",
    "def chunks(l, n_chunks):\n",
    "    l = list(l)\n",
    "    size = len(l)\n",
    "    n = next_multiple(n_chunks, size) // n_chunks\n",
    "    for i in range(0, size, n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [(search_term, original_term, term_type)]\n",
    "search_terms = [(t, k2, k1)\n",
    "                   for k1 in term_yaml\n",
    "                       for k2 in term_yaml[k1]\n",
    "                           for t in term_yaml[k1][k2]\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "\n",
    "# api_keys = [\n",
    "#     \"3439a9084efa80c4f5fb1d290dfc1b44:11:70233981\", # my api key\n",
    "#     \"a5c709f3168b829711241b243457e9d6:13:70235641\", # the other api key\n",
    "# ]\n",
    "\n",
    "# search_terms = [\n",
    "#     ('\"entrepreneur\"', 'entrepreneur', 1),\n",
    "#     ('\"executive\"', 'executive', 1),\n",
    "#     ('\"google\"', 'google', 2),\n",
    "#     ('\"amazon\"', 'amazon.com', 2),\n",
    "#     ('\"ebay\"', 'e-bay', 3),\n",
    "#     ('\"facebook\"', 'facebook', 3),\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "search_terms_by_api_key = defaultdict(list)\n",
    "for i in range(1, 4):\n",
    "    search_terms_i = list(filter(lambda x: x[2] == i, search_terms))\n",
    "    for t in zip(api_keys, chunks(search_terms_i, len(api_keys))):\n",
    "        search_terms_by_api_key[t[0]].extend(t[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def month_duration(d):\n",
    "    if d.month in [1, 3, 5, 7, 8, 10, 12]:\n",
    "        ndays = 31\n",
    "    elif d.month in [4, 6, 9, 11]:\n",
    "        ndays = 30\n",
    "    else: # d.month == 2\n",
    "        if d.year % 400 == 0 or d.year % 4 == 0 and d.year % 100 != 0: # lap-year\n",
    "            ndays = 29\n",
    "        else:\n",
    "            ndays = 28\n",
    "    return ndays\n",
    "\n",
    "def n_days(d, n_months):\n",
    "    ndays = 0\n",
    "    new_d = d\n",
    "    for _ in range(n_months):\n",
    "        m_duration = month_duration(d)\n",
    "        d += timedelta(m_duration)\n",
    "        ndays += m_duration\n",
    "    return ndays - 1\n",
    "\n",
    "def date_ranges(begin_date, end_date, n_months=1):\n",
    "    aux_date = begin_date\n",
    "    while aux_date < end_date:\n",
    "        ndays = n_days(aux_date, n_months)\n",
    "        yield (aux_date, min(aux_date + timedelta(ndays), end_date))\n",
    "        aux_date += timedelta(ndays + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LAST_REQUEST = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def wait(f, *args, t=9):\n",
    "    global LAST_REQUEST\n",
    "    now = time()\n",
    "    elapsed_time = now - LAST_REQUEST\n",
    "    if elapsed_time < t:\n",
    "        sleep(t - elapsed_time)\n",
    "    LAST_REQUEST = time()\n",
    "    return f(*args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Query:\n",
    "    def __init__(self, term, begin_date, end_date, page, api_key):\n",
    "        self.term = term\n",
    "        self.begin_date = begin_date\n",
    "        self.end_date = end_date\n",
    "        self.page = page\n",
    "        self.api_key =api_key\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'Q<{}, {}, {}, {}, {}>'.format(self.term, self.begin_date, self.end_date, self.page, self.api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BEGIN_DATE = date(1999, 1, 1)\n",
    "END_DATE = date(2014, 12, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search(q):\n",
    "    try:\n",
    "        base_url = 'http://api.nytimes.com/svc/search/v2/articlesearch.json'\n",
    "        payload = {'q': q.term, 'begin_date': q.begin_date, 'end_date': q.end_date, 'page': q.page, 'api-key': q.api_key}\n",
    "        fl = [\n",
    "            'web_url', 'snippet', 'lead_paragraph', 'abstract', 'source', 'headline',\n",
    "            'keywords', 'pub_date', 'document_type', 'section_name', '_id',\n",
    "        ]\n",
    "        payload.update({'sort': 'oldest', 'fq': 'source:(\"The New York Times\")', 'fl': ','.join(fl)})\n",
    "        r = requests.get(base_url, params=payload)\n",
    "        response = r.json()\n",
    "        \n",
    "        if response['status'] == 'OK':\n",
    "            write_log(q, status='SEARCH OK {}'.format(response['response']['meta']['hits']))\n",
    "        elif response['status'] == 'ERROR':\n",
    "            write_log(q, status='SEARCH ERROR {}'.format(response['errors']))\n",
    "        else:\n",
    "            write_log(q, status='SEARCH {}'.format(response['status']))\n",
    "    except ValueError as e:\n",
    "        if str(e) == 'Expecting value: line 1 column 1 (char 0)':\n",
    "            write_log(q, status='SEARCH ERROR api-key')\n",
    "        else:\n",
    "            write_log(q, status='SEARCH EXCEPTION {}'.format(e))\n",
    "        response = {'status': 'ERROR'}\n",
    "    except Exception as e:\n",
    "        write_log(q, status='SEARCH EXCEPTION {}'.format(e))\n",
    "        response = {'status': 'ERROR'}\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_documents_by_page(q, term, total_results):\n",
    "    n_pages = math.ceil(total_results / 10)\n",
    "    for page in range(n_pages):\n",
    "        q.page = page\n",
    "        response = wait(search, q)\n",
    "        \n",
    "        if response['status'] == 'OK':\n",
    "            docs = response['response']['docs']\n",
    "            for doc in docs:\n",
    "                doc.update({'q': q.__dict__, 'original_term': term[1], 'term_type': term[2]})\n",
    "            insert_documents(docs, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_documents_by_query(q, term):\n",
    "    response = wait(search, q)\n",
    "    \n",
    "    if response['status'] == 'OK':\n",
    "        total_results = response['response']['meta']['hits']\n",
    "        if total_results <= 1010:\n",
    "            get_documents_by_page(q, term, total_results)\n",
    "        else:\n",
    "            bd = parser.parse(q.begin_date)\n",
    "            ed = parser.parse(q.end_date)\n",
    "            half = (ed - bd) // 2\n",
    "            \n",
    "            begin_date1 = q.begin_date\n",
    "            end_date1 = (bd + timedelta(half.days)).strftime(\"%Y%m%d\")\n",
    "            q1 = Query(q.term, begin_date1, end_date1, 0, q.api_key)\n",
    "            get_documents_by_query(q1, term)\n",
    "            \n",
    "            begin_date2 = (bd + timedelta(half.days + 1)).strftime(\"%Y%m%d\")\n",
    "            end_date2 = q.end_date\n",
    "            q2 = Query(q.term, begin_date2, end_date2, 0, q.api_key)\n",
    "            get_documents_by_query(q2, term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_by_date_ranges(term, api_key):\n",
    "    for r in date_ranges(BEGIN_DATE, END_DATE, 1):\n",
    "        begin_date = r[0].strftime(\"%Y%m%d\")\n",
    "        end_date = r[1].strftime(\"%Y%m%d\")\n",
    "        q = Query(term[0], begin_date, end_date, 0, api_key)\n",
    "        get_documents_by_query(q, term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def download_by_term(term, api_key):\n",
    "    begin_date = BEGIN_DATE.strftime(\"%Y%m%d\")\n",
    "    end_date = END_DATE.strftime(\"%Y%m%d\")\n",
    "    q = Query(term[0], begin_date, end_date, 0, api_key)\n",
    "    response = wait(search, q)\n",
    "    \n",
    "    if response['status'] == 'OK':\n",
    "        total_results = response['response']['meta']['hits']\n",
    "        if total_results <= 1010:\n",
    "            get_documents_by_page(q, term, total_results)\n",
    "        else:\n",
    "            download_by_date_ranges(term, api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_by_key(terms, api_key):\n",
    "    for term in terms:\n",
    "        try:\n",
    "            download_by_term(term, api_key)\n",
    "        except Exception as e:\n",
    "            write_log(e, status='DOWNLOAD EXCEPTION {} {}'.format(term, api_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def downloader(search_terms_by_api_key, api_keys):\n",
    "    # Version parallel\n",
    "    Parallel(n_jobs=8)(delayed(download_by_key)(search_terms_by_api_key[api_key], api_key) for api_key in api_keys)\n",
    "\n",
    "    # Version sequencial\n",
    "#     for api_key in api_keys:\n",
    "#         download_by_key(search_terms_by_api_key[api_key], api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "downloader(search_terms_by_api_key, api_keys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
