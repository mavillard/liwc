{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Sentence extractor for The New York Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from datetime import timedelta, date\n",
    "from dateutil import parser\n",
    "from time import sleep\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import ujson as json\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Search terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "st_file = open('search_terms.txt')\n",
    "search_terms = map(lambda x: x.strip(), st_file.readlines())\n",
    "search_terms = ['executive']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##NYTimes API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One API key for each of the cores\n",
    "api_keys = [\n",
    "    \"3439a9084efa80c4f5fb1d290dfc1b44:11:70233981\", # my api key\n",
    "#     \"a5c709f3168b829711241b243457e9d6:13:70235641\",\n",
    "#     \"ba47374fd391c9bc5fd3ca51ff953a44:14:70229228\",\n",
    "#     \"4557e02788189abb3642a33bca7469ff:11:69136863\",\n",
    "#     \"2b3d39fd4c7836168a2a370c25ad6232:16:70235576\",\n",
    "#     \"87d7b22c0feec4f3112d80b71d0b500a:1:69642501\",\n",
    "#     \"d7655429355ab2df4621a10c01d04865:8:69135199\",\n",
    "#     \"1944df13b86dd83e4a8c4ea82e767975:2:65092848\",\n",
    "#     \"730e30f5220059551e666430644fbf87:11:69642501\", # developer inactive\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_multiple(n, m):\n",
    "    # 4, 17 ==> 20\n",
    "    rest = m % n\n",
    "    return m if rest == 0 else m + n - rest\n",
    "\n",
    "def chunks(l, n_chunks):\n",
    "    size = len(l)\n",
    "    n = next_multiple(n_chunks, size) // n_chunks\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i+n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "search_terms_by_api_key = {}\n",
    "for t in zip(api_keys, chunks(list(search_terms), len(api_keys))):\n",
    "    search_terms_by_api_key[t[0]] = t[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def month_duration(d):\n",
    "    if d.month in [1, 3, 5, 7, 8, 10, 12]:\n",
    "        ndays = 31\n",
    "    elif d.month in [4, 6, 9, 11]:\n",
    "        ndays = 30\n",
    "    else: # d.month == 2\n",
    "        if d.year % 400 == 0 or d.year % 4 == 0 and d.year % 100 != 0: # lap-year\n",
    "            ndays = 29\n",
    "        else:\n",
    "            ndays = 28\n",
    "    return ndays\n",
    "\n",
    "def n_days(d, n_months):\n",
    "    ndays = 0\n",
    "    new_d = d\n",
    "    for _ in range(n_months):\n",
    "        m_duration = month_duration(d)\n",
    "        d += timedelta(m_duration)\n",
    "        ndays += m_duration\n",
    "    return ndays - 1\n",
    "\n",
    "def date_ranges(begin_date, end_date, n_months=1):\n",
    "    aux_date = begin_date\n",
    "    while aux_date < end_date:\n",
    "        ndays = n_days(aux_date, n_months)\n",
    "        yield (aux_date, min(aux_date + timedelta(ndays), end_date))\n",
    "        aux_date += timedelta(ndays + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_query(term):\n",
    "    return term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def search(q, begin_date, end_date, sort, page, api_key):\n",
    "    base_url = 'http://api.nytimes.com/svc/search/v2/articlesearch.json'\n",
    "    payload = {'q': q, 'begin_date': begin_date, 'end_date': end_date, 'sort': sort, 'page': page, 'api-key': api_key}\n",
    "    response = requests.get(base_url, params=payload)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_documents(term, begin_date='19990101', end_date='20141231', sort='oldest', page=0, api_key='sample-key'):\n",
    "    print(term, begin_date, end_date, sort, page, api_key)\n",
    "    q = format_query(term)\n",
    "    response = search(q, begin_date, end_date, sort, page, api_key)\n",
    "    documents = []\n",
    "    \n",
    "    if response['status'] != 'OK':\n",
    "        print('Error')\n",
    "    else:\n",
    "        total_results = response['response']['meta']['hits']\n",
    "        if total_results == 0:\n",
    "            print('No results found.')\n",
    "        elif total_results <= 1010:\n",
    "            print('%d results found.' % total_results)\n",
    "            n_pages = math.ceil(total_results / 10)\n",
    "            for page in range(n_pages):\n",
    "                r = search(q, begin_date, end_date, sort, page, api_key)\n",
    "                if response['status'] != 'OK':\n",
    "                    print('Error')\n",
    "                else:\n",
    "                    docs = r['response']['docs']\n",
    "                    documents.extend(docs)\n",
    "        else: # total_results > 1010\n",
    "            bd = parser.parse(begin_date)\n",
    "            ed = parser.parse(end_date)\n",
    "            half = (ed - bd) // 2\n",
    "            \n",
    "            begin_date1 = begin_date\n",
    "            end_date1 = (bd + timedelta(half.days)).strftime(\"%Y%m%d\")\n",
    "            docs1 = get_documents(term, begin_date1, end_date1, sort, page, api_key)\n",
    "            \n",
    "            begin_date2 = (bd + timedelta(half.days + 1)).strftime(\"%Y%m%d\")\n",
    "            end_date2 = end_date\n",
    "            docs2 = get_documents(term, begin_date2, end_date2, sort, page, api_key)\n",
    "            \n",
    "            documents = docs1 + docs2\n",
    "    \n",
    "    # no sera una lista, sino un Dataframe\n",
    "    # eliminar posibles duplicados\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_by_date_range(term, api_key):\n",
    "    documents = []\n",
    "#     begin_date = date(1999, 1, 1)\n",
    "#     end_date = date(2014, 12, 31)\n",
    "    begin_date = date(2014, 1, 1)\n",
    "    end_date = date(2014, 2, 28)\n",
    "    for r in date_ranges(begin_date, end_date, 1):\n",
    "        begin_date = r[0].strftime(\"%Y%m%d\")\n",
    "        end_date = r[1].strftime(\"%Y%m%d\")\n",
    "        print(begin_date, end_date)\n",
    "        docs = get_documents(term, begin_date=begin_date, end_date=end_date, api_key=api_key)\n",
    "        documents.extend(docs)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_documents(api_key, terms):\n",
    "    documents = []\n",
    "    for term in terms:\n",
    "        docs = download_by_date_range(term, api_key)\n",
    "        documents.extend(docs)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def downloader(api_keys, search_terms_by_api_key):\n",
    "#     Parallel(n_jobs=2)(delayed(download_documents)(api_key, search_terms_by_api_key[api_key]) for api_key in api_keys)\n",
    "    documents = []\n",
    "    for api_key in api_keys:\n",
    "        docs = download_documents(api_key, search_terms_by_api_key[api_key])\n",
    "        documents.extend(docs)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20140101 20140131\n",
      "executive 20140101 20140131 oldest 0 3439a9084efa80c4f5fb1d290dfc1b44:11:70233981\n",
      "TOTAL: 2703\n",
      "2014-01-01 00:00:00\n",
      "2014-01-31 00:00:00\n",
      "15 days, 0:00:00\n",
      "executive 20140101 20140116 oldest 0 3439a9084efa80c4f5fb1d290dfc1b44:11:70233981\n",
      "TOTAL: 1283\n",
      "2014-01-01 00:00:00\n",
      "2014-01-16 00:00:00\n",
      "7 days, 12:00:00\n",
      "executive 20140101 20140108 oldest 0 3439a9084efa80c4f5fb1d290dfc1b44:11:70233981\n",
      "TOTAL: 487\n",
      "executive 20140109 20140116 oldest 0 3439a9084efa80c4f5fb1d290dfc1b44:11:70233981\n",
      "TOTAL: 796\n",
      "executive 20140117 20140131 oldest 0 3439a9084efa80c4f5fb1d290dfc1b44:11:70233981\n",
      "TOTAL: 1420\n",
      "2014-01-17 00:00:00\n",
      "2014-01-31 00:00:00\n",
      "7 days, 0:00:00\n",
      "executive 20140117 20140124 oldest 0 3439a9084efa80c4f5fb1d290dfc1b44:11:70233981\n",
      "TOTAL: 716\n",
      "executive 20140125 20140131 oldest 0 3439a9084efa80c4f5fb1d290dfc1b44:11:70233981\n",
      "TOTAL: 704\n",
      "20140201 20140228\n",
      "executive 20140201 20140228 oldest 0 3439a9084efa80c4f5fb1d290dfc1b44:11:70233981\n",
      "TOTAL: 2532\n",
      "2014-02-01 00:00:00\n",
      "2014-02-28 00:00:00\n",
      "13 days, 12:00:00\n",
      "executive 20140201 20140214 oldest 0 3439a9084efa80c4f5fb1d290dfc1b44:11:70233981\n",
      "TOTAL: 1315\n",
      "2014-02-01 00:00:00\n",
      "2014-02-14 00:00:00\n",
      "6 days, 12:00:00\n",
      "executive 20140201 20140207 oldest 0 3439a9084efa80c4f5fb1d290dfc1b44:11:70233981\n",
      "TOTAL: 662\n",
      "executive 20140208 20140214 oldest 0 3439a9084efa80c4f5fb1d290dfc1b44:11:70233981\n",
      "TOTAL: 653\n",
      "executive 20140215 20140228 oldest 0 3439a9084efa80c4f5fb1d290dfc1b44:11:70233981\n",
      "TOTAL: 1217\n",
      "2014-02-15 00:00:00\n",
      "2014-02-28 00:00:00\n",
      "6 days, 12:00:00\n",
      "executive 20140215 20140221 oldest 0 3439a9084efa80c4f5fb1d290dfc1b44:11:70233981\n",
      "TOTAL: 558\n",
      "executive 20140222 20140228 oldest 0 3439a9084efa80c4f5fb1d290dfc1b44:11:70233981\n",
      "TOTAL: 659\n"
     ]
    }
   ],
   "source": [
    "documents = downloader(api_keys, search_terms_by_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5235"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5235"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5233"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.drop_duplicates('_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1937"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1=date(1999,1,1)\n",
    "d2=date(1999,2,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(d2-d1).days // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(1999, 1, 1, 0, 0)"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse('19990101')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ValuesView(OrderedDict([('a', 2), ('b', 1)]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "d=OrderedDict({'b':1, 'a':2})\n",
    "d.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "3439a9084efa80c4f5fb1d290dfc1b44%3A11%3A70233981\n",
    "3439a9084efa80c4f5fb1d290dfc1b44%3A11%3A70233981"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "http://api.nytimes.com/svc/search/v2/articlesearch.json?sort=oldest&begin_date=19990101&api_key=3439a9084efa80c4f5fb1d290dfc1b44%3A11%3A70233981&end_date=20141231&q=entrepreneur&page=0\n",
    "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=entrepreneur&begin_date=19990101&end_date=20141231&sort=oldest&page=0&api-key=3439a9084efa80c4f5fb1d290dfc1b44%3A11%3A70233981\n",
    "http://api.nytimes.com/svc/search/v2/articlesearch.json?q=entrepreneur&begin_date=19990101&end_date=20141231&sort=oldest&page=0&api_key=3439a9084efa80c4f5fb1d290dfc1b44%3A11%3A70233981"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url1='http://api.nytimes.com/svc/search/v2/articlesearch.json?q=entrepreneur&begin_date=19990101&end_date=20141231&sort=oldest&page=0&api-key=3439a9084efa80c4f5fb1d290dfc1b44%3A11%3A70233981'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url2='http://api.nytimes.com/svc/search/v2/articlesearch.json?q=entrepreneur&begin_date=19990101&end_date=20141231&sort=oldest&page=0&api_key=3439a9084efa80c4f5fb1d290dfc1b44%3A11%3A70233981'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=131\n",
    "url1[:n]==url2[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://api.nytimes.com/svc/search/v2/articlesearch.json?q=entrepreneur&begin_date=19990101&end_date=20141231&sort=oldest&page=0&api'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url1[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(url2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://api.nytimes.com/svc/search/v2/articlesearch.json?q=entrepreneur&begin_date=19990101&end_date=20141231&sort=oldest&page=0&api-key=3439a9084efa80c4f5fb1d290dfc1b44%3A11%3A70233981'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://api.nytimes.com/svc/search/v2/articlesearch.json?q=entrepreneur&begin_date=19990101&end_date=20141231&sort=oldest&page=0&api_key=3439a9084efa80c4f5fb1d290dfc1b44%3A11%3A70233981'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://api.nytimes.com/svc/search/v2/articlesearch.json?sort=oldest&begin_date=19990101&api_key=3439a9084efa80c4f5fb1d290dfc1b44%3A11%3A70233981&end_date=20141231&q=entrepreneur&page=0'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://api.nytimes.com/svc/search/v2/articlesearch.json?q=entrepreneur&begin_date=19990101&end_date=20141231&sort=oldest&page=0&api_key=3439a9084efa80c4f5fb1d290dfc1b44%3A11%3A70233981'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "421"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(search_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "URL='http://api.nytimes.com/svc/search/v2/articlesearch.json?q=new+startups&begin_date=20130101&end_date=20130201&sort=newest&api-key=sample-key'\n",
    "r=urlopen(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ujson as json\n",
    "d=json.load(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['status', 'response', 'copyright'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "# NY Times - API requests automated script\n",
    "#\n",
    "# CulturePlex lab.\n",
    "#\n",
    "# @versae, @mavillard\n",
    "#\n",
    "# This cell contains unified in the same cell the downloading files and the article treatment.\n",
    "#\n",
    "#########\n",
    "\n",
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "import ujson as json\n",
    "from dateutil import parser\n",
    "from newspaper import Article\n",
    "from pandas.io.json import json_normalize\n",
    "from time import sleep\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# API Keys\n",
    "\n",
    "# api_key = '730e30f5220059551e666430644fbf87:11:69642501'\n",
    "# api_key = 'd7655429355ab2df4621a10c01d04865:8:69135199'\n",
    "api_key = '3439a9084efa80c4f5fb1d290dfc1b44:11:70233981'\n",
    "\n",
    "# We need to take into account the tag list used for the requests\n",
    "\n",
    "tag_lists = [['startup']]\n",
    "\n",
    "# This variable is used to create the appropiate path for the articles of the files\n",
    "\n",
    "# current_tags = ''\n",
    "\n",
    "# Dates for the requests\n",
    "\n",
    "begin_date = '20130101'\n",
    "# end_date = '20131231'\n",
    "end_date = '20130201'\n",
    "\n",
    "# Url for the requests and the paths to save the files\n",
    "\n",
    "base_url = 'http://api.nytimes.com/svc/search/v2/articlesearch.json'\n",
    "entries_url = base_url + '?q={q}&sort={sort}&begin_date={begin_date}&end_date={end_date}&api-key={api_key}&page={page}'\n",
    "\n",
    "data_path = os.path.join('data', 'nytimes_files')\n",
    "json_path = os.path.join(data_path, 'json_original')\n",
    "csv_path = os.path.join(data_path, 'csv_url')\n",
    "txt_path = os.path.join(data_path, 'txt_article')\n",
    "\n",
    "# Urls and variables needed for the Article extraction\n",
    "\n",
    "txt_article_temp_data_path = ''\n",
    "\n",
    "data_total = pd.DataFrame()\n",
    "\n",
    "wanted_columns = ['_id', 'web_url']\n",
    "new_column = 'url_works'\n",
    "\n",
    "change_columns_name = {'_id': 'id', 'web_url': 'url'}\n",
    "index_column = 'id'\n",
    "\n",
    "ordered_columns = ['url_works', 'url']\n",
    "\n",
    "##\n",
    "# get_url_works(row, tags)\n",
    "#\n",
    "# Function that checks if the article url works and, in that case, download and treat\n",
    "# the article text.\n",
    "##\n",
    "\n",
    "def get_url_works(row, attempts=3):\n",
    "    works = 0\n",
    "    url = row.url\n",
    "    \n",
    "    tag_dir = '_'.join(tags)\n",
    "    full_data_path = os.join.path(txt_path, tag_dir)\n",
    "    if not os.path.exists(tag_path):\n",
    "        os.makedirs(tag_path)\n",
    "    \n",
    "    if url.startswith('http://www.nytimes.com/') or url.startswith('https://www.nytimes.com/'):\n",
    "        url += '?pagewanted=all'\n",
    "    \n",
    "    while attempts > 0:\n",
    "        try:\n",
    "            a = Article(url, fetch_images=False, memorize_articles=False)\n",
    "            a.download()\n",
    "            a.parse()\n",
    "            \n",
    "            if a.is_valid_body():\n",
    "                works = 1\n",
    "                \n",
    "                path = os.path.join(tag_path, row.id + '.txt')\n",
    "                with io.open(path, 'w') as outfile:\n",
    "                    outfile.write(a.url)\n",
    "                    outfile.write(u'\\n\\n')\n",
    "                    outfile.write(a.title)\n",
    "                    outfile.write(u'\\n\\n')\n",
    "                    outfile.write(a.text)\n",
    "            \n",
    "            attempts = 0\n",
    "        \n",
    "        except:\n",
    "            attempts -= 1\n",
    "    \n",
    "    return pd.Series({new_column: works})\n",
    "\n",
    "# We create the csv path and txt article path if they don't exist.\n",
    "\n",
    "if not os.path.exists(csv_path):\n",
    "    os.makedirs(csv_path)\n",
    "    \n",
    "if not os.path.exists(txt_path):\n",
    "    os.makedirs(txt_path)\n",
    "\n",
    "# This is the main loop to request to the NY Times API for every tags in the tags list\n",
    "\n",
    "for tags in tag_lists:\n",
    "    \n",
    "    # Initialization variables\n",
    "    \n",
    "    perform_requests = True\n",
    "    first_request = True\n",
    "    \n",
    "    iteration = 0\n",
    "    page = 0\n",
    "    offset = 0\n",
    "    hits = 0\n",
    "    start_date = begin_date\n",
    "    \n",
    "    # We prepare the necessary paths\n",
    "    \n",
    "    tag_dir = '_'.join(tags)\n",
    "    full_data_path = os.join.path(txt_path, tag_dir)\n",
    "    print full_data_path\n",
    "    if not os.path.exists(full_data_path):\n",
    "        os.makedirs(full_data_path)\n",
    "    \n",
    "    # And now, we start with the requests\n",
    "    \n",
    "    while perform_requests:\n",
    "        q = '+'.join(tags)\n",
    "        request_url = entries_url.format(q=q, begin_date=start_date, end_date=end_date, api_key=api_key, page=page)\n",
    "        print request_url\n",
    "        response = urlopen(request_url)\n",
    "        sleep(0.1)  # Max. 10 request per second\n",
    "        \n",
    "        # Right now, we load the reponse in the data variable. This variable contains the JSON result.\n",
    "        \n",
    "        data = json.load(response)\n",
    "        \n",
    "        if (data['status'] != 'OK'):\n",
    "            perform_requests = False\n",
    "            print('\\tFinishing with errors for tags:', q)\n",
    "            print('\\tPrinting the response...')\n",
    "            print('\\t\\t' + response)\n",
    "            break\n",
    "        \n",
    "        if (len(data['response']['docs']) == 0):\n",
    "            perform_requests = False\n",
    "            print '\\tFinishing, tags:', '_'.join(tags)\n",
    "            break\n",
    "        \n",
    "        # We need to control the page, because the pagination ends when reach to 100.\n",
    "        # So, we need to change the begin date to start a new pagination from 0.\n",
    "        \n",
    "        page += 1\n",
    "        if page > 100:\n",
    "            last_index = len(data['response']['docs']) - 1\n",
    "            \n",
    "            last_date = parser.parse(data['response']['docs'][last_index]['pub_date'])\n",
    "            begin_date_aux = str(last_date.year) + '%02d' % last_date.month + '%02d' % last_date.day\n",
    "\n",
    "            page = 0\n",
    "            iteration += 1\n",
    "        \n",
    "        # We save the content in the JSON file.\n",
    "        \n",
    "        filename = '_'.join(tags) + '_' + str((iteration * 100) + page + iteration) + '.json'\n",
    "        print '\\tSaving file', filename\n",
    "        \n",
    "        with open(full_data_path + '/' + filename, 'w') as outfile:            \n",
    "            json.dump(data, outfile)\n",
    "        \n",
    "        # Once the file is saved, we focus on the article extraction.\n",
    "        \n",
    "        data_aux = json_normalize(data['response'], 'docs')[wanted_columns]\n",
    "            \n",
    "        data_aux.rename(columns=change_columns_name, inplace=True)\n",
    "        data_aux[new_column] = 0\n",
    "\n",
    "        url_works = data_aux.apply(get_url_works, axis=1)\n",
    "        data_aux.update(url_works)\n",
    "\n",
    "        if first_request:\n",
    "            data_total = data_aux.copy()\n",
    "            first_request = False\n",
    "        else:\n",
    "            data_total = data_total.append(data_aux)\n",
    "\n",
    "        data_aux = None\n",
    "\n",
    "    # And we finalize saving the csv files that links for every folder.    \n",
    "    \n",
    "    print '\\tTreating the big dataFrame'\n",
    "    \n",
    "    print '\\tShape before remove bad rows:', data_total.shape\n",
    "    filter_empty_url = (data_total.url != '')  # If doing this after saving to CSV, instead of that you need this: data.text.notnull()\n",
    "    data_total = data_total[(filter_empty_url)].copy()\n",
    "    print '\\tShape after remove bad rows:', data_total.shape\n",
    "        \n",
    "    print '\\tShape before remove duplicated rows:', data_total.shape\n",
    "    data_total.drop_duplicates(['id'], inplace=True)\n",
    "    print '\\tShape after remove duplicated rows:', data_total.shape\n",
    "    \n",
    "    data_total.set_index('id', inplace=True)\n",
    "    \n",
    "    print '\\tSaving to CSV'\n",
    "    \n",
    "    data_total = data_total.reindex_axis(ordered_columns, axis=1)\n",
    "    folder = '_'.join(tags)\n",
    "    data_total.to_csv(csv_url_data_path + '/' + folder + '.csv', encoding='utf-8')\n",
    "    \n",
    "    data_total = None\n",
    "\n",
    "print 'Finished!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "# NY Times - API requests automated script\n",
    "#\n",
    "# CulturePlex lab.\n",
    "#\n",
    "# @versae, @josemazo and @gabmunrio\n",
    "#\n",
    "# This cell contains all the logic to get the readability, diversity and sentiment from the texts.\n",
    "#\n",
    "#########\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ujson as json\n",
    "\n",
    "from pandas.io.json import json_normalize\n",
    "from pattern.metrics import readability\n",
    "from pattern.metrics import ttr as diversity\n",
    "from pattern.en import sentiment\n",
    "\n",
    "# Necessary paths to save and access to the files\n",
    "\n",
    "json_data_path = 'data/files_new_york_times/json_original/'\n",
    "article_data_path = 'data/files_new_york_times/txt_article/'\n",
    "\n",
    "csv_data_path = 'data/files_new_york_times/'\n",
    "text_data_path = 'data/files_new_york_times/data/'\n",
    "\n",
    "# Some variables for a correct behaviour\n",
    "\n",
    "exist_article_column = 'exist_article'\n",
    "article_text_column = 'text'\n",
    "text_file_path_column = 'file'\n",
    "\n",
    "no_company_topics = ['entrepreneur', 'startup', 'new_venture', 'manager', 'executive', 'founder']\n",
    "media = 'nyt'\n",
    "is_company = 0\n",
    "search_words = []\n",
    "dir_name = ''\n",
    "\n",
    "first = True\n",
    "data = None\n",
    "\n",
    "# Columns to reorder the different csv files\n",
    "\n",
    "sentiment_columns = ['readability', 'diversity', 'polarity', 'subjetivity']\n",
    "\n",
    "change_columns_name = {'web_url': 'url'}\n",
    "\n",
    "ordered_columns = ['media', 'search_words', 'is_company', 'pub_date',\n",
    "                   'readability', 'diversity', 'polarity', 'subjetivity',\n",
    "                   'text', 'url', 'file']\n",
    "\n",
    "ordered_no_text_columns = ['media', 'search_words', 'is_company', 'pub_date',\n",
    "                           'readability', 'diversity', 'polarity', 'subjetivity',\n",
    "                           'url', 'file']\n",
    "\n",
    "##\n",
    "# check_article_and_proceed(row)\n",
    "#\n",
    "# Function that checks if the article exists and proceed to treat it it is. In the treatment,\n",
    "# we calculate the values for readability, diversity and sentiment.\n",
    "#\n",
    "##\n",
    "\n",
    "def check_article_and_proceed(row):\n",
    "    file_name = article_data_path + dir_name + '/' + row._id + '.txt' \n",
    "    exist = os.path.isfile(file_name)\n",
    "    \n",
    "    save_file_name = ''\n",
    "    text = ''\n",
    "    \n",
    "    read = 0\n",
    "    diver = 0\n",
    "    pol = 0\n",
    "    sub = 0\n",
    "    \n",
    "    if exist:\n",
    "        if is_company:\n",
    "            company_topic = 'company'\n",
    "        else:\n",
    "            company_topic = 'topic'\n",
    "            \n",
    "        dt = datetime.datetime.strptime(row.pub_date, '%Y-%m-%dT%H:%M:%SZ')\n",
    "        dt = dt.strftime('%Y%m%dT%H%M%S')\n",
    "        \n",
    "        save_file_name = '_'.join([media, company_topic, search_words[0], dt, row._id]) + '.txt'\n",
    "        \n",
    "        with open(file_name, 'r') as file_in:\n",
    "            file_lines = file_in.read().splitlines(True)\n",
    "        \n",
    "        with open(text_data_path + save_file_name, 'w') as file_out:\n",
    "            file_out.writelines(file_lines[4:])\n",
    "        \n",
    "        with open(text_data_path + save_file_name, 'r') as file_in:\n",
    "            text = file_in.read()\n",
    "        \n",
    "        try:\n",
    "            read = readability(text)\n",
    "        except:\n",
    "            read = 0\n",
    "        \n",
    "        try:\n",
    "            diver = diversity(text)\n",
    "        except:\n",
    "            diver = 0\n",
    "            \n",
    "        try:\n",
    "            sent = sentiment(text)\n",
    "        except:\n",
    "            sent = (0, 0)\n",
    "        \n",
    "        pol = sent[0]\n",
    "        sub = sent[1]\n",
    "        \n",
    "    return pd.Series({\n",
    "        exist_article_column: exist,\n",
    "        article_text_column: text,\n",
    "        text_file_path_column: 'data/' + save_file_name,\n",
    "        sentiment_columns[0]: read,\n",
    "        sentiment_columns[1]: diver,\n",
    "        sentiment_columns[2]: pol,\n",
    "        sentiment_columns[3]: sub\n",
    "    })\n",
    "\n",
    "# We create the text_data_path if it does not exist.\n",
    "\n",
    "if not os.path.exists(text_data_path):\n",
    "    os.makedirs(text_data_path)\n",
    "\n",
    "# Main loop to create the final csv file. We are going to have two csv files, one with the\n",
    "# associated text of every row, and another without that text.\n",
    "    \n",
    "for dir_name in os.walk(json_data_path).next()[1]:\n",
    "    json_topic_path = json_data_path + dir_name + '/'\n",
    "    \n",
    "    if dir_name in no_company_topics:\n",
    "        is_company = 0\n",
    "    else:\n",
    "        is_company = 1\n",
    "    \n",
    "    search_words = dir_name.split('_')\n",
    "    \n",
    "    for file_name in os.walk(json_topic_path).next()[2]:\n",
    "        json_topic_file = json_topic_path + file_name\n",
    "        \n",
    "        json_file = json.load(open(json_topic_file))\n",
    "        data_aux = json_normalize(json_file['response'], 'docs')\n",
    "        \n",
    "        data_aux.rename(columns=change_columns_name, inplace=True)\n",
    "        \n",
    "        data_aux[exist_article_column] = False\n",
    "        data_aux[article_text_column] = ''\n",
    "        data_aux[text_file_path_column] = ''\n",
    "        data_aux[sentiment_columns[0]] = 0\n",
    "        data_aux[sentiment_columns[1]] = 0\n",
    "        data_aux[sentiment_columns[2]] = 0\n",
    "        data_aux[sentiment_columns[3]] = 0\n",
    "        \n",
    "        ordered_columns = ['media', 'search_words', 'is_company', 'pub_date',\n",
    "                   'readability', 'diversity', 'polarity', 'subjetivity',\n",
    "                   'text', 'url', 'file']\n",
    "        \n",
    "        data_aux[ordered_columns[0]] = media\n",
    "        data_aux[ordered_columns[1]] = ' '.join(search_words)\n",
    "        data_aux[ordered_columns[2]] = is_company\n",
    "        \n",
    "        data_exist = data_aux[['_id', 'pub_date']].apply(check_article_and_proceed, axis=1)\n",
    "        data_aux.update(data_exist)\n",
    "        \n",
    "        exist_filter = (data_aux[exist_article_column] == True)\n",
    "        data_aux = data_aux[exist_filter]\n",
    "        \n",
    "        empty_text_filter = (data_aux[article_text_column] != '')\n",
    "        data_aux = data_aux[empty_text_filter]\n",
    "        \n",
    "        data_aux = data_aux[ordered_columns]\n",
    "        data_aux = data_aux.reindex_axis(ordered_columns, axis=1)\n",
    "        \n",
    "        if first:\n",
    "            data = data_aux.copy()\n",
    "            first = False\n",
    "        else:\n",
    "            data = data.append(data_aux, ignore_index=True)\n",
    "\n",
    "        data_aux = None\n",
    "    \n",
    "    print 'Finished with ' + dir_name\n",
    "    \n",
    "data.to_csv(csv_data_path + 'new_york_times' + '.csv', index=False, encoding='utf-8')\n",
    "\n",
    "data[ordered_no_text_columns].to_csv(csv_data_path + 'new_york_times_no_text' + '.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print 'FINISHED!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.read_csv('data/files_new_york_times/new_york_times_no_text.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame = pd.read_csv('data/files_new_york_times/csv_url/startup.csv', encoding='utf-8')\n",
    "count = 0\n",
    "for result in dataFrame['url_works']:\n",
    "    if result == 1:\n",
    "        count = count + 1\n",
    "print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x=end_date - start_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_date += timedelta(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2013, 1, 2)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 28, 56, 84, 112, 140, 168, 196, 224, 252, 280, 308, 336, 364]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(0, 365, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(a,b):\n",
    "    aux = a\n",
    "    while aux<b:\n",
    "        yield (aux, b)\n",
    "        aux += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 10)\n",
      "(4, 10)\n",
      "(5, 10)\n",
      "(6, 10)\n",
      "(7, 10)\n",
      "(8, 10)\n",
      "(9, 10)\n"
     ]
    }
   ],
   "source": [
    "for x in f(3, 10):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012-01-01 20121130\n",
      "2012-12-01 20131031\n",
      "2013-11-01 20131231\n"
     ]
    }
   ],
   "source": [
    "def month_duration(d):\n",
    "    if d.month in [1, 3, 5, 7, 8, 10, 12]:\n",
    "        ndays = 31\n",
    "    elif d.month in [4, 6, 9, 11]:\n",
    "        ndays = 30\n",
    "    else: # d.month == 2\n",
    "        if d.year % 400 == 0 or d.year % 4 == 0 and d.year % 100 != 0: # lap-year\n",
    "            ndays = 29\n",
    "        else:\n",
    "            ndays = 28\n",
    "    return ndays\n",
    "\n",
    "def n_days(d, n_months):\n",
    "    ndays = 0\n",
    "    new_d = d\n",
    "    for _ in range(n_months):\n",
    "        m_duration = month_duration(d)\n",
    "        d += timedelta(m_duration)\n",
    "        ndays += m_duration\n",
    "    return ndays - 1\n",
    "\n",
    "def date_ranges(begin_date, end_date, n_months=1):\n",
    "    aux_date = begin_date\n",
    "    while aux_date < end_date:\n",
    "        ndays = n_days(aux_date, n_months)\n",
    "        yield (aux_date, min(aux_date + timedelta(ndays), end_date))\n",
    "        aux_date += timedelta(ndays + 1)\n",
    "\n",
    "start_date = date(2012, 1, 1)\n",
    "end_date = date(2013, 12, 31)\n",
    "for a,b in date_ranges(start_date, end_date, 11):\n",
    "    print(a,b.strftime(\"%Y%m%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "month_duration(date(2100, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f(d):\n",
    "    d += timedelta(1)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2012, 1, 1)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'months' is an invalid keyword argument for this function",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-241-07bc8f427a3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstart_date\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonths\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'months' is an invalid keyword argument for this function"
     ]
    }
   ],
   "source": [
    "start_date += timedelta(months=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-206-84ba409e8216>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-206-84ba409e8216>\"\u001b[1;36m, line \u001b[1;32m10\u001b[0m\n\u001b[1;33m    print(single_date.strftime(\"%Y-%m-%d\"))a\u001b[0m\n\u001b[1;37m                                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta, date\n",
    "\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int ((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "start_date = date(2013, 1, 1)\n",
    "end_date = date(2015, 6, 2)\n",
    "for single_date in daterange(start_date, end_date):\n",
    "    print(single_date.strftime(\"%Y-%m-%d\"))a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_date.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
