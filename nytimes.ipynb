{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Sentence extractor for The New York Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "from datetime import timedelta, date, datetime\n",
    "from dateutil import parser\n",
    "from time import sleep, time\n",
    "\n",
    "import requests\n",
    "from joblib import Parallel, delayed\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import BulkWriteError, DuplicateKeyError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.remove('search.log')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logging.getLogger().handlers = []\n",
    "logging.getLogger('requests.packages.urllib3').setLevel(logging.WARNING)\n",
    "logging.basicConfig(filename='search.log', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_log(*args, status=None):\n",
    "    dt_str = datetime.now().strftime('%Y-%m-%d @ %I:%M:%S %p')\n",
    "    record = '{} -- {} ==> {}'.format(dt_str, args, status)\n",
    "    logging.info(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "client.drop_database('nytimes')\n",
    "db = client.nytimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def insert_documents(docs):\n",
    "    total_inserted = 0\n",
    "    try:\n",
    "        inserted = db.articles.insert_many(docs)\n",
    "        total_inserted = len(inserted.inserted_ids)\n",
    "    except BulkWriteError as e:\n",
    "        write_log(e, status='EXCEPTION')\n",
    "        for doc in docs:\n",
    "            try:\n",
    "                db.articles.insert_one(doc)\n",
    "                total_inserted += 1\n",
    "            except DuplicateKeyError as e:\n",
    "                write_log(e, status='EXCEPTION')\n",
    "    except Exception as e:\n",
    "        write_log(e, status='EXCEPTION')\n",
    "    return total_inserted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Search terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_terms(term_list):\n",
    "    search_terms = []\n",
    "    for term in term_list:\n",
    "        if '-' in term or ' & ' in term or ' and ' in term:\n",
    "            if '-' in term:\n",
    "                search_terms.append(term)\n",
    "                search_terms.append(term.replace('-', ''))\n",
    "                search_terms.append(term.replace('-', ' '))\n",
    "            if ' & ' in term:\n",
    "                search_terms.append(term.replace(' & ', ' '))\n",
    "            if ' and ' in term:\n",
    "                search_terms.append(term.replace(' and ', ' '))\n",
    "        else:\n",
    "            search_terms.append(term)\n",
    "    search_terms = map(lambda x: '\"{}\"'.format(x), search_terms)\n",
    "    return search_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "st_file = open('search_terms.txt')\n",
    "term_list = map(lambda x: x.strip(), st_file.readlines())\n",
    "search_terms = preprocess_terms(term_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##NYTimes API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One API key for each of the cores\n",
    "api_keys = [\n",
    "#     \"3439a9084efa80c4f5fb1d290dfc1b44:11:70233981\", # my api key\n",
    "#     \"a5c709f3168b829711241b243457e9d6:13:70235641\", # the other api key\n",
    "    \"c7ba2eac72924572152e63f4516210d7:14:72380734\", # my second api key\n",
    "    \"7e692d35c7bd20618395859a3c4cbef6:15:72380785\", # my third api key\n",
    "    \"ba47374fd391c9bc5fd3ca51ff953a44:14:70229228\",\n",
    "    \"4557e02788189abb3642a33bca7469ff:11:69136863\",\n",
    "    \"2b3d39fd4c7836168a2a370c25ad6232:16:70235576\",\n",
    "    \"87d7b22c0feec4f3112d80b71d0b500a:1:69642501\",\n",
    "    \"d7655429355ab2df4621a10c01d04865:8:69135199\",\n",
    "    \"1944df13b86dd83e4a8c4ea82e767975:2:65092848\",\n",
    "#     \"730e30f5220059551e666430644fbf87:11:69642501\", # developer inactive\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_multiple(n, m):\n",
    "    # 4, 17 ==> 20\n",
    "    rest = m % n\n",
    "    return m if rest == 0 else m + n - rest\n",
    "\n",
    "def chunks(l, n_chunks):\n",
    "    l = list(l)\n",
    "    size = len(l)\n",
    "    n = next_multiple(n_chunks, size) // n_chunks\n",
    "    for i in range(0, size, n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "search_terms_by_api_key = {}\n",
    "for t in zip(api_keys, chunks(search_terms, len(api_keys))):\n",
    "    search_terms_by_api_key[t[0]] = t[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def month_duration(d):\n",
    "    if d.month in [1, 3, 5, 7, 8, 10, 12]:\n",
    "        ndays = 31\n",
    "    elif d.month in [4, 6, 9, 11]:\n",
    "        ndays = 30\n",
    "    else: # d.month == 2\n",
    "        if d.year % 400 == 0 or d.year % 4 == 0 and d.year % 100 != 0: # lap-year\n",
    "            ndays = 29\n",
    "        else:\n",
    "            ndays = 28\n",
    "    return ndays\n",
    "\n",
    "def n_days(d, n_months):\n",
    "    ndays = 0\n",
    "    new_d = d\n",
    "    for _ in range(n_months):\n",
    "        m_duration = month_duration(d)\n",
    "        d += timedelta(m_duration)\n",
    "        ndays += m_duration\n",
    "    return ndays - 1\n",
    "\n",
    "def date_ranges(begin_date, end_date, n_months=1):\n",
    "    aux_date = begin_date\n",
    "    while aux_date < end_date:\n",
    "        ndays = n_days(aux_date, n_months)\n",
    "        yield (aux_date, min(aux_date + timedelta(ndays), end_date))\n",
    "        aux_date += timedelta(ndays + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "last_request = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def wait(f, *args, t=9):\n",
    "    global last_request\n",
    "    now = time()\n",
    "    elapsed_time = now - last_request\n",
    "    if elapsed_time < t:\n",
    "        sleep(t - elapsed_time)\n",
    "    last_request = time()\n",
    "    return f(*args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_fields_doc(doc):\n",
    "    fl = [\n",
    "        'web_url', 'snippet', 'lead_paragraph', 'abstract', 'source', 'headline',\n",
    "        'keywords', 'pub_date', 'document_type', 'section_name', '_id',\n",
    "    ]\n",
    "    filtered = {key: doc[key] for key in fl}\n",
    "    filtered['headline'] = filtered['headline']['main']\n",
    "    return filtered\n",
    "\n",
    "def filter_fields(docs):\n",
    "    return list(map(filter_fields_doc, docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search(q, begin_date, end_date, page, api_key):\n",
    "    base_url = 'http://api.nytimes.com/svc/search/v2/articlesearch.json'\n",
    "    payload = {'q': q, 'begin_date': begin_date, 'end_date': end_date, 'page': page, 'api-key': api_key}\n",
    "    payload.update({'sort': 'oldest', 'fq': 'source:(\"The New York Times\")'})\n",
    "    response = requests.get(base_url, params=payload)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_documents(term, begin_date, end_date, api_key='sample-key'):\n",
    "    q = term\n",
    "    response = wait(search, q, begin_date, end_date, 0, api_key)\n",
    "    \n",
    "    if response['status'] != 'OK':\n",
    "        write_log(q, begin_date, end_date, api_key, status=response['status'])\n",
    "    else:\n",
    "        total_results = response['response']['meta']['hits']\n",
    "        write_log(q, begin_date, end_date, api_key, status='{} results found'.format(total_results))\n",
    "        \n",
    "        if total_results <= 1010:\n",
    "            if total_results == 0:\n",
    "                write_log(q, begin_date, end_date, api_key, status='0')\n",
    "                \n",
    "            n_pages = math.ceil(total_results / 10)\n",
    "            for page in range(n_pages):\n",
    "                r = wait(search, q, begin_date, end_date, page, api_key)\n",
    "                if r['status'] != 'OK':\n",
    "                    write_log(q, begin_date, end_date, page, api_key, status=response['status'])\n",
    "                else:\n",
    "                    total_inserted = insert_documents(filter_fields(r['response']['docs']))\n",
    "                    write_log(q, begin_date, end_date, page, api_key, status='{} results written'.format(total_inserted))\n",
    "        else:\n",
    "            bd = parser.parse(begin_date)\n",
    "            ed = parser.parse(end_date)\n",
    "            half = (ed - bd) // 2\n",
    "            \n",
    "            begin_date1 = begin_date\n",
    "            end_date1 = (bd + timedelta(half.days)).strftime(\"%Y%m%d\")\n",
    "            get_documents(term, begin_date1, end_date1, api_key)\n",
    "            \n",
    "            begin_date2 = (bd + timedelta(half.days + 1)).strftime(\"%Y%m%d\")\n",
    "            end_date2 = end_date\n",
    "            get_documents(term, begin_date2, end_date2, api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_by_date_range(term, api_key):\n",
    "    begin_date = date(1999, 1, 1)\n",
    "    end_date = date(2014, 12, 31)\n",
    "    for r in date_ranges(begin_date, end_date, 1):\n",
    "        try:\n",
    "            begin_date = r[0].strftime(\"%Y%m%d\")\n",
    "            end_date = r[1].strftime(\"%Y%m%d\")\n",
    "            get_documents(term, begin_date=begin_date, end_date=end_date, api_key=api_key)\n",
    "        except Exception as e:\n",
    "            write_log(e, status='EXCEPTION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_documents(terms, api_key):\n",
    "    begin_date = '19990101'\n",
    "    end_date = '20141231'\n",
    "    for term in terms:\n",
    "        q = term\n",
    "        try:\n",
    "            response = wait(search, q, begin_date, end_date, 0, api_key)\n",
    "            if response['status'] == 'OK':\n",
    "                if response['response']['meta']['hits'] == 0:\n",
    "                    write_log(q, begin_date, end_date, api_key, status='0')\n",
    "                else:\n",
    "                    download_by_date_range(term, api_key)\n",
    "        except Exception as e:\n",
    "            write_log(e, status='EXCEPTION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def downloader(search_terms_by_api_key, api_keys):\n",
    "    try:\n",
    "        Parallel(n_jobs=8)(delayed(download_documents)(search_terms_by_api_key[api_key], api_key) for api_key in api_keys)\n",
    "    except Exception as e:\n",
    "        write_log(e, status='EXCEPTION')\n",
    "        for api_key in api_keys:\n",
    "            try:\n",
    "                download_documents(search_terms_by_api_key[api_key], api_key)\n",
    "            except Exception as e:\n",
    "                write_log(e, status='EXCEPTION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "downloader(search_terms_by_api_key, api_keys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
