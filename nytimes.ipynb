{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Sentence extractor for New York Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "from time import sleep\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import ujson as json\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Search terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "st_file = open('search_terms.txt')\n",
    "search_terms = map(lambda x: x.strip(), st_file.readlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##NYTimes API keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One API key for each of the cores\n",
    "api_keys = [\n",
    "    \"3439a9084efa80c4f5fb1d290dfc1b44:11:70233981\", # my api key\n",
    "    \"a5c709f3168b829711241b243457e9d6:13:70235641\",\n",
    "    \"ba47374fd391c9bc5fd3ca51ff953a44:14:70229228\",\n",
    "    \"4557e02788189abb3642a33bca7469ff:11:69136863\",\n",
    "    \"87d7b22c0feec4f3112d80b71d0b500a:1:69642501\",\n",
    "    \"2b3d39fd4c7836168a2a370c25ad6232:16:70235576\",\n",
    "    \"d7655429355ab2df4621a10c01d04865:8:69135199\",\n",
    "#     \"730e30f5220059551e666430644fbf87:11:69642501\",  # Inactive\n",
    "    \"1944df13b86dd83e4a8c4ea82e767975:2:65092848\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_multiple(n, m):\n",
    "    # 4, 17 ==> 20\n",
    "    rest = m % n\n",
    "    return m if rest == 0 else m + n - rest\n",
    "\n",
    "def chunks(l, n_chunks):\n",
    "    size = len(l)\n",
    "    n = next_multiple(n_chunks, size) // n_chunks\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i+n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "search_terms_by_api_key = {}\n",
    "for t in zip(api_keys, chunks(list(search_terms), len(api_keys))):\n",
    "    search_terms_by_api_key[t[0]] = t[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_query(term):\n",
    "    return term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_response(response):\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search(q, start_date, end_date, sort, page, api_key):\n",
    "    base_url = 'http://api.nytimes.com/svc/search/v2/articlesearch.json'\n",
    "    payload = {'q': q, 'begin_date': start_date, 'end_date': end_date, 'sort': sort, 'page': page, 'api_key': api_key}\n",
    "    response = requests.get(base_url, params=payload)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_documents(term, begin_date='19990101', end_date='20141231', sort='oldest', page=0, api_key='sample-key'):\n",
    "    q = format_query(term)\n",
    "    response = search(q, start_date, end_date, sort, page, api_key)\n",
    "    \n",
    "    if response['status'] != 'OK':\n",
    "        documents = []\n",
    "        print('Error')\n",
    "    \n",
    "    total_results = response['response']['meta']['hits']\n",
    "    if total_results == 0:\n",
    "        documents = []\n",
    "        print('No results found.')\n",
    "    elif total_results <= 10:\n",
    "        documents = response['response']['docs']\n",
    "        print('>>>', len(docs), 'results found.')\n",
    "    elif total_results <= 1010:\n",
    "        documents = []\n",
    "        n_pages = math.ceil(total_results / 10)\n",
    "        for page in range(n_pages):\n",
    "            docs = get_documents(term, begin_date, end_date, sort, page, api_key)\n",
    "            documents.append(docs)\n",
    "    else: # total_results > 1010\n",
    "        docs = []\n",
    "        for page in range(n_pages):\n",
    "            docs = get_documents(term, begin_date, end_date, sort, page, api_key)\n",
    "            documents.append(docs)\n",
    "    \n",
    "    # no sera una lista, sino un Dataframe\n",
    "    # eliminar posibles duplicados\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_documents_for_term(term, begin_date='19990101', end_date='20141231', sort='oldest', page=0, api_key='sample-key'):\n",
    "    q = format_query(term)\n",
    "    response = search(q, start_date, end_date, sort, page, api_key)\n",
    "    \n",
    "    if response['status'] != 'OK':\n",
    "        pass\n",
    "    \n",
    "    total_results = response['response']['meta']['hits']\n",
    "    if total_results == 0:\n",
    "        docs = []\n",
    "        print('No results found.')\n",
    "    elif total_results <= 10:\n",
    "        docs = response['response']['docs']\n",
    "        print('>>>', len(docs), 'results found.')\n",
    "    elif total_results <= 1010:\n",
    "        n_pages = math.ceil(total_results / 10)\n",
    "        docs = \n",
    "        for page in range(n_pages)\n",
    "    elif total_results <= 10:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_documents(api_key, terms):\n",
    "    for term in terms:\n",
    "        download_documents_for_term(api_key, term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def downloader(api_keys, search_terms_by_api_key):\n",
    "    Parallel(n_jobs=8)(delayed(download_documents)(api_key, search_terms_by_api_key[api_key]) for api_key in api_keys)\n",
    "#     for api_key in api_keys:\n",
    "#         download_terms(api_key, search_terms_by_api_key[api_key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "downloader(api_keys, search_terms_by_api_key):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "421"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(search_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "URL='http://api.nytimes.com/svc/search/v2/articlesearch.json?q=new+startups&begin_date=20130101&end_date=20130201&sort=newest&api-key=sample-key'\n",
    "r=urlopen(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ujson as json\n",
    "d=json.load(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['status', 'response', 'copyright'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "# NY Times - API requests automated script\n",
    "#\n",
    "# CulturePlex lab.\n",
    "#\n",
    "# @versae, @mavillard\n",
    "#\n",
    "# This cell contains unified in the same cell the downloading files and the article treatment.\n",
    "#\n",
    "#########\n",
    "\n",
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "import ujson as json\n",
    "from dateutil import parser\n",
    "from newspaper import Article\n",
    "from pandas.io.json import json_normalize\n",
    "from time import sleep\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# API Keys\n",
    "\n",
    "# api_key = '730e30f5220059551e666430644fbf87:11:69642501'\n",
    "# api_key = 'd7655429355ab2df4621a10c01d04865:8:69135199'\n",
    "api_key = '3439a9084efa80c4f5fb1d290dfc1b44:11:70233981'\n",
    "\n",
    "# We need to take into account the tag list used for the requests\n",
    "\n",
    "tag_lists = [['startup']]\n",
    "\n",
    "# This variable is used to create the appropiate path for the articles of the files\n",
    "\n",
    "# current_tags = ''\n",
    "\n",
    "# Dates for the requests\n",
    "\n",
    "begin_date = '20130101'\n",
    "# end_date = '20131231'\n",
    "end_date = '20130201'\n",
    "\n",
    "# Url for the requests and the paths to save the files\n",
    "\n",
    "base_url = 'http://api.nytimes.com/svc/search/v2/articlesearch.json'\n",
    "entries_url = base_url + '?q={q}&sort={sort}&begin_date={begin_date}&end_date={end_date}&api-key={api_key}&page={page}'\n",
    "\n",
    "data_path = os.path.join('data', 'nytimes_files')\n",
    "json_path = os.path.join(data_path, 'json_original')\n",
    "csv_path = os.path.join(data_path, 'csv_url')\n",
    "txt_path = os.path.join(data_path, 'txt_article')\n",
    "\n",
    "# Urls and variables needed for the Article extraction\n",
    "\n",
    "txt_article_temp_data_path = ''\n",
    "\n",
    "data_total = pd.DataFrame()\n",
    "\n",
    "wanted_columns = ['_id', 'web_url']\n",
    "new_column = 'url_works'\n",
    "\n",
    "change_columns_name = {'_id': 'id', 'web_url': 'url'}\n",
    "index_column = 'id'\n",
    "\n",
    "ordered_columns = ['url_works', 'url']\n",
    "\n",
    "##\n",
    "# get_url_works(row, tags)\n",
    "#\n",
    "# Function that checks if the article url works and, in that case, download and treat\n",
    "# the article text.\n",
    "##\n",
    "\n",
    "def get_url_works(row, attempts=3):\n",
    "    works = 0\n",
    "    url = row.url\n",
    "    \n",
    "    tag_dir = '_'.join(tags)\n",
    "    full_data_path = os.join.path(txt_path, tag_dir)\n",
    "    if not os.path.exists(tag_path):\n",
    "        os.makedirs(tag_path)\n",
    "    \n",
    "    if url.startswith('http://www.nytimes.com/') or url.startswith('https://www.nytimes.com/'):\n",
    "        url += '?pagewanted=all'\n",
    "    \n",
    "    while attempts > 0:\n",
    "        try:\n",
    "            a = Article(url, fetch_images=False, memorize_articles=False)\n",
    "            a.download()\n",
    "            a.parse()\n",
    "            \n",
    "            if a.is_valid_body():\n",
    "                works = 1\n",
    "                \n",
    "                path = os.path.join(tag_path, row.id + '.txt')\n",
    "                with io.open(path, 'w') as outfile:\n",
    "                    outfile.write(a.url)\n",
    "                    outfile.write(u'\\n\\n')\n",
    "                    outfile.write(a.title)\n",
    "                    outfile.write(u'\\n\\n')\n",
    "                    outfile.write(a.text)\n",
    "            \n",
    "            attempts = 0\n",
    "        \n",
    "        except:\n",
    "            attempts -= 1\n",
    "    \n",
    "    return pd.Series({new_column: works})\n",
    "\n",
    "# We create the csv path and txt article path if they don't exist.\n",
    "\n",
    "if not os.path.exists(csv_path):\n",
    "    os.makedirs(csv_path)\n",
    "    \n",
    "if not os.path.exists(txt_path):\n",
    "    os.makedirs(txt_path)\n",
    "\n",
    "# This is the main loop to request to the NY Times API for every tags in the tags list\n",
    "\n",
    "for tags in tag_lists:\n",
    "    \n",
    "    # Initialization variables\n",
    "    \n",
    "    perform_requests = True\n",
    "    first_request = True\n",
    "    \n",
    "    iteration = 0\n",
    "    page = 0\n",
    "    offset = 0\n",
    "    hits = 0\n",
    "    start_date = begin_date\n",
    "    \n",
    "    # We prepare the necessary paths\n",
    "    \n",
    "    tag_dir = '_'.join(tags)\n",
    "    full_data_path = os.join.path(txt_path, tag_dir)\n",
    "    print full_data_path\n",
    "    if not os.path.exists(full_data_path):\n",
    "        os.makedirs(full_data_path)\n",
    "    \n",
    "    # And now, we start with the requests\n",
    "    \n",
    "    while perform_requests:\n",
    "        q = '+'.join(tags)\n",
    "        request_url = entries_url.format(q=q, begin_date=start_date, end_date=end_date, api_key=api_key, page=page)\n",
    "        print request_url\n",
    "        response = urlopen(request_url)\n",
    "        sleep(0.1)  # Max. 10 request per second\n",
    "        \n",
    "        # Right now, we load the reponse in the data variable. This variable contains the JSON result.\n",
    "        \n",
    "        data = json.load(response)\n",
    "        \n",
    "        if (data['status'] != 'OK'):\n",
    "            perform_requests = False\n",
    "            print('\\tFinishing with errors for tags:', q)\n",
    "            print('\\tPrinting the response...')\n",
    "            print('\\t\\t' + response)\n",
    "            break\n",
    "        \n",
    "        if (len(data['response']['docs']) == 0):\n",
    "            perform_requests = False\n",
    "            print '\\tFinishing, tags:', '_'.join(tags)\n",
    "            break\n",
    "        \n",
    "        # We need to control the page, because the pagination ends when reach to 100.\n",
    "        # So, we need to change the begin date to start a new pagination from 0.\n",
    "        \n",
    "        page += 1\n",
    "        if page > 100:\n",
    "            last_index = len(data['response']['docs']) - 1\n",
    "            \n",
    "            last_date = parser.parse(data['response']['docs'][last_index]['pub_date'])\n",
    "            begin_date_aux = str(last_date.year) + '%02d' % last_date.month + '%02d' % last_date.day\n",
    "\n",
    "            page = 0\n",
    "            iteration += 1\n",
    "        \n",
    "        # We save the content in the JSON file.\n",
    "        \n",
    "        filename = '_'.join(tags) + '_' + str((iteration * 100) + page + iteration) + '.json'\n",
    "        print '\\tSaving file', filename\n",
    "        \n",
    "        with open(full_data_path + '/' + filename, 'w') as outfile:            \n",
    "            json.dump(data, outfile)\n",
    "        \n",
    "        # Once the file is saved, we focus on the article extraction.\n",
    "        \n",
    "        data_aux = json_normalize(data['response'], 'docs')[wanted_columns]\n",
    "            \n",
    "        data_aux.rename(columns=change_columns_name, inplace=True)\n",
    "        data_aux[new_column] = 0\n",
    "\n",
    "        url_works = data_aux.apply(get_url_works, axis=1)\n",
    "        data_aux.update(url_works)\n",
    "\n",
    "        if first_request:\n",
    "            data_total = data_aux.copy()\n",
    "            first_request = False\n",
    "        else:\n",
    "            data_total = data_total.append(data_aux)\n",
    "\n",
    "        data_aux = None\n",
    "\n",
    "    # And we finalize saving the csv files that links for every folder.    \n",
    "    \n",
    "    print '\\tTreating the big dataFrame'\n",
    "    \n",
    "    print '\\tShape before remove bad rows:', data_total.shape\n",
    "    filter_empty_url = (data_total.url != '')  # If doing this after saving to CSV, instead of that you need this: data.text.notnull()\n",
    "    data_total = data_total[(filter_empty_url)].copy()\n",
    "    print '\\tShape after remove bad rows:', data_total.shape\n",
    "        \n",
    "    print '\\tShape before remove duplicated rows:', data_total.shape\n",
    "    data_total.drop_duplicates(['id'], inplace=True)\n",
    "    print '\\tShape after remove duplicated rows:', data_total.shape\n",
    "    \n",
    "    data_total.set_index('id', inplace=True)\n",
    "    \n",
    "    print '\\tSaving to CSV'\n",
    "    \n",
    "    data_total = data_total.reindex_axis(ordered_columns, axis=1)\n",
    "    folder = '_'.join(tags)\n",
    "    data_total.to_csv(csv_url_data_path + '/' + folder + '.csv', encoding='utf-8')\n",
    "    \n",
    "    data_total = None\n",
    "\n",
    "print 'Finished!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "# NY Times - API requests automated script\n",
    "#\n",
    "# CulturePlex lab.\n",
    "#\n",
    "# @versae, @josemazo and @gabmunrio\n",
    "#\n",
    "# This cell contains all the logic to get the readability, diversity and sentiment from the texts.\n",
    "#\n",
    "#########\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ujson as json\n",
    "\n",
    "from pandas.io.json import json_normalize\n",
    "from pattern.metrics import readability\n",
    "from pattern.metrics import ttr as diversity\n",
    "from pattern.en import sentiment\n",
    "\n",
    "# Necessary paths to save and access to the files\n",
    "\n",
    "json_data_path = 'data/files_new_york_times/json_original/'\n",
    "article_data_path = 'data/files_new_york_times/txt_article/'\n",
    "\n",
    "csv_data_path = 'data/files_new_york_times/'\n",
    "text_data_path = 'data/files_new_york_times/data/'\n",
    "\n",
    "# Some variables for a correct behaviour\n",
    "\n",
    "exist_article_column = 'exist_article'\n",
    "article_text_column = 'text'\n",
    "text_file_path_column = 'file'\n",
    "\n",
    "no_company_topics = ['entrepreneur', 'startup', 'new_venture', 'manager', 'executive', 'founder']\n",
    "media = 'nyt'\n",
    "is_company = 0\n",
    "search_words = []\n",
    "dir_name = ''\n",
    "\n",
    "first = True\n",
    "data = None\n",
    "\n",
    "# Columns to reorder the different csv files\n",
    "\n",
    "sentiment_columns = ['readability', 'diversity', 'polarity', 'subjetivity']\n",
    "\n",
    "change_columns_name = {'web_url': 'url'}\n",
    "\n",
    "ordered_columns = ['media', 'search_words', 'is_company', 'pub_date',\n",
    "                   'readability', 'diversity', 'polarity', 'subjetivity',\n",
    "                   'text', 'url', 'file']\n",
    "\n",
    "ordered_no_text_columns = ['media', 'search_words', 'is_company', 'pub_date',\n",
    "                           'readability', 'diversity', 'polarity', 'subjetivity',\n",
    "                           'url', 'file']\n",
    "\n",
    "##\n",
    "# check_article_and_proceed(row)\n",
    "#\n",
    "# Function that checks if the article exists and proceed to treat it it is. In the treatment,\n",
    "# we calculate the values for readability, diversity and sentiment.\n",
    "#\n",
    "##\n",
    "\n",
    "def check_article_and_proceed(row):\n",
    "    file_name = article_data_path + dir_name + '/' + row._id + '.txt' \n",
    "    exist = os.path.isfile(file_name)\n",
    "    \n",
    "    save_file_name = ''\n",
    "    text = ''\n",
    "    \n",
    "    read = 0\n",
    "    diver = 0\n",
    "    pol = 0\n",
    "    sub = 0\n",
    "    \n",
    "    if exist:\n",
    "        if is_company:\n",
    "            company_topic = 'company'\n",
    "        else:\n",
    "            company_topic = 'topic'\n",
    "            \n",
    "        dt = datetime.datetime.strptime(row.pub_date, '%Y-%m-%dT%H:%M:%SZ')\n",
    "        dt = dt.strftime('%Y%m%dT%H%M%S')\n",
    "        \n",
    "        save_file_name = '_'.join([media, company_topic, search_words[0], dt, row._id]) + '.txt'\n",
    "        \n",
    "        with open(file_name, 'r') as file_in:\n",
    "            file_lines = file_in.read().splitlines(True)\n",
    "        \n",
    "        with open(text_data_path + save_file_name, 'w') as file_out:\n",
    "            file_out.writelines(file_lines[4:])\n",
    "        \n",
    "        with open(text_data_path + save_file_name, 'r') as file_in:\n",
    "            text = file_in.read()\n",
    "        \n",
    "        try:\n",
    "            read = readability(text)\n",
    "        except:\n",
    "            read = 0\n",
    "        \n",
    "        try:\n",
    "            diver = diversity(text)\n",
    "        except:\n",
    "            diver = 0\n",
    "            \n",
    "        try:\n",
    "            sent = sentiment(text)\n",
    "        except:\n",
    "            sent = (0, 0)\n",
    "        \n",
    "        pol = sent[0]\n",
    "        sub = sent[1]\n",
    "        \n",
    "    return pd.Series({\n",
    "        exist_article_column: exist,\n",
    "        article_text_column: text,\n",
    "        text_file_path_column: 'data/' + save_file_name,\n",
    "        sentiment_columns[0]: read,\n",
    "        sentiment_columns[1]: diver,\n",
    "        sentiment_columns[2]: pol,\n",
    "        sentiment_columns[3]: sub\n",
    "    })\n",
    "\n",
    "# We create the text_data_path if it does not exist.\n",
    "\n",
    "if not os.path.exists(text_data_path):\n",
    "    os.makedirs(text_data_path)\n",
    "\n",
    "# Main loop to create the final csv file. We are going to have two csv files, one with the\n",
    "# associated text of every row, and another without that text.\n",
    "    \n",
    "for dir_name in os.walk(json_data_path).next()[1]:\n",
    "    json_topic_path = json_data_path + dir_name + '/'\n",
    "    \n",
    "    if dir_name in no_company_topics:\n",
    "        is_company = 0\n",
    "    else:\n",
    "        is_company = 1\n",
    "    \n",
    "    search_words = dir_name.split('_')\n",
    "    \n",
    "    for file_name in os.walk(json_topic_path).next()[2]:\n",
    "        json_topic_file = json_topic_path + file_name\n",
    "        \n",
    "        json_file = json.load(open(json_topic_file))\n",
    "        data_aux = json_normalize(json_file['response'], 'docs')\n",
    "        \n",
    "        data_aux.rename(columns=change_columns_name, inplace=True)\n",
    "        \n",
    "        data_aux[exist_article_column] = False\n",
    "        data_aux[article_text_column] = ''\n",
    "        data_aux[text_file_path_column] = ''\n",
    "        data_aux[sentiment_columns[0]] = 0\n",
    "        data_aux[sentiment_columns[1]] = 0\n",
    "        data_aux[sentiment_columns[2]] = 0\n",
    "        data_aux[sentiment_columns[3]] = 0\n",
    "        \n",
    "        ordered_columns = ['media', 'search_words', 'is_company', 'pub_date',\n",
    "                   'readability', 'diversity', 'polarity', 'subjetivity',\n",
    "                   'text', 'url', 'file']\n",
    "        \n",
    "        data_aux[ordered_columns[0]] = media\n",
    "        data_aux[ordered_columns[1]] = ' '.join(search_words)\n",
    "        data_aux[ordered_columns[2]] = is_company\n",
    "        \n",
    "        data_exist = data_aux[['_id', 'pub_date']].apply(check_article_and_proceed, axis=1)\n",
    "        data_aux.update(data_exist)\n",
    "        \n",
    "        exist_filter = (data_aux[exist_article_column] == True)\n",
    "        data_aux = data_aux[exist_filter]\n",
    "        \n",
    "        empty_text_filter = (data_aux[article_text_column] != '')\n",
    "        data_aux = data_aux[empty_text_filter]\n",
    "        \n",
    "        data_aux = data_aux[ordered_columns]\n",
    "        data_aux = data_aux.reindex_axis(ordered_columns, axis=1)\n",
    "        \n",
    "        if first:\n",
    "            data = data_aux.copy()\n",
    "            first = False\n",
    "        else:\n",
    "            data = data.append(data_aux, ignore_index=True)\n",
    "\n",
    "        data_aux = None\n",
    "    \n",
    "    print 'Finished with ' + dir_name\n",
    "    \n",
    "data.to_csv(csv_data_path + 'new_york_times' + '.csv', index=False, encoding='utf-8')\n",
    "\n",
    "data[ordered_no_text_columns].to_csv(csv_data_path + 'new_york_times_no_text' + '.csv', index=False, encoding='utf-8')\n",
    "\n",
    "print 'FINISHED!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.read_csv('data/files_new_york_times/new_york_times_no_text.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataFrame = pd.read_csv('data/files_new_york_times/csv_url/startup.csv', encoding='utf-8')\n",
    "count = 0\n",
    "for result in dataFrame['url_works']:\n",
    "    if result == 1:\n",
    "        count = count + 1\n",
    "print count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
