{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Preparing Data\n",
    "=============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# import cPickle as pickle\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import factorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bossa/control_tasks.json       bossa/tasks_export.json\r\n",
      "bossa/control_tasks_runs.json  bossa/tasks_runs_export.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls bossa/*json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOSSA Results\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing `results_bossa.json` to get a *dictionary* with keys the task ids, and values in as the average value of the scores. To do that, we first convert scores from categorical (`neg`, `neu`, `pos`) to a numeric scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result_id</th>\n",
       "      <th>seconds</th>\n",
       "      <th>task_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>11203</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>52775</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    result_id   seconds  task_id score\n",
       "50      11203  0.000025    52775     1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bossa_results = pd.read_json(\"bossa/tasks_runs_export.json\")\n",
    "bossa_results.rename(columns={\"created\": \"start_time\", \"id\": \"result_id\", \"info\": \"score\"}, inplace=True)\n",
    "bossa_results[['start_time']]= bossa_results[['start_time']].apply(pd.to_datetime, dayfirst=True)\n",
    "bossa_results[['finish_time']]= bossa_results[['finish_time']].apply(pd.to_datetime, dayfirst=True)\n",
    "bossa_results['score'] = pd.Categorical(bossa_results['score'], categories=['vneg', 'neg', 'neu', 'pos', 'vpos'])\n",
    "bossa_results['score'].cat.rename_categories([-2, -1, 0, 1, 2], inplace=True)\n",
    "# Normalize everything to -1, 0, 1\n",
    "# bossa_results['score'] = bossa_results['score'].astype(float).apply(lambda x: -1 if x < 0 else 1 if x > 0 else 0)\n",
    "bossa_results[\"seconds\"] = (bossa_results[\"finish_time\"] - bossa_results[\"start_time\"]).astype('timedelta64[us]') / 1e6\n",
    "bossa_results = bossa_results[[\"result_id\", \"seconds\", \"task_id\", \"score\"]]\n",
    "bossa_results.ix[[50]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information about the sentence comes in a dictionary inside the cells of the serie `info`, so we expand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_id</th>\n",
       "      <th>info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>52851</td>\n",
       "      <td>{'url': 'http://www.nytimes.com/2013/02/22/art...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    task_id                                               info\n",
       "50    52851  {'url': 'http://www.nytimes.com/2013/02/22/art..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bossa_tasks = pd.read_json(\"bossa/tasks_export.json\")\n",
    "bossa_tasks[['created']]= bossa_tasks[['created']].apply(pd.to_datetime, dayfirst=True)\n",
    "bossa_tasks.rename(columns={'id': 'task_id'}, inplace=True)\n",
    "bossa_tasks = bossa_tasks[['task_id', 'info']]\n",
    "bossa_tasks.ix[[50]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we merge the `DataFrame` with the scores with the one containing the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result_id</th>\n",
       "      <th>seconds</th>\n",
       "      <th>task_id</th>\n",
       "      <th>score</th>\n",
       "      <th>info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>11195</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>52776</td>\n",
       "      <td>2</td>\n",
       "      <td>{'url': 'http://dealbook.nytimes.com/2013/05/1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    result_id   seconds  task_id  score  \\\n",
       "50      11195  0.000021    52776      2   \n",
       "\n",
       "                                                 info  \n",
       "50  {'url': 'http://dealbook.nytimes.com/2013/05/1...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bossa_tasks_scores = pd.merge(bossa_results, bossa_tasks, on='task_id')\n",
    "bossa_tasks_scores.ix[[50]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now expand the column `info` into as many new columns as keys has the dictionary `info`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['url', 'search_words', 'pub_date', 'text', 'appears_in_sentence', 'sentence', 'sentence_id', 'is_company', 'noun_phrases', 'appears_in_noun_phrases', 'media'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bossa_tasks_scores.ix[50].info.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result_id</th>\n",
       "      <th>seconds</th>\n",
       "      <th>task_id</th>\n",
       "      <th>score</th>\n",
       "      <th>url</th>\n",
       "      <th>search_words</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>text</th>\n",
       "      <th>appears_in_sentence</th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>is_company</th>\n",
       "      <th>noun_phrases</th>\n",
       "      <th>appears_in_noun_phrases</th>\n",
       "      <th>media</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>11195</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>52776</td>\n",
       "      <td>2</td>\n",
       "      <td>http://dealbook.nytimes.com/2013/05/17/a-toeho...</td>\n",
       "      <td>executive</td>\n",
       "      <td>2013-05-17T11:47:51Z</td>\n",
       "      <td>Chinese investors are increasingly opting to b...</td>\n",
       "      <td>0</td>\n",
       "      <td>Chinese investors are increasingly opting to b...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>[chinese investors, overseas companies, politi...</td>\n",
       "      <td>0</td>\n",
       "      <td>nyt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>11205</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>52776</td>\n",
       "      <td>-1</td>\n",
       "      <td>http://dealbook.nytimes.com/2013/05/17/a-toeho...</td>\n",
       "      <td>executive</td>\n",
       "      <td>2013-05-17T11:47:51Z</td>\n",
       "      <td>Chinese investors are increasingly opting to b...</td>\n",
       "      <td>0</td>\n",
       "      <td>Chinese investors are increasingly opting to b...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>[chinese investors, overseas companies, politi...</td>\n",
       "      <td>0</td>\n",
       "      <td>nyt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>11207</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>52776</td>\n",
       "      <td>1</td>\n",
       "      <td>http://dealbook.nytimes.com/2013/05/17/a-toeho...</td>\n",
       "      <td>executive</td>\n",
       "      <td>2013-05-17T11:47:51Z</td>\n",
       "      <td>Chinese investors are increasingly opting to b...</td>\n",
       "      <td>0</td>\n",
       "      <td>Chinese investors are increasingly opting to b...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>[chinese investors, overseas companies, politi...</td>\n",
       "      <td>0</td>\n",
       "      <td>nyt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>11209</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>52776</td>\n",
       "      <td>-2</td>\n",
       "      <td>http://dealbook.nytimes.com/2013/05/17/a-toeho...</td>\n",
       "      <td>executive</td>\n",
       "      <td>2013-05-17T11:47:51Z</td>\n",
       "      <td>Chinese investors are increasingly opting to b...</td>\n",
       "      <td>0</td>\n",
       "      <td>Chinese investors are increasingly opting to b...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>[chinese investors, overseas companies, politi...</td>\n",
       "      <td>0</td>\n",
       "      <td>nyt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    result_id   seconds  task_id  score  \\\n",
       "50      11195  0.000021    52776      2   \n",
       "51      11205  0.000018    52776     -1   \n",
       "52      11207  0.000017    52776      1   \n",
       "53      11209  0.000017    52776     -2   \n",
       "\n",
       "                                                  url search_words  \\\n",
       "50  http://dealbook.nytimes.com/2013/05/17/a-toeho...    executive   \n",
       "51  http://dealbook.nytimes.com/2013/05/17/a-toeho...    executive   \n",
       "52  http://dealbook.nytimes.com/2013/05/17/a-toeho...    executive   \n",
       "53  http://dealbook.nytimes.com/2013/05/17/a-toeho...    executive   \n",
       "\n",
       "                pub_date                                               text  \\\n",
       "50  2013-05-17T11:47:51Z  Chinese investors are increasingly opting to b...   \n",
       "51  2013-05-17T11:47:51Z  Chinese investors are increasingly opting to b...   \n",
       "52  2013-05-17T11:47:51Z  Chinese investors are increasingly opting to b...   \n",
       "53  2013-05-17T11:47:51Z  Chinese investors are increasingly opting to b...   \n",
       "\n",
       "   appears_in_sentence                                           sentence  \\\n",
       "50                   0  Chinese investors are increasingly opting to b...   \n",
       "51                   0  Chinese investors are increasingly opting to b...   \n",
       "52                   0  Chinese investors are increasingly opting to b...   \n",
       "53                   0  Chinese investors are increasingly opting to b...   \n",
       "\n",
       "   sentence_id is_company                                       noun_phrases  \\\n",
       "50          14          0  [chinese investors, overseas companies, politi...   \n",
       "51          14          0  [chinese investors, overseas companies, politi...   \n",
       "52          14          0  [chinese investors, overseas companies, politi...   \n",
       "53          14          0  [chinese investors, overseas companies, politi...   \n",
       "\n",
       "   appears_in_noun_phrases media  \n",
       "50                       0   nyt  \n",
       "51                       0   nyt  \n",
       "52                       0   nyt  \n",
       "53                       0   nyt  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def json_to_series(info):\n",
    "#     keys, values = zip(*info.iteritems())\n",
    "    keys, values = zip(*info.items())\n",
    "    return pd.Series(values, index=keys)\n",
    "\n",
    "bossa_info = bossa_tasks_scores[\"info\"].apply(json_to_series)\n",
    "bossa_info.reset_index()\n",
    "bossa = pd.concat([bossa_tasks_scores, bossa_info], axis=1)\n",
    "bossa.pop(\"info\")\n",
    "# bossa['id'] = bossa['id'].astype(float)\n",
    "bossa.ix[50:53]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now aggregate calculating the average per `sentence_id` using a group by. In the process, we lose the source of the data, that's why we first have to save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bossa.to_csv(\"sentiment/scores_ungrouped.csv\", encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we aggregate and create a new `DataFrame` for the different sentences and their score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score    8996\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'We must hope after so much prevarication that this time Google's proposals represent a genuine attempt to address the concerns identified,' said David Wood, the legal counsel for Icomp, an industry group backed by Microsoft and a number of other companies.</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'We must push our leaders to step up and commit to action,' said Hugh Evans, the founder and chief executive of the charity.</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'We need them to tell the story of how we are making decisions and putting the organization together,' said George Postolos, the Astros' president and chief executive, who added that the team would not want a broadcaster who was uncomfortable explaining the front office's strategy.</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                            score\n",
       "sentence                                                                                                                                                                                                                                                                                         \n",
       "'We must hope after so much prevarication that this time Google's proposals represent a genuine attempt to address the concerns identified,' said David Wood, the legal counsel for Icomp, an industry group backed by Microsoft and a number of other companies.                               0\n",
       "'We must push our leaders to step up and commit to action,' said Hugh Evans, the founder and chief executive of the charity.                                                                                                                                                                   -1\n",
       "'We need them to tell the story of how we are making decisions and putting the organization together,' said George Postolos, the Astros' president and chief executive, who added that the team would not want a broadcaster who was uncomfortable explaining the front office's strategy.     -1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def majority(series): #receives a Pandas Series\n",
    "    c = Counter(map(lambda x: 1 if x > 0 else -1 if x < 0 else 0, series))\n",
    "    commons = c.most_common()\n",
    "    if len(commons) > 1 and commons[0][1] == commons[1][1]:\n",
    "        value = -9\n",
    "    else:\n",
    "        value = commons[0][0]\n",
    "    return value\n",
    "#     return Counter(map(lambda x: 1 if x > 0 else -1 if x < 0 else 0, series)).most_common(1)[0][0]\n",
    "\n",
    "# score_calculate = np.average\n",
    "score_calculate = majority\n",
    "\n",
    "sentences = bossa.groupby(['sentence'])[['score']].aggregate(score_calculate)\n",
    "sentences.to_csv(\"sentiment/scores.csv\", encoding=\"utf8\")\n",
    "print(sentences.count())\n",
    "sentences[1001:1004]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the tranining and testing sets (data and labels) from a randomized version of the set of assessed sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    3991\n",
       "-1    2193\n",
       "-9    1530\n",
       " 0    1282\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.reset_index()['score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence    8996\n",
       "score       8996\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.reset_index().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# raw_scores = sentences.reset_index()\n",
    "# scores = raw_scores\n",
    "# print('Zero:', len(scores[scores.score==0]))\n",
    "# print('Non-zero:', len(scores[scores.score!=0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could consider 3 classes, but it toruns out that using binary classficication seems to produce better results. Still, try multi-classs classifiers is something worth trying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_scores = sentences.reset_index()\n",
    "scores = raw_scores\n",
    "scores = scores[scores.score!=0]  # We ignore the neutral sentences\n",
    "scores = scores[scores.score!=-9]  # And ambiguous sentences\n",
    "scores['sentiment'] = scores['score'].apply(lambda s: 'pos' if s > 0 else 'neg')\n",
    "# scores['sentiment'] = scores['score'].apply(lambda s: 'pos' if s > 0 else 'neg' if s < 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences_df = scores[['sentence', 'sentiment']].reset_index(drop=True)\n",
    "# sentences_df = sentences_df.reindex(np.random.permutation(sentences_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>General Motors will recall nearly 3,200 manua...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\" And Aim's problem is that many of its larges...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"'Don't be evil,' he cried, while being chased...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"A lot of companies seem to prefer it to other...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"An entrepreneur may well be unreasonable beca...</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence sentiment\n",
       "0   General Motors will recall nearly 3,200 manua...       neg\n",
       "1  \" And Aim's problem is that many of its larges...       neg\n",
       "2  \"'Don't be evil,' he cried, while being chased...       neg\n",
       "3  \"A lot of companies seem to prefer it to other...       pos\n",
       "4  \"An entrepreneur may well be unreasonable beca...       neg"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#SCIKIT-LEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold,train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier, Perceptron, RidgeClassifier, SGDClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.utils.extmath import density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# categories = [\n",
    "#     'pos',\n",
    "#     'neg',\n",
    "# ]\n",
    "# print(\"Loading sentences for categories:\")\n",
    "# print(categories)\n",
    "# data_train, data_test = train_test_split(document_df, train_size=0.9, test_size=0.1, random_state=100)\n",
    "# print('Data loaded:')\n",
    "# print('Train set: {} samples'.format(len(data_train)))\n",
    "# print('Test set: {} samples'.format(len(data_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def size_mb(docs):\n",
    "    return sum(len(s.encode('utf-8')) for s in docs) / 1e6\n",
    "\n",
    "# data_train_size_mb = size_mb(data_train['sentence'])\n",
    "# data_test_size_mb = size_mb(data_test['sentence'])\n",
    "\n",
    "# print(\"%d documents - %0.3fMB (training set)\" % (\n",
    "#     len(data_train), data_train_size_mb))\n",
    "# print(\"%d documents - %0.3fMB (test set)\" % (\n",
    "#     len(data_test), data_test_size_mb))\n",
    "# print(\"%d categories\" % len(categories))\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Benchmark classifiers\n",
    "def benchmark(name, clf, X_train, X_test, y_train, y_test):\n",
    "    print(\"_\" * 80)\n",
    "    print(\"Training:\", name)\n",
    "    print(clf)\n",
    "    \n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "\n",
    "#     precision = metrics.precision_score(y_test, pred)\n",
    "#     print(\"precision:   %0.3f\" % score)\n",
    "\n",
    "#     recall = metrics.recall_score(y_test, pred)\n",
    "#     print(\"recall:   %0.3f\" % score)\n",
    "\n",
    "    print(\"classification report:\")\n",
    "    print(metrics.classification_report(y_test, pred))\n",
    "\n",
    "    print(\"confusion matrix:\")\n",
    "    print(metrics.confusion_matrix(y_test, pred))\n",
    "\n",
    "    print()\n",
    "    clf_descr = str(clf).split(\"(\")[0]\n",
    "    return score#, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Classifiers\n",
    "# clf_list = [\n",
    "#     (RidgeClassifier(tol=1e-2, solver=\"lsqr\"), \"Ridge classifier\"),\n",
    "# #     (Perceptron(n_iter=50), \"Perceptron\"),\n",
    "# #     (PassiveAggressiveClassifier(n_iter=50), \"Passive-aggressive\"),\n",
    "# #     (KNeighborsClassifier(n_neighbors=10), \"kNN\"),\n",
    "# #     (RandomForestClassifier(n_estimators=100), \"Random forest\")\n",
    "# ]\n",
    "\n",
    "# Classifiers\n",
    "clf_list = [\n",
    "    (RidgeClassifier(alpha=.00001, tol=1e-2, solver=\"lsqr\"), \"Ridge classifier\"),\n",
    "    (Perceptron(alpha=.00001, n_iter=50), \"Perceptron\"),\n",
    "    (PassiveAggressiveClassifier(n_iter=50), \"Passive-aggressive\"),\n",
    "    (KNeighborsClassifier(n_neighbors=10), \"kNN\"),\n",
    "    (RandomForestClassifier(n_estimators=100), \"Random forest\"),\n",
    "    (LinearSVC(loss='squared_hinge', penalty='l2', dual=False, tol=1e-3), 'Linear SVC'),\n",
    "    (SGDClassifier(alpha=.000001, n_iter=50, penalty='l1'), 'SGDClassifier'),\n",
    "    (NearestCentroid(), 'Nearest Centroid'),\n",
    "    (MultinomialNB(alpha=.00001), 'Multinomial NB'),\n",
    "    (BernoulliNB(alpha=.00001), 'Bernoulli NB'),\n",
    "    (LinearSVC(penalty=\"l1\", dual=False, tol=1e-3), 'Linear SVC'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with open('sentiment/myclf.pickle', 'wb') as myclf_file:\n",
    "#     pickle.dump(pipeline, myclf_file, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# with open('sentiment/myclf.pickle', 'rb') as myclf_file:\n",
    "#     my_clf = pickle.load(myclf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Params\n",
    "K = 10\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__max_features': (None, 1000, 2000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2), (1, 3)),\n",
    "    'vect__stop_words': (None, stopwords.words('english')),\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'clf__metric': ('euclidean', 'manhattan'),\n",
    "#     'clf__shrink_threshold': (None, 0.1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 216 candidates, totalling 2160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done  50 jobs       | elapsed:   18.9s\n",
      "[Parallel(n_jobs=-1)]: Done 200 jobs       | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 450 jobs       | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 800 jobs       | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1250 jobs       | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1800 jobs       | elapsed: 12.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2160 out of 2160 | elapsed: 15.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "Best score: 0.659282018111\n",
      "\n",
      "Best parameters:\n",
      "{'tfidf__smooth_idf': True, 'vect__analyzer': 'word', 'vect__encoding': 'utf-8', 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now'], 'vect__ngram_range': (1, 3), 'tfidf__use_idf': True, 'vect__dtype': <class 'numpy.int64'>, 'tfidf__norm': 'l2', 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'vect__input': 'content', 'vect__lowercase': True, 'vect__decode_error': 'strict', 'tfidf__sublinear_tf': False, 'vect__min_df': 1, 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', '...'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now'],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'vect__max_features': None, 'vect__tokenizer': None, 'vect__vocabulary': None, 'vect__preprocessor': None, 'vect__binary': False, 'clf__metric': 'euclidean', 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'vect__max_df': 0.5, 'clf__shrink_threshold': None, 'clf': NearestCentroid(metric='euclidean', shrink_threshold=None), 'vect__strip_accents': None}\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "X = sentences_df['sentence']\n",
    "y = sentences_df['sentiment']\n",
    "\n",
    "skf = StratifiedKFold(y, K)\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', NearestCentroid()),\n",
    "])\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, cv=skf)\n",
    "grid_search.fit(X, y)\n",
    "bs = grid_search.best_score_\n",
    "be = grid_search.best_estimator_\n",
    "\n",
    "print('*'*40)\n",
    "print('Best score:', bs)\n",
    "print()\n",
    "print('Best parameters:')\n",
    "print(be.get_params())\n",
    "print('*'*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('sentiment/gs.pickle', 'wb') as gs_file:\n",
    "    pickle.dump(grid_search, gs_file, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('sentiment/be.pickle', 'wb') as be_file:\n",
    "    pickle.dump(be, be_file, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Training: NearestCentroid\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None,\n",
      "        stop_words=['i', 'me',...inear_tf=False, use_idf=True)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=None))])\n",
      "train time: 1.581s\n",
      "test time:  0.135s\n",
      "accuracy:   0.603\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.45      0.56      0.50       220\n",
      "        pos       0.72      0.63      0.67       400\n",
      "\n",
      "avg / total       0.63      0.60      0.61       620\n",
      "\n",
      "confusion matrix:\n",
      "[[123  97]\n",
      " [149 251]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: NearestCentroid\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None,\n",
      "        stop_words=['i', 'me',...inear_tf=False, use_idf=True)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=None))])\n",
      "train time: 1.592s\n",
      "test time:  0.136s\n",
      "accuracy:   0.651\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.51      0.50      0.50       220\n",
      "        pos       0.73      0.73      0.73       399\n",
      "\n",
      "avg / total       0.65      0.65      0.65       619\n",
      "\n",
      "confusion matrix:\n",
      "[[110 110]\n",
      " [106 293]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: NearestCentroid\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None,\n",
      "        stop_words=['i', 'me',...inear_tf=False, use_idf=True)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=None))])\n",
      "train time: 1.710s\n",
      "test time:  0.146s\n",
      "accuracy:   0.662\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.53      0.46      0.49       220\n",
      "        pos       0.72      0.77      0.75       399\n",
      "\n",
      "avg / total       0.65      0.66      0.66       619\n",
      "\n",
      "confusion matrix:\n",
      "[[101 119]\n",
      " [ 90 309]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: NearestCentroid\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None,\n",
      "        stop_words=['i', 'me',...inear_tf=False, use_idf=True)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=None))])\n",
      "train time: 1.698s\n",
      "test time:  0.158s\n",
      "accuracy:   0.646\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.50      0.54      0.52       219\n",
      "        pos       0.74      0.70      0.72       399\n",
      "\n",
      "avg / total       0.65      0.65      0.65       618\n",
      "\n",
      "confusion matrix:\n",
      "[[118 101]\n",
      " [118 281]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: NearestCentroid\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None,\n",
      "        stop_words=['i', 'me',...inear_tf=False, use_idf=True)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=None))])\n",
      "train time: 2.328s\n",
      "test time:  0.138s\n",
      "accuracy:   0.647\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.50      0.51      0.51       219\n",
      "        pos       0.73      0.72      0.73       399\n",
      "\n",
      "avg / total       0.65      0.65      0.65       618\n",
      "\n",
      "confusion matrix:\n",
      "[[112 107]\n",
      " [111 288]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: NearestCentroid\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None,\n",
      "        stop_words=['i', 'me',...inear_tf=False, use_idf=True)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=None))])\n",
      "train time: 1.561s\n",
      "test time:  0.134s\n",
      "accuracy:   0.644\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.50      0.54      0.52       219\n",
      "        pos       0.73      0.70      0.72       399\n",
      "\n",
      "avg / total       0.65      0.64      0.65       618\n",
      "\n",
      "confusion matrix:\n",
      "[[118 101]\n",
      " [119 280]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: NearestCentroid\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None,\n",
      "        stop_words=['i', 'me',...inear_tf=False, use_idf=True)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=None))])\n",
      "train time: 1.589s\n",
      "test time:  0.138s\n",
      "accuracy:   0.686\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.56      0.53      0.54       219\n",
      "        pos       0.75      0.77      0.76       399\n",
      "\n",
      "avg / total       0.68      0.69      0.68       618\n",
      "\n",
      "confusion matrix:\n",
      "[[116 103]\n",
      " [ 91 308]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: NearestCentroid\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None,\n",
      "        stop_words=['i', 'me',...inear_tf=False, use_idf=True)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=None))])\n",
      "train time: 1.572s\n",
      "test time:  0.140s\n",
      "accuracy:   0.672\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.54      0.52      0.53       219\n",
      "        pos       0.74      0.75      0.75       399\n",
      "\n",
      "avg / total       0.67      0.67      0.67       618\n",
      "\n",
      "confusion matrix:\n",
      "[[114 105]\n",
      " [ 98 301]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: NearestCentroid\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None,\n",
      "        stop_words=['i', 'me',...inear_tf=False, use_idf=True)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=None))])\n",
      "train time: 1.594s\n",
      "test time:  0.142s\n",
      "accuracy:   0.704\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.60      0.51      0.55       219\n",
      "        pos       0.75      0.81      0.78       399\n",
      "\n",
      "avg / total       0.70      0.70      0.70       618\n",
      "\n",
      "confusion matrix:\n",
      "[[112 107]\n",
      " [ 76 323]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: NearestCentroid\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None,\n",
      "        stop_words=['i', 'me',...inear_tf=False, use_idf=True)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=None))])\n",
      "train time: 1.620s\n",
      "test time:  0.136s\n",
      "accuracy:   0.678\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.55      0.52      0.53       219\n",
      "        pos       0.74      0.76      0.75       399\n",
      "\n",
      "avg / total       0.67      0.68      0.68       618\n",
      "\n",
      "confusion matrix:\n",
      "[[114 105]\n",
      " [ 94 305]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for train_index, test_index in StratifiedKFold(y, K):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    benchmark_result = benchmark('NearestCentroid', be, X_train, X_test, y_train, y_test)\n",
    "    results.append(benchmark_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Training: NearestCentroid\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
      "        ...inear_tf=False, use_idf=True)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=None))])\n",
      "train time: 0.776s\n",
      "test time:  0.082s\n",
      "accuracy:   0.631\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.45      0.46      0.45       221\n",
      "        pos       0.72      0.72      0.72       438\n",
      "\n",
      "avg / total       0.63      0.63      0.63       659\n",
      "\n",
      "confusion matrix:\n",
      "[[101 120]\n",
      " [123 315]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "X = sentences_df['sentence']\n",
    "y = sentences_df['sentiment']\n",
    "X_data, X_val, y_data, y_val = train_test_split(X, y, train_size=0.9, test_size=0.1, random_state=100)\n",
    "X_data = X_data.reset_index(drop=True)\n",
    "X_val = X_val.reset_index(drop=True)\n",
    "y_data = y_data.reset_index(drop=True)\n",
    "y_val = y_val.reset_index(drop=True)\n",
    "for train_index, test_index in StratifiedKFold(y_data, K):\n",
    "    X_train, X_test = X_data[train_index], X_data[test_index]\n",
    "    y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer(ngram_range=(1, 2), max_df=1.0, stop_words='english')),\n",
    "        ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "        ('clf', NearestCentroid(metric='euclidean')),\n",
    "    ])\n",
    "#     grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, cv=StratifiedKFold(y_data, K))\n",
    "#     grid_search.fit(X_train, y_train)\n",
    "#     be = grid_search.best_estimator_\n",
    "#     print('*'*40)\n",
    "#     print(be.get_params())\n",
    "#     print('*'*40)\n",
    "    benchmark_results = benchmark('NearestCentroid', pipeline, X_train, X_test, y_train, y_test)\n",
    "    \n",
    "#     results.append((be, grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 540 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done  50 jobs       | elapsed:   13.0s\n",
      "[Parallel(n_jobs=-1)]: Done 200 jobs       | elapsed:   47.4s\n",
      "[Parallel(n_jobs=-1)]: Done 450 jobs       | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 800 jobs       | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1250 jobs       | elapsed:  6.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1620 out of 1620 | elapsed:  8.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "{'vect__lowercase': True, 'vect__decode_error': 'strict', 'vect__tokenizer': None, 'tfidf__norm': 'l2', 'vect__ngram_range': (1, 1), 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tfidf__sublinear_tf': False, 'vect__analyzer': 'word', 'clf__shrink_threshold': None, 'tfidf__smooth_idf': True, 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'vect__max_df': 0.75, 'vect__strip_accents': None, 'vect__dtype': <class 'numpy.int64'>, 'clf': NearestCentroid(metric='l1', shrink_threshold=None), 'vect__vocabulary': None, 'vect__min_df': 1, 'clf__metric': 'l1', 'vect__input': 'content', 'vect__stop_words': None, 'vect__encoding': 'utf-8', 'tfidf__use_idf': True, 'vect__max_features': 1000, 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'vect__preprocessor': None, 'vect__binary': False}\n",
      "****************************************\n",
      "________________________________________________________________________________\n",
      "Training: NearestCentroid\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        stri...e, sublinear_tf=False, use_idf=True)), ('clf', NearestCentroid(metric='l1', shrink_threshold=None))])\n",
      "train time: 0.220s\n",
      "test time:  0.026s\n",
      "accuracy:   0.668\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.60      0.02      0.03       198\n",
      "        pos       0.67      0.99      0.80       395\n",
      "\n",
      "avg / total       0.65      0.67      0.54       593\n",
      "\n",
      "confusion matrix:\n",
      "[[  3 195]\n",
      " [  2 393]]\n",
      "\n",
      "Fitting 3 folds for each of 540 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  50 jobs       | elapsed:   12.3s\n",
      "[Parallel(n_jobs=-1)]: Done 200 jobs       | elapsed:   46.0s\n",
      "[Parallel(n_jobs=-1)]: Done 450 jobs       | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 800 jobs       | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1250 jobs       | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1620 out of 1620 | elapsed:  8.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "{'vect__lowercase': True, 'vect__decode_error': 'strict', 'vect__tokenizer': None, 'tfidf__norm': 'l2', 'vect__ngram_range': (1, 1), 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tfidf__sublinear_tf': False, 'vect__analyzer': 'word', 'clf__shrink_threshold': None, 'tfidf__smooth_idf': True, 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'vect__max_df': 0.75, 'vect__strip_accents': None, 'vect__dtype': <class 'numpy.int64'>, 'clf': NearestCentroid(metric='l1', shrink_threshold=None), 'vect__vocabulary': None, 'vect__min_df': 1, 'clf__metric': 'l1', 'vect__input': 'content', 'vect__stop_words': None, 'vect__encoding': 'utf-8', 'tfidf__use_idf': True, 'vect__max_features': 1000, 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'vect__preprocessor': None, 'vect__binary': False}\n",
      "****************************************\n",
      "________________________________________________________________________________\n",
      "Training: NearestCentroid\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.75, max_features=1000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        stri...e, sublinear_tf=False, use_idf=True)), ('clf', NearestCentroid(metric='l1', shrink_threshold=None))])\n",
      "train time: 0.220s\n",
      "test time:  0.039s\n",
      "accuracy:   0.675\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.69      0.05      0.09       198\n",
      "        pos       0.67      0.99      0.80       395\n",
      "\n",
      "avg / total       0.68      0.67      0.56       593\n",
      "\n",
      "confusion matrix:\n",
      "[[  9 189]\n",
      " [  4 391]]\n",
      "\n",
      "Fitting 3 folds for each of 540 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  50 jobs       | elapsed:   12.4s\n",
      "[Parallel(n_jobs=-1)]: Done 200 jobs       | elapsed:   46.6s\n",
      "[Parallel(n_jobs=-1)]: Done 450 jobs       | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 800 jobs       | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1250 jobs       | elapsed:  6.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1620 out of 1620 | elapsed:  8.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "{'vect__lowercase': True, 'vect__decode_error': 'strict', 'vect__tokenizer': None, 'tfidf__norm': 'l2', 'vect__ngram_range': (1, 1), 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tfidf__sublinear_tf': False, 'vect__analyzer': 'word', 'clf__shrink_threshold': None, 'tfidf__smooth_idf': True, 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'vect__max_df': 1.0, 'vect__strip_accents': None, 'vect__dtype': <class 'numpy.int64'>, 'clf': NearestCentroid(metric='l1', shrink_threshold=None), 'vect__vocabulary': None, 'vect__min_df': 1, 'clf__metric': 'l1', 'vect__input': 'content', 'vect__stop_words': None, 'vect__encoding': 'utf-8', 'tfidf__use_idf': True, 'vect__max_features': 1000, 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'vect__preprocessor': None, 'vect__binary': False}\n",
      "****************************************\n",
      "________________________________________________________________________________\n",
      "Training: NearestCentroid\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip...e, sublinear_tf=False, use_idf=True)), ('clf', NearestCentroid(metric='l1', shrink_threshold=None))])\n",
      "train time: 0.212s\n",
      "test time:  0.026s\n",
      "accuracy:   0.671\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.67      0.02      0.04       197\n",
      "        pos       0.67      0.99      0.80       395\n",
      "\n",
      "avg / total       0.67      0.67      0.55       592\n",
      "\n",
      "confusion matrix:\n",
      "[[  4 193]\n",
      " [  2 393]]\n",
      "\n",
      "Fitting 3 folds for each of 540 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  50 jobs       | elapsed:   12.5s\n",
      "[Parallel(n_jobs=-1)]: Done 200 jobs       | elapsed:   45.8s\n",
      "[Parallel(n_jobs=-1)]: Done 450 jobs       | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 800 jobs       | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1250 jobs       | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1620 out of 1620 | elapsed:  8.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "{'vect__lowercase': True, 'vect__decode_error': 'strict', 'vect__tokenizer': None, 'tfidf__norm': 'l2', 'vect__ngram_range': (1, 2), 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tfidf__sublinear_tf': False, 'vect__analyzer': 'word', 'clf__shrink_threshold': None, 'tfidf__smooth_idf': True, 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'vect__max_df': 1.0, 'vect__strip_accents': None, 'vect__dtype': <class 'numpy.int64'>, 'clf': NearestCentroid(metric='l1', shrink_threshold=None), 'vect__vocabulary': None, 'vect__min_df': 1, 'clf__metric': 'l1', 'vect__input': 'content', 'vect__stop_words': None, 'vect__encoding': 'utf-8', 'tfidf__use_idf': True, 'vect__max_features': 1000, 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'vect__preprocessor': None, 'vect__binary': False}\n",
      "****************************************\n",
      "________________________________________________________________________________\n",
      "Training: NearestCentroid\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip...e, sublinear_tf=False, use_idf=True)), ('clf', NearestCentroid(metric='l1', shrink_threshold=None))])\n",
      "train time: 0.810s\n",
      "test time:  0.051s\n",
      "accuracy:   0.667\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.50      0.02      0.03       197\n",
      "        pos       0.67      0.99      0.80       395\n",
      "\n",
      "avg / total       0.61      0.67      0.54       592\n",
      "\n",
      "confusion matrix:\n",
      "[[  3 194]\n",
      " [  3 392]]\n",
      "\n",
      "Fitting 3 folds for each of 540 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  50 jobs       | elapsed:   12.5s\n",
      "[Parallel(n_jobs=-1)]: Done 200 jobs       | elapsed:   46.3s\n",
      "[Parallel(n_jobs=-1)]: Done 450 jobs       | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 800 jobs       | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1250 jobs       | elapsed:  6.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1606 out of 1620 | elapsed:  8.0min remaining:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1620 out of 1620 | elapsed:  8.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "{'vect__lowercase': True, 'vect__decode_error': 'strict', 'vect__tokenizer': None, 'tfidf__norm': 'l2', 'vect__ngram_range': (1, 1), 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tfidf__sublinear_tf': False, 'vect__analyzer': 'word', 'clf__shrink_threshold': None, 'tfidf__smooth_idf': True, 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'vect__max_df': 1.0, 'vect__strip_accents': None, 'vect__dtype': <class 'numpy.int64'>, 'clf': NearestCentroid(metric='l1', shrink_threshold=None), 'vect__vocabulary': None, 'vect__min_df': 1, 'clf__metric': 'l1', 'vect__input': 'content', 'vect__stop_words': None, 'vect__encoding': 'utf-8', 'tfidf__use_idf': True, 'vect__max_features': 1000, 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'vect__preprocessor': None, 'vect__binary': False}\n",
      "****************************************\n",
      "________________________________________________________________________________\n",
      "Training: NearestCentroid\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=1000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip...e, sublinear_tf=False, use_idf=True)), ('clf', NearestCentroid(metric='l1', shrink_threshold=None))])\n",
      "train time: 0.219s\n",
      "test time:  0.026s\n",
      "accuracy:   0.674\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.61      0.06      0.10       197\n",
      "        pos       0.68      0.98      0.80       395\n",
      "\n",
      "avg / total       0.65      0.67      0.57       592\n",
      "\n",
      "confusion matrix:\n",
      "[[ 11 186]\n",
      " [  7 388]]\n",
      "\n",
      "Fitting 3 folds for each of 540 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done  50 jobs       | elapsed:   12.6s\n",
      "[Parallel(n_jobs=-1)]: Done 200 jobs       | elapsed:   46.2s\n",
      "[Parallel(n_jobs=-1)]: Done 450 jobs       | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 800 jobs       | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1250 jobs       | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1606 out of 1620 | elapsed:  8.0min remaining:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1620 out of 1620 | elapsed:  8.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "{'vect__lowercase': True, 'vect__decode_error': 'strict', 'vect__tokenizer': None, 'tfidf__norm': 'l2', 'vect__ngram_range': (1, 3), 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tfidf__sublinear_tf': False, 'vect__analyzer': 'word', 'clf__shrink_threshold': None, 'tfidf__smooth_idf': True, 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'vect__max_df': 0.5, 'vect__strip_accents': None, 'vect__dtype': <class 'numpy.int64'>, 'clf': NearestCentroid(metric='l1', shrink_threshold=None), 'vect__vocabulary': None, 'vect__min_df': 1, 'clf__metric': 'l1', 'vect__input': 'content', 'vect__stop_words': None, 'vect__encoding': 'utf-8', 'tfidf__use_idf': True, 'vect__max_features': 2000, 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=2000, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'vect__preprocessor': None, 'vect__binary': False}\n",
      "****************************************\n",
      "________________________________________________________________________________\n",
      "Training: NearestCentroid\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=2000, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip...e, sublinear_tf=False, use_idf=True)), ('clf', NearestCentroid(metric='l1', shrink_threshold=None))])\n",
      "train time: 1.573s\n",
      "test time:  0.072s\n",
      "accuracy:   0.672\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.57      0.07      0.12       197\n",
      "        pos       0.68      0.97      0.80       395\n",
      "\n",
      "avg / total       0.64      0.67      0.57       592\n",
      "\n",
      "confusion matrix:\n",
      "[[ 13 184]\n",
      " [ 10 385]]\n",
      "\n",
      "Fitting 3 folds for each of 540 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  50 jobs       | elapsed:   12.6s\n",
      "[Parallel(n_jobs=-1)]: Done 200 jobs       | elapsed:   46.4s\n",
      "[Parallel(n_jobs=-1)]: Done 450 jobs       | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 800 jobs       | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1250 jobs       | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1620 out of 1620 | elapsed:  8.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "{'vect__lowercase': True, 'vect__decode_error': 'strict', 'vect__tokenizer': None, 'tfidf__norm': 'l2', 'vect__ngram_range': (1, 1), 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tfidf__sublinear_tf': False, 'vect__analyzer': 'word', 'clf__shrink_threshold': None, 'tfidf__smooth_idf': True, 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'vect__max_df': 0.5, 'vect__strip_accents': None, 'vect__dtype': <class 'numpy.int64'>, 'clf': NearestCentroid(metric='l1', shrink_threshold=None), 'vect__vocabulary': None, 'vect__min_df': 1, 'clf__metric': 'l1', 'vect__input': 'content', 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now'], 'vect__encoding': 'utf-8', 'tfidf__use_idf': True, 'vect__max_features': 2000, 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=2000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', '...'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now'],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'vect__preprocessor': None, 'vect__binary': False}\n",
      "****************************************\n",
      "________________________________________________________________________________\n",
      "Training: NearestCentroid\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=2000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None,\n",
      "        stop_words=['i', 'me',...e, sublinear_tf=False, use_idf=True)), ('clf', NearestCentroid(metric='l1', shrink_threshold=None))])\n",
      "train time: 0.494s\n",
      "test time:  0.063s\n",
      "accuracy:   0.682\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.71      0.08      0.14       197\n",
      "        pos       0.68      0.98      0.81       395\n",
      "\n",
      "avg / total       0.69      0.68      0.58       592\n",
      "\n",
      "confusion matrix:\n",
      "[[ 15 182]\n",
      " [  6 389]]\n",
      "\n",
      "Fitting 3 folds for each of 540 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    0.6s\n",
      "[Parallel(n_jobs=-1)]: Done  50 jobs       | elapsed:   12.9s\n",
      "[Parallel(n_jobs=-1)]: Done 200 jobs       | elapsed:   46.3s\n",
      "[Parallel(n_jobs=-1)]: Done 450 jobs       | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 800 jobs       | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1250 jobs       | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1606 out of 1620 | elapsed:  8.0min remaining:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1620 out of 1620 | elapsed:  8.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "{'vect__lowercase': True, 'vect__decode_error': 'strict', 'vect__tokenizer': None, 'tfidf__norm': 'l2', 'vect__ngram_range': (1, 2), 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tfidf__sublinear_tf': False, 'vect__analyzer': 'word', 'clf__shrink_threshold': None, 'tfidf__smooth_idf': True, 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'vect__max_df': 0.5, 'vect__strip_accents': None, 'vect__dtype': <class 'numpy.int64'>, 'clf': NearestCentroid(metric='l1', shrink_threshold=None), 'vect__vocabulary': None, 'vect__min_df': 1, 'clf__metric': 'l1', 'vect__input': 'content', 'vect__stop_words': None, 'vect__encoding': 'utf-8', 'tfidf__use_idf': True, 'vect__max_features': 1000, 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'vect__preprocessor': None, 'vect__binary': False}\n",
      "****************************************\n",
      "________________________________________________________________________________\n",
      "Training: NearestCentroid\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip...e, sublinear_tf=False, use_idf=True)), ('clf', NearestCentroid(metric='l1', shrink_threshold=None))])\n",
      "train time: 0.825s\n",
      "test time:  0.055s\n",
      "accuracy:   0.681\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.70      0.07      0.13       197\n",
      "        pos       0.68      0.98      0.80       395\n",
      "\n",
      "avg / total       0.69      0.68      0.58       592\n",
      "\n",
      "confusion matrix:\n",
      "[[ 14 183]\n",
      " [  6 389]]\n",
      "\n",
      "Fitting 3 folds for each of 540 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  50 jobs       | elapsed:   12.5s\n",
      "[Parallel(n_jobs=-1)]: Done 200 jobs       | elapsed:   46.1s\n",
      "[Parallel(n_jobs=-1)]: Done 450 jobs       | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 800 jobs       | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1250 jobs       | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1620 out of 1620 | elapsed:  8.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "{'vect__lowercase': True, 'vect__decode_error': 'strict', 'vect__tokenizer': None, 'tfidf__norm': 'l2', 'vect__ngram_range': (1, 1), 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tfidf__sublinear_tf': False, 'vect__analyzer': 'word', 'clf__shrink_threshold': None, 'tfidf__smooth_idf': True, 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'vect__max_df': 0.5, 'vect__strip_accents': None, 'vect__dtype': <class 'numpy.int64'>, 'clf': NearestCentroid(metric='l1', shrink_threshold=None), 'vect__vocabulary': None, 'vect__min_df': 1, 'clf__metric': 'l1', 'vect__input': 'content', 'vect__stop_words': None, 'vect__encoding': 'utf-8', 'tfidf__use_idf': True, 'vect__max_features': 1000, 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'vect__preprocessor': None, 'vect__binary': False}\n",
      "****************************************\n",
      "________________________________________________________________________________\n",
      "Training: NearestCentroid\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=1000, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip...e, sublinear_tf=False, use_idf=True)), ('clf', NearestCentroid(metric='l1', shrink_threshold=None))])\n",
      "train time: 0.213s\n",
      "test time:  0.027s\n",
      "accuracy:   0.677\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.71      0.05      0.09       197\n",
      "        pos       0.68      0.99      0.80       395\n",
      "\n",
      "avg / total       0.69      0.68      0.57       592\n",
      "\n",
      "confusion matrix:\n",
      "[[ 10 187]\n",
      " [  4 391]]\n",
      "\n",
      "Fitting 3 folds for each of 540 candidates, totalling 1620 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done  50 jobs       | elapsed:   12.7s\n",
      "[Parallel(n_jobs=-1)]: Done 200 jobs       | elapsed:   47.0s\n",
      "[Parallel(n_jobs=-1)]: Done 450 jobs       | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 800 jobs       | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1250 jobs       | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1606 out of 1620 | elapsed:  8.1min remaining:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1620 out of 1620 | elapsed:  8.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "{'vect__lowercase': True, 'vect__decode_error': 'strict', 'vect__tokenizer': None, 'tfidf__norm': 'l2', 'vect__ngram_range': (1, 3), 'vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tfidf__sublinear_tf': False, 'vect__analyzer': 'word', 'clf__shrink_threshold': None, 'tfidf__smooth_idf': True, 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True), 'vect__max_df': 0.5, 'vect__strip_accents': None, 'vect__dtype': <class 'numpy.int64'>, 'clf': NearestCentroid(metric='euclidean', shrink_threshold=None), 'vect__vocabulary': None, 'vect__min_df': 1, 'clf__metric': 'euclidean', 'vect__input': 'content', 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now'], 'vect__encoding': 'utf-8', 'tfidf__use_idf': True, 'vect__max_features': None, 'vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None,\n",
      "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', '...'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now'],\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None), 'vect__preprocessor': None, 'vect__binary': False}\n",
      "****************************************\n",
      "________________________________________________________________________________\n",
      "Training: NearestCentroid\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None,\n",
      "        stop_words=['i', 'me',...inear_tf=False, use_idf=True)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=None))])\n",
      "train time: 1.513s\n",
      "test time:  0.130s\n",
      "accuracy:   0.662\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.49      0.48      0.49       197\n",
      "        pos       0.74      0.75      0.75       395\n",
      "\n",
      "avg / total       0.66      0.66      0.66       592\n",
      "\n",
      "confusion matrix:\n",
      "[[ 95 102]\n",
      " [ 98 297]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "X = sentences_df['sentence']\n",
    "y = sentences_df['sentiment']\n",
    "X_data, X_val, y_data, y_val = train_test_split(X, y, train_size=0.9, test_size=0.1, random_state=100)\n",
    "X_data = X_data.reset_index(drop=True)\n",
    "X_val = X_val.reset_index(drop=True)\n",
    "y_data = y_data.reset_index(drop=True)\n",
    "y_val = y_val.reset_index(drop=True)\n",
    "for train_index, test_index in StratifiedKFold(y_data, K):\n",
    "    X_train, X_test = X_data[train_index], X_data[test_index]\n",
    "    y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', NearestCentroid()),\n",
    "    ])\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    be = grid_search.best_estimator_\n",
    "    print('*'*40)\n",
    "    print(be.get_params())\n",
    "    print('*'*40)\n",
    "    benchmark_results = benchmark('NearestCentroid', be, X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    results.append((be, grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Training: Ridge classifier\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip...True,\n",
      "        fit_intercept=True, max_iter=None, normalize=False, solver='lsqr',\n",
      "        tol=0.01))])\n",
      "train time: 0.481s\n",
      "test time:  0.233s\n",
      "accuracy:   0.661\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.49      0.32      0.39       986\n",
      "        pos       0.71      0.83      0.77      1975\n",
      "\n",
      "avg / total       0.64      0.66      0.64      2961\n",
      "\n",
      "confusion matrix:\n",
      "[[ 320  666]\n",
      " [ 339 1636]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: Perceptron\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip..._iter=50, n_jobs=1, penalty=None, random_state=0, shuffle=True,\n",
      "      verbose=0, warm_start=False))])\n",
      "train time: 0.505s\n",
      "test time:  0.230s\n",
      "accuracy:   0.652\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.47      0.37      0.41       986\n",
      "        pos       0.72      0.80      0.75      1975\n",
      "\n",
      "avg / total       0.63      0.65      0.64      2961\n",
      "\n",
      "confusion matrix:\n",
      "[[ 361  625]\n",
      " [ 404 1571]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: Passive-aggressive\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip...  n_iter=50, n_jobs=1, random_state=None, shuffle=True,\n",
      "              verbose=0, warm_start=False))])\n",
      "train time: 0.519s\n",
      "test time:  0.229s\n",
      "accuracy:   0.664\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.49      0.31      0.38       986\n",
      "        pos       0.71      0.84      0.77      1975\n",
      "\n",
      "avg / total       0.64      0.66      0.64      2961\n",
      "\n",
      "confusion matrix:\n",
      "[[ 308  678]\n",
      " [ 316 1659]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: kNN\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip...ize=30, metric='minkowski',\n",
      "           metric_params=None, n_neighbors=10, p=2, weights='uniform'))])\n",
      "train time: 0.455s\n",
      "test time:  0.759s\n",
      "accuracy:   0.653\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.47      0.30      0.36       986\n",
      "        pos       0.70      0.83      0.76      1975\n",
      "\n",
      "avg / total       0.62      0.65      0.63      2961\n",
      "\n",
      "confusion matrix:\n",
      "[[ 293  693]\n",
      " [ 334 1641]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: Random forest\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip...n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False))])\n",
      "train time: 38.957s\n",
      "test time:  0.632s\n",
      "accuracy:   0.672\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.54      0.10      0.16       986\n",
      "        pos       0.68      0.96      0.80      1975\n",
      "\n",
      "avg / total       0.63      0.67      0.59      2961\n",
      "\n",
      "confusion matrix:\n",
      "[[  95  891]\n",
      " [  80 1895]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: Linear SVC\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip...max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.001,\n",
      "     verbose=0))])\n",
      "train time: 0.824s\n",
      "test time:  0.281s\n",
      "accuracy:   0.674\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.52      0.27      0.36       986\n",
      "        pos       0.71      0.88      0.78      1975\n",
      "\n",
      "avg / total       0.64      0.67      0.64      2961\n",
      "\n",
      "confusion matrix:\n",
      "[[ 267  719]\n",
      " [ 245 1730]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: SGDClassifier\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip...   penalty='l1', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False))])\n",
      "train time: 0.612s\n",
      "test time:  0.225s\n",
      "accuracy:   0.654\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.47      0.34      0.40       986\n",
      "        pos       0.71      0.81      0.76      1975\n",
      "\n",
      "avg / total       0.63      0.65      0.64      2961\n",
      "\n",
      "confusion matrix:\n",
      "[[ 335  651]\n",
      " [ 374 1601]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: Nearest Centroid\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip...inear_tf=False, use_idf=True)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=None))])\n",
      "train time: 0.459s\n",
      "test time:  0.270s\n",
      "accuracy:   0.647\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.47      0.51      0.49       986\n",
      "        pos       0.75      0.71      0.73      1975\n",
      "\n",
      "avg / total       0.65      0.65      0.65      2961\n",
      "\n",
      "confusion matrix:\n",
      "[[ 504  482]\n",
      " [ 563 1412]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: Multinomial NB\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip...ear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1e-05, class_prior=None, fit_prior=True))])\n",
      "train time: 0.471s\n",
      "test time:  0.231s\n",
      "accuracy:   0.656\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.47      0.26      0.33       986\n",
      "        pos       0.70      0.86      0.77      1975\n",
      "\n",
      "avg / total       0.62      0.66      0.62      2961\n",
      "\n",
      "confusion matrix:\n",
      "[[ 254  732]\n",
      " [ 286 1689]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: Bernoulli NB\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip..., use_idf=True)), ('clf', BernoulliNB(alpha=1e-05, binarize=0.0, class_prior=None, fit_prior=True))])\n",
      "train time: 0.466s\n",
      "test time:  0.234s\n",
      "accuracy:   0.658\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.47      0.25      0.33       986\n",
      "        pos       0.70      0.86      0.77      1975\n",
      "\n",
      "avg / total       0.62      0.66      0.62      2961\n",
      "\n",
      "confusion matrix:\n",
      "[[ 246  740]\n",
      " [ 274 1701]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: Linear SVC\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip...max_iter=1000,\n",
      "     multi_class='ovr', penalty='l1', random_state=None, tol=0.001,\n",
      "     verbose=0))])\n",
      "train time: 0.581s\n",
      "test time:  0.226s\n",
      "accuracy:   0.682\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.53      0.39      0.45       986\n",
      "        pos       0.73      0.83      0.78      1975\n",
      "\n",
      "avg / total       0.66      0.68      0.67      2961\n",
      "\n",
      "confusion matrix:\n",
      "[[ 385  601]\n",
      " [ 342 1633]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: Ridge classifier\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip...True,\n",
      "        fit_intercept=True, max_iter=None, normalize=False, solver='lsqr',\n",
      "        tol=0.01))])\n",
      "train time: 0.475s\n",
      "test time:  0.223s\n",
      "accuracy:   0.675\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.52      0.38      0.44       986\n",
      "        pos       0.73      0.82      0.77      1975\n",
      "\n",
      "avg / total       0.66      0.68      0.66      2961\n",
      "\n",
      "confusion matrix:\n",
      "[[ 372  614]\n",
      " [ 347 1628]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: Perceptron\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip..._iter=50, n_jobs=1, penalty=None, random_state=0, shuffle=True,\n",
      "      verbose=0, warm_start=False))])\n",
      "train time: 0.521s\n",
      "test time:  0.227s\n",
      "accuracy:   0.660\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.49      0.39      0.43       986\n",
      "        pos       0.72      0.79      0.76      1975\n",
      "\n",
      "avg / total       0.64      0.66      0.65      2961\n",
      "\n",
      "confusion matrix:\n",
      "[[ 384  602]\n",
      " [ 405 1570]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: Passive-aggressive\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip...  n_iter=50, n_jobs=1, random_state=None, shuffle=True,\n",
      "              verbose=0, warm_start=False))])\n",
      "train time: 0.497s\n",
      "test time:  0.221s\n",
      "accuracy:   0.680\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.53      0.35      0.42       986\n",
      "        pos       0.72      0.84      0.78      1975\n",
      "\n",
      "avg / total       0.66      0.68      0.66      2961\n",
      "\n",
      "confusion matrix:\n",
      "[[ 347  639]\n",
      " [ 309 1666]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: kNN\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip...ize=30, metric='minkowski',\n",
      "           metric_params=None, n_neighbors=10, p=2, weights='uniform'))])\n",
      "train time: 0.495s\n",
      "test time:  0.742s\n",
      "accuracy:   0.655\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.47      0.27      0.34       986\n",
      "        pos       0.70      0.85      0.77      1975\n",
      "\n",
      "avg / total       0.62      0.66      0.63      2961\n",
      "\n",
      "confusion matrix:\n",
      "[[ 266  720]\n",
      " [ 301 1674]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: Random forest\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip...n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False))])\n",
      "train time: 40.236s\n",
      "test time:  0.454s\n",
      "accuracy:   0.686\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.68      0.11      0.19       986\n",
      "        pos       0.69      0.97      0.81      1975\n",
      "\n",
      "avg / total       0.68      0.69      0.60      2961\n",
      "\n",
      "confusion matrix:\n",
      "[[ 107  879]\n",
      " [  51 1924]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: Linear SVC\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip...max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.001,\n",
      "     verbose=0))])\n",
      "train time: 0.534s\n",
      "test time:  0.264s\n",
      "accuracy:   0.693\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.58      0.30      0.39       986\n",
      "        pos       0.72      0.89      0.79      1975\n",
      "\n",
      "avg / total       0.67      0.69      0.66      2961\n",
      "\n",
      "confusion matrix:\n",
      "[[ 293  693]\n",
      " [ 215 1760]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: SGDClassifier\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip...   penalty='l1', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False))])\n",
      "train time: 0.562s\n",
      "test time:  0.220s\n",
      "accuracy:   0.666\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.50      0.41      0.45       986\n",
      "        pos       0.73      0.80      0.76      1975\n",
      "\n",
      "avg / total       0.65      0.67      0.66      2961\n",
      "\n",
      "confusion matrix:\n",
      "[[ 400  586]\n",
      " [ 402 1573]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: Nearest Centroid\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip...inear_tf=False, use_idf=True)), ('clf', NearestCentroid(metric='euclidean', shrink_threshold=None))])\n",
      "train time: 0.460s\n",
      "test time:  0.267s\n",
      "accuracy:   0.673\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.51      0.50      0.51       986\n",
      "        pos       0.75      0.76      0.76      1975\n",
      "\n",
      "avg / total       0.67      0.67      0.67      2961\n",
      "\n",
      "confusion matrix:\n",
      "[[ 497  489]\n",
      " [ 480 1495]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: Multinomial NB\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip...ear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1e-05, class_prior=None, fit_prior=True))])\n",
      "train time: 0.468s\n",
      "test time:  0.220s\n",
      "accuracy:   0.652\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.46      0.27      0.34       986\n",
      "        pos       0.70      0.84      0.76      1975\n",
      "\n",
      "avg / total       0.62      0.65      0.62      2961\n",
      "\n",
      "confusion matrix:\n",
      "[[ 263  723]\n",
      " [ 308 1667]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: Bernoulli NB\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip..., use_idf=True)), ('clf', BernoulliNB(alpha=1e-05, binarize=0.0, class_prior=None, fit_prior=True))])\n",
      "train time: 0.474s\n",
      "test time:  0.236s\n",
      "accuracy:   0.651\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.46      0.27      0.34       986\n",
      "        pos       0.70      0.84      0.76      1975\n",
      "\n",
      "avg / total       0.62      0.65      0.62      2961\n",
      "\n",
      "confusion matrix:\n",
      "[[ 264  722]\n",
      " [ 311 1664]]\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: Linear SVC\n",
      "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "        strip...max_iter=1000,\n",
      "     multi_class='ovr', penalty='l1', random_state=None, tol=0.001,\n",
      "     verbose=0))])\n",
      "train time: 0.566s\n",
      "test time:  0.220s\n",
      "accuracy:   0.674\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.52      0.36      0.42       986\n",
      "        pos       0.72      0.83      0.77      1975\n",
      "\n",
      "avg / total       0.65      0.67      0.66      2961\n",
      "\n",
      "confusion matrix:\n",
      "[[ 351  635]\n",
      " [ 330 1645]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "# results = defaultdict(lambda: defaultdict(list))\n",
    "results = defaultdict(list)\n",
    "\n",
    "X = sentences_df['sentence']\n",
    "y = sentences_df['sentiment']\n",
    "X_data, X_val, y_data, y_val = train_test_split(X, y, train_size=0.9, test_size=0.1, random_state=100)\n",
    "X_data = X_data.reset_index(drop=True)\n",
    "X_val = X_val.reset_index(drop=True)\n",
    "y_data = y_data.reset_index(drop=True)\n",
    "y_val = y_val.reset_index(drop=True)\n",
    "for train_index, test_index in StratifiedKFold(y_data, 2):\n",
    "    X_train, X_test = X_data[train_index], X_data[test_index]\n",
    "    y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "    \n",
    "    for clf, name in clf_list:\n",
    "        pipeline = Pipeline([\n",
    "            ('vect', CountVectorizer(ngram_range=(1, 2))),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('clf', clf),\n",
    "        ])\n",
    "#         grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "        benchmark_results = benchmark(name, pipeline, X_train, X_test, y_train, y_test)\n",
    "        results[name].append(benchmark_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos    1975\n",
       "neg     986\n",
       "dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Three years ago, according to a previous NPD s...\n",
       "1    While it might be counter-intuitive to expect ...\n",
       "2    'I've seen five movies today, and it has been ...\n",
       "3    Large checks came in from Pfizer, Caterpillar,...\n",
       "4    The group stressed it had not employed any sen...\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data.reset_index(drop=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    pos\n",
       "1    pos\n",
       "2    pos\n",
       "3    pos\n",
       "4    neg\n",
       "Name: sentiment, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data.reset_index(drop=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5329"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "582     'Our results for the fourth quarter of 2012 co...\n",
       "594     'People enjoy the humor on board, and that we ...\n",
       "595     'People have been bragging about booking six f...\n",
       "596     'People have been expressing interest in this ...\n",
       "597                                                   NaN\n",
       "598     'People say you can't change China, but I woul...\n",
       "599     'People were driving by daily and not realizin...\n",
       "600                                                   NaN\n",
       "601     'People who are trying too hard or people who ...\n",
       "602                                                   NaN\n",
       "603                                                   NaN\n",
       "604     'President Obama is on pace to break the two m...\n",
       "605     'President Obama's plan to cut Social Security...\n",
       "606                                                   NaN\n",
       "607                                                   NaN\n",
       "608                                                   NaN\n",
       "609     'Quality content has never been more important...\n",
       "610                                                   NaN\n",
       "611                                                   NaN\n",
       "612                                                   NaN\n",
       "613     'Regular Singing' (in previews; opens on Frida...\n",
       "614     'Regular Singing' (previews start on Saturday;...\n",
       "615                                                   NaN\n",
       "616     'Resale Royalty,' at 9, follows Sue McCarthy a...\n",
       "617                                                   NaN\n",
       "618     'Right now we have a situation where the execu...\n",
       "619     'Right now, Apple is being priced as though it...\n",
       "620     'Right now, our plan is to hold back the spot ...\n",
       "621     'Robert is one of those great social entrepren...\n",
       "622                                                   NaN\n",
       "                              ...                        \n",
       "5892                                                  NaN\n",
       "5893    On Saturday, the police said Boeing confirmed ...\n",
       "5894    On Sunday at MetLife Stadium, their top priori...\n",
       "5895    On Thursday evening, police officers opened fi...\n",
       "5896    On Thursday night, more than 100 members of th...\n",
       "5897    On Thursday, Berkshire Hathaway and the invest...\n",
       "5898    On Thursday, Berkshire Hathaway, the conglomer...\n",
       "5899                                                  NaN\n",
       "5900                                                  NaN\n",
       "5901                                                  NaN\n",
       "5902    On Thursday, Nike became the latest corporate ...\n",
       "5903    On Thursday, it was the top-selling book on Am...\n",
       "5904    On Thursday, shares in the Aramark Holdings Co...\n",
       "5905                                                  NaN\n",
       "5906    On Tuesday evening, thousands of young Shiites...\n",
       "5907    On Tuesday morning asset manager Ashmore annou...\n",
       "5908                                                  NaN\n",
       "5909    On Tuesday, China will release its services pu...\n",
       "5910    On Tuesday, Exxon Mobil said it was cutting ba...\n",
       "5911                                                  NaN\n",
       "5912    On Tuesday, SoftBank's chief executive, Masayo...\n",
       "5913    On Tuesday, Timothy D. Cook, Apple's chief exe...\n",
       "5914                                                  NaN\n",
       "5915    On Tuesday, the United States International Tr...\n",
       "5916    On Twitter yesterday, I engaged in some discus...\n",
       "5917    On Wednesday, 'we're breaking the mold of trad...\n",
       "5918    On Wednesday, JPMorgan Chase also reported a s...\n",
       "5919    On Wednesday, Navalny's mayoral campaign manag...\n",
       "5920    On Wednesday, Target and Co.Labs plan to annou...\n",
       "5921                                                  NaN\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61      \"The tapering of quantitative easing has only ...\n",
       "5264    More and more Chinese parents apparently disag...\n",
       "5267    More recently, General Motors offered shares t...\n",
       "3719    He managed Lewis until 2010, when tensions ari...\n",
       "4226    In a recent and noteworthy post at Realclimate...\n",
       "399     'If you're buying something in an Apple store,...\n",
       "8370    Three individuals with different levels of obj...\n",
       "7080    The French choreographer and founder of the L....\n",
       "8166    There is also some bad blood behind this story...\n",
       "6235    Raiding the endowment 'was a suicidal thing to...\n",
       "7348    The business's founders promote it as a low-pr...\n",
       "3075    Don Steinbrugge, managing partner with Agecrof...\n",
       "3963    Hyundai is offering a $750 credit for people w...\n",
       "4437    In time, music became a sideline, as he took a...\n",
       "7283    The authorities said the fraud was carried out...\n",
       "4107    In 1998, Research in Motion sought professiona...\n",
       "650     'Students' social media and digital footprint ...\n",
       "4332    In its lawsuit, Apple said Psystar had violate...\n",
       "640     'Since the '80s, the days of the dictator Gen....\n",
       "5233    Midway through the Bush administration, the ex...\n",
       "4540    It is a shotgun wedding that has consigned Rya...\n",
       "1505    According to the memo, 'existing and new staff...\n",
       "1818    And Mr. Loeb is expected to argue that Sony's ...\n",
       "7896    The panda cameras are funded by a grant from t...\n",
       "7909    The phrase 'middle out' itself seems to have g...\n",
       "5537    Mr. Pratchett, whose books have sold more than...\n",
       "1613    All the while, beginning with a quiet deal soo...\n",
       "349     'I think these brands are cheapening the town,...\n",
       "8907    Xiaomi has generated considerable attention be...\n",
       "8917    Yet McCarthy has found more than 70 additional...\n",
       "                              ...                        \n",
       "7478    The company, which is no longer controlled by ...\n",
       "1280    A combination with Dish Networks would pose mo...\n",
       "6845    TEHRAN -- All over this city of 12 million peo...\n",
       "1569    After the closing bell, shares of Cisco System...\n",
       "7060    The Dutch company sued General Motors for dama...\n",
       "832     'These investors want to diversify their wealt...\n",
       "987     'We have a real problem, and that's, 'How do w...\n",
       "7231    The administration passed an executive order l...\n",
       "8908    Yahoo's lack of growth exasperated Yahoo share...\n",
       "7103    The Internet and North Korea have been togethe...\n",
       "4633                      It won't make Apple very happy.\n",
       "238     'Facebook and Instagram are spiritual brothers...\n",
       "3141    Economists predict the yen to weaken further, ...\n",
       "3689    He indicated that he would carry on the mainly...\n",
       "3042    Despite the assault on senior management, no e...\n",
       "4338    In later years, after Mattel reduced and then ...\n",
       "7326    The bride's father is the chairman and chief e...\n",
       "8035    The sting operation, executed by the F.B.I. a ...\n",
       "3469    Gary Willoughby, executive director of the Tol...\n",
       "5268    More than 1,300 federal and state policies, ex...\n",
       "0        General Motors will recall nearly 3,200 manua...\n",
       "7566    The executive director of Hawaii's exchange, C...\n",
       "328     'I never wanted there to be any lingering anim...\n",
       "2015    Apple hires brand converts for its stores beca...\n",
       "5166    Microsoft Office 2010 (or Office 2011 for the ...\n",
       "3136    Earlier this year, AT&T said that 9 out of 10 ...\n",
       "4871    Ken Ehrlich, the show's executive producer, ha...\n",
       "2230    Asked about the implications of the app, Rajee...\n",
       "2949    Craig Jackson, chairman and chief executive of...\n",
       "4528    It has now received backing from investors inc...\n",
       "Name: sentence, dtype: object"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data.reindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "597",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-1f8ff5eaed02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m597\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/antonio/.virtualenvs/lexisnexis/local/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/antonio/.virtualenvs/lexisnexis/local/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m    922\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_iterable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/antonio/.virtualenvs/lexisnexis/local/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_get_label\u001b[1;34m(self, label, axis)\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m         elif (isinstance(label, tuple) and\n\u001b[0;32m     86\u001b[0m                 isinstance(label[axis], slice)):\n",
      "\u001b[1;32m/home/antonio/.virtualenvs/lexisnexis/local/lib/python2.7/site-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    512\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 514\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/antonio/.virtualenvs/lexisnexis/local/lib/python2.7/site-packages/pandas/core/index.pyc\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   1458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1459\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1460\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1461\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1462\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minferred_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'integer'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'boolean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_value (pandas/index.c:3113)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_value (pandas/index.c:2844)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:3704)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.Int64HashTable.get_item (pandas/hashtable.c:7255)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.Int64HashTable.get_item (pandas/hashtable.c:7193)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 597"
     ]
    }
   ],
   "source": [
    "X_data.ix[597]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.ix[597]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# K = 10\n",
    "# parameters = {\n",
    "#     'vect__max_df': (0.5, 0.75, 1.0),\n",
    "#     'vect__max_features': (None, 1000, 5000, 10000, 50000),\n",
    "#     'vect__ngram_range': ((1, 1), (1, 2), (2, 2), (1, 3), (2, 3), (3, 3)),  # unigrams or bigrams or trigrams\n",
    "#     'vect__stop_words': (None, stopwords.words('english')),\n",
    "#     'tfidf__use_idf': (True, False),\n",
    "#     'tfidf__norm': ('l1', 'l2'),\n",
    "# #     'clf__alpha': (1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6),\n",
    "# #     'clf__penalty': ('l1', 'l2', 'elasticnet'),\n",
    "# #     'clf__n_iter': (10, 50, 80),\n",
    "# #     'clf__loss': ('log', 'modified_huber'),\n",
    "# }\n",
    "# csf_list = [\n",
    "#     (RidgeClassifier(tol=1e-2, solver=\"lsqr\"), \"Ridge Classifier\"),\n",
    "#     (Perceptron(n_iter=50), \"Perceptron\"),\n",
    "#     (PassiveAggressiveClassifier(n_iter=50), \"Passive-Aggressive\"),\n",
    "#     (KNeighborsClassifier(n_neighbors=10), \"kNN\"),\n",
    "#     (RandomForestClassifier(n_estimators=100), \"Random forest\")\n",
    "# ]\n",
    "# results = {}\n",
    "# for clf, name in csf_list:\n",
    "#     pipeline = Pipeline([\n",
    "#         ('vect', CountVectorizer()),\n",
    "#         ('tfidf', TfidfTransformer()),\n",
    "#         ('clf', clf),\n",
    "#     ])\n",
    "#     grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "#     skf = StratifiedKFold(y, K)\n",
    "#     results[name] = cross_val_score(grid_search, X, y, cv=skf, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from the training data using a sparse vectorizer\n",
      "done in 0.398901s at 2.688MB/s\n",
      "n_samples: 5922, n_features: 18088\n",
      "\n",
      "Extracting features from the test data using the same vectorizer\n",
      "done in 0.027828s at 4.383MB/s\n",
      "n_samples: 659, n_features: 18088\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split a training set and a test set\n",
    "y_train = train_labels\n",
    "y_test = test_labels\n",
    "\n",
    "print(\"Extracting features from the training data using a sparse vectorizer\")\n",
    "t0 = time()\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                             stop_words='english')\n",
    "X_train = vectorizer.fit_transform(data_train.sentence.tolist())\n",
    "duration = time() - t0\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, data_train_size_mb / duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % X_train.shape)\n",
    "print()\n",
    "\n",
    "print(\"Extracting features from the test data using the same vectorizer\")\n",
    "t0 = time()\n",
    "X_test = vectorizer.transform(data_test.sentence.tolist())\n",
    "duration = time() - t0\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, data_test_size_mb / duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % X_test.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting 1000 best features by a chi-squared test\n",
      "done in 0.022103s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mapping from integer feature name to original token string\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "opts_select_chi2 = 100\n",
    "\n",
    "print(\"Extracting %d best features by a chi-squared test\" %\n",
    "      opts_select_chi2)\n",
    "t0 = time()\n",
    "ch2 = SelectKBest(chi2, k=opts_select_chi2)\n",
    "X_train = ch2.fit_transform(X_train, y_train)\n",
    "X_test = ch2.transform(X_test)\n",
    "if feature_names:\n",
    "    # keep selected feature names\n",
    "    feature_names = [feature_names[i] for i\n",
    "                     in ch2.get_support(indices=True)]\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "feature_names = np.asarray(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trim(s):\n",
    "    \"\"\"Trim string to fit on terminal (assuming 80-column display)\"\"\"\n",
    "    return s if len(s) < 80 else s[:75] + \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.metrics.pairwise.pairwise_distances>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.pairwise.pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
