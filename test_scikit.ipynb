{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing Data\n",
    "=============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import cPickle as pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import factorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bossa/tasks_export.json  bossa/tasks_runs_export.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls bossa/*json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BOSSA Results\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing `results_bossa.json` to get a *dictionary* with keys the task ids, and values in as the average value of the scores. To do that, we first convert scores from categorical (`neg`, `neu`, `pos`) to a numeric scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result_id</th>\n",
       "      <th>seconds</th>\n",
       "      <th>task_id</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>11203</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>52775</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    result_id   seconds  task_id score\n",
       "50      11203  0.000025    52775     1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bossa_results = pd.read_json(\"bossa/tasks_runs_export.json\")\n",
    "bossa_results.rename(columns={\"created\": \"start_time\", \"id\": \"result_id\", \"info\": \"score\"}, inplace=True)\n",
    "bossa_results[['start_time']]= bossa_results[['start_time']].apply(pd.to_datetime, dayfirst=True)\n",
    "bossa_results[['finish_time']]= bossa_results[['finish_time']].apply(pd.to_datetime, dayfirst=True)\n",
    "bossa_results['score'] = pd.Categorical(bossa_results['score'], categories=['vneg', 'neg', 'neu', 'pos', 'vpos'])\n",
    "bossa_results['score'].cat.rename_categories([-2, -1, 0, 1, 2], inplace=True)\n",
    "# Normalize everything to -1, 0, 1\n",
    "# bossa_results['score'] = bossa_results['score'].astype(float).apply(lambda x: -1 if x < 0 else 1 if x > 0 else 0)\n",
    "bossa_results[\"seconds\"] = (bossa_results[\"finish_time\"] - bossa_results[\"start_time\"]).astype('timedelta64[us]') / 1e6\n",
    "bossa_results = bossa_results[[\"result_id\", \"seconds\", \"task_id\", \"score\"]]\n",
    "bossa_results.ix[[50]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information about the sentence comes in a dictionary inside the cells of the serie `info`, so we expand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_id</th>\n",
       "      <th>info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>52851</td>\n",
       "      <td>{u'search_words': u'founder', u'appears_in_sen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    task_id                                               info\n",
       "50    52851  {u'search_words': u'founder', u'appears_in_sen..."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bossa_tasks = pd.read_json(\"bossa/tasks_export.json\")\n",
    "bossa_tasks[['created']]= bossa_tasks[['created']].apply(pd.to_datetime, dayfirst=True)\n",
    "bossa_tasks.rename(columns={'id': 'task_id'}, inplace=True)\n",
    "bossa_tasks = bossa_tasks[['task_id', 'info']]\n",
    "bossa_tasks.ix[[50]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we merge the `DataFrame` with the scores with the one containing the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result_id</th>\n",
       "      <th>seconds</th>\n",
       "      <th>task_id</th>\n",
       "      <th>score</th>\n",
       "      <th>info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>11195</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>52776</td>\n",
       "      <td>2</td>\n",
       "      <td>{u'search_words': u'executive', u'appears_in_s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    result_id   seconds  task_id  score  \\\n",
       "50      11195  0.000021    52776      2   \n",
       "\n",
       "                                                 info  \n",
       "50  {u'search_words': u'executive', u'appears_in_s...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bossa_tasks_scores = pd.merge(bossa_results, bossa_tasks, on='task_id')\n",
    "bossa_tasks_scores.ix[[50]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now expand the column `info` into as many new columns as keys has the dictionary `info`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'search_words',\n",
       " u'appears_in_sentence',\n",
       " u'url',\n",
       " u'media',\n",
       " u'appears_in_noun_phrases',\n",
       " u'noun_phrases',\n",
       " u'sentence_id',\n",
       " u'text',\n",
       " u'sentence',\n",
       " u'pub_date',\n",
       " u'is_company']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bossa_tasks_scores.ix[50].info.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result_id</th>\n",
       "      <th>seconds</th>\n",
       "      <th>task_id</th>\n",
       "      <th>score</th>\n",
       "      <th>search_words</th>\n",
       "      <th>appears_in_sentence</th>\n",
       "      <th>url</th>\n",
       "      <th>media</th>\n",
       "      <th>appears_in_noun_phrases</th>\n",
       "      <th>noun_phrases</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>text</th>\n",
       "      <th>sentence</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>is_company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>11195</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>52776</td>\n",
       "      <td>2</td>\n",
       "      <td>executive</td>\n",
       "      <td>0</td>\n",
       "      <td>http://dealbook.nytimes.com/2013/05/17/a-toeho...</td>\n",
       "      <td>nyt</td>\n",
       "      <td>0</td>\n",
       "      <td>[chinese investors, overseas companies, politi...</td>\n",
       "      <td>14</td>\n",
       "      <td>Chinese investors are increasingly opting to b...</td>\n",
       "      <td>Chinese investors are increasingly opting to b...</td>\n",
       "      <td>2013-05-17T11:47:51Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>11205</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>52776</td>\n",
       "      <td>-1</td>\n",
       "      <td>executive</td>\n",
       "      <td>0</td>\n",
       "      <td>http://dealbook.nytimes.com/2013/05/17/a-toeho...</td>\n",
       "      <td>nyt</td>\n",
       "      <td>0</td>\n",
       "      <td>[chinese investors, overseas companies, politi...</td>\n",
       "      <td>14</td>\n",
       "      <td>Chinese investors are increasingly opting to b...</td>\n",
       "      <td>Chinese investors are increasingly opting to b...</td>\n",
       "      <td>2013-05-17T11:47:51Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>11207</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>52776</td>\n",
       "      <td>1</td>\n",
       "      <td>executive</td>\n",
       "      <td>0</td>\n",
       "      <td>http://dealbook.nytimes.com/2013/05/17/a-toeho...</td>\n",
       "      <td>nyt</td>\n",
       "      <td>0</td>\n",
       "      <td>[chinese investors, overseas companies, politi...</td>\n",
       "      <td>14</td>\n",
       "      <td>Chinese investors are increasingly opting to b...</td>\n",
       "      <td>Chinese investors are increasingly opting to b...</td>\n",
       "      <td>2013-05-17T11:47:51Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>11209</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>52776</td>\n",
       "      <td>-2</td>\n",
       "      <td>executive</td>\n",
       "      <td>0</td>\n",
       "      <td>http://dealbook.nytimes.com/2013/05/17/a-toeho...</td>\n",
       "      <td>nyt</td>\n",
       "      <td>0</td>\n",
       "      <td>[chinese investors, overseas companies, politi...</td>\n",
       "      <td>14</td>\n",
       "      <td>Chinese investors are increasingly opting to b...</td>\n",
       "      <td>Chinese investors are increasingly opting to b...</td>\n",
       "      <td>2013-05-17T11:47:51Z</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    result_id   seconds  task_id  score search_words appears_in_sentence  \\\n",
       "50      11195  0.000021    52776      2    executive                   0   \n",
       "51      11205  0.000018    52776     -1    executive                   0   \n",
       "52      11207  0.000017    52776      1    executive                   0   \n",
       "53      11209  0.000017    52776     -2    executive                   0   \n",
       "\n",
       "                                                  url media  \\\n",
       "50  http://dealbook.nytimes.com/2013/05/17/a-toeho...   nyt   \n",
       "51  http://dealbook.nytimes.com/2013/05/17/a-toeho...   nyt   \n",
       "52  http://dealbook.nytimes.com/2013/05/17/a-toeho...   nyt   \n",
       "53  http://dealbook.nytimes.com/2013/05/17/a-toeho...   nyt   \n",
       "\n",
       "   appears_in_noun_phrases                                       noun_phrases  \\\n",
       "50                       0  [chinese investors, overseas companies, politi...   \n",
       "51                       0  [chinese investors, overseas companies, politi...   \n",
       "52                       0  [chinese investors, overseas companies, politi...   \n",
       "53                       0  [chinese investors, overseas companies, politi...   \n",
       "\n",
       "   sentence_id                                               text  \\\n",
       "50          14  Chinese investors are increasingly opting to b...   \n",
       "51          14  Chinese investors are increasingly opting to b...   \n",
       "52          14  Chinese investors are increasingly opting to b...   \n",
       "53          14  Chinese investors are increasingly opting to b...   \n",
       "\n",
       "                                             sentence              pub_date  \\\n",
       "50  Chinese investors are increasingly opting to b...  2013-05-17T11:47:51Z   \n",
       "51  Chinese investors are increasingly opting to b...  2013-05-17T11:47:51Z   \n",
       "52  Chinese investors are increasingly opting to b...  2013-05-17T11:47:51Z   \n",
       "53  Chinese investors are increasingly opting to b...  2013-05-17T11:47:51Z   \n",
       "\n",
       "   is_company  \n",
       "50          0  \n",
       "51          0  \n",
       "52          0  \n",
       "53          0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def json_to_series(info):\n",
    "    keys, values = zip(*info.iteritems())\n",
    "    return pd.Series(values, index=keys)\n",
    "\n",
    "bossa_info = bossa_tasks_scores[\"info\"].apply(json_to_series)\n",
    "bossa_info.reset_index()\n",
    "bossa = pd.concat([bossa_tasks_scores, bossa_info], axis=1)\n",
    "bossa.pop(\"info\")\n",
    "# bossa['id'] = bossa['id'].astype(float)\n",
    "bossa.ix[50:53]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now aggregate calculating the average per `sentence_id` using a group by. In the process, we lose the source of the data, that's why we first have to save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bossa.to_csv(\"sentiment/scores_ungrouped.csv\", encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we aggregate and create a new `DataFrame` for the different sentences and their score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score    8996\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'We must hope after so much prevarication that this time Google's proposals represent a genuine attempt to address the concerns identified,' said David Wood, the legal counsel for Icomp, an industry group backed by Microsoft and a number of other companies.</th>\n",
       "      <td>-0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'We must push our leaders to step up and commit to action,' said Hugh Evans, the founder and chief executive of the charity.</th>\n",
       "      <td>-0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'We need them to tell the story of how we are making decisions and putting the organization together,' said George Postolos, the Astros' president and chief executive, who added that the team would not want a broadcaster who was uncomfortable explaining the front office's strategy.</th>\n",
       "      <td>-0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                               score\n",
       "sentence                                                                                                                                                                                                                                                                                            \n",
       "'We must hope after so much prevarication that this time Google's proposals represent a genuine attempt to address the concerns identified,' said David Wood, the legal counsel for Icomp, an industry group backed by Microsoft and a number of other companies.                          -0.333333\n",
       "'We must push our leaders to step up and commit to action,' said Hugh Evans, the founder and chief executive of the charity.                                                                                                                                                               -0.285714\n",
       "'We need them to tell the story of how we are making decisions and putting the organization together,' said George Postolos, the Astros' president and chief executive, who added that the team would not want a broadcaster who was uncomfortable explaining the front office's strategy. -0.666667"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = bossa.groupby(['sentence'])[['score']].aggregate(np.average)\n",
    "sentences.to_csv(\"sentiment/scores.csv\", encoding=\"utf8\")\n",
    "print(sentences.count())\n",
    "sentences[1001:1004]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence Classifier\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the tranining and testing sets (data and labels) from a randomized version of the set of assessed sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence    8996\n",
       "score       8996\n",
       "dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.reset_index().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could consider 3 classes, but it toruns out that using binary classficication seems to produce better results. Still, try multi-classs classifiers is something worth trying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos 2939 4281 755\n",
      "neg 2939 2498 440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonio/.virtualenvs/lexisnexis/lib/python2.7/site-packages/IPython/kernel/__main__.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "raw_scores = sentences.reset_index()\n",
    "scores = raw_scores\n",
    "scores = scores[scores.score!=0]  # We ignore the neutral sentences\n",
    "scores['sentiment'] = scores['score'].apply(lambda s: 'pos' if s > 0 else 'neg')\n",
    "percentage = 0.85  #  percentage for training, rest for for testing\n",
    "# We split to have enough representativenesss for both positive and negative sentiments\n",
    "sent_min = min(\n",
    "    scores[scores.sentiment=='pos'].sentiment.count(),\n",
    "    scores[scores.sentiment=='neg'].sentiment.count(),\n",
    ")\n",
    "scores = scores[[\"sentence\", \"sentiment\"]]\n",
    "train_data = np.array([])\n",
    "train_labels = np.array([])\n",
    "test_data = np.array([])\n",
    "test_labels = np.array([])\n",
    "for sent in ('pos', 'neg'):\n",
    "    sent_scores = scores[scores['sentiment']==sent]\n",
    "    sent_scores = sent_scores.reindex(np.random.permutation(sent_scores.index))\n",
    "    sent_sentences_count = int(sent_scores['sentence'].count())\n",
    "    sent_train = sent_scores[[\"sentence\", \"sentiment\"]][:int(sent_sentences_count * percentage)]\n",
    "    sent_test = sent_scores[[\"sentence\", \"sentiment\"]][int(sent_sentences_count * percentage) + 1:]\n",
    "    print(sent, sent_min, sent_train.sentiment.count(), sent_test.sentiment.count())\n",
    "    train_data = np.append(train_data, sent_train[\"sentence\"])\n",
    "    train_labels = np.append(train_labels, sent_train[\"sentiment\"])\n",
    "    test_data = np.append(test_data, sent_test[\"sentence\"])\n",
    "    test_labels = np.append(test_labels, sent_test[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "document_df = scores[['sentence', 'sentiment']]\n",
    "document_df = document_df.reindex(np.random.permutation(document_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7178"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = int(len(document_df) * 0.9)\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Author: Peter Prettenhofer <peter.prettenhofer@gmail.com>\n",
    "#         Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Mathieu Blondel <mathieu@mblondel.org>\n",
    "#         Lars Buitinck <L.J.Buitinck@uva.nl>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sentences for categories:\n",
      "['pos']\n",
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "categories = [\n",
    "    'pos',\n",
    "    'neg',\n",
    "]\n",
    "print(\"Loading sentences for categories:\")\n",
    "print(categories if categories else \"all\")\n",
    "data_train = document_df[:size]\n",
    "data_test = document_df[size:]\n",
    "print('data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos    5037\n",
       "neg    2939\n",
       "dtype: int64"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos    4540\n",
       "neg    2638\n",
       "dtype: int64"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = np.array(map(lambda x: 1 if x == 'pos' else 0, data_train['sentiment']))\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0,\n",
       "       1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "       1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "       1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n",
       "       0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,\n",
       "       1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels = np.array(map(lambda x: 1 if x == 'pos' else 0, data_test['sentiment']))\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7178 documents - 1.296MB (training set)\n",
      "798 documents - 0.145MB (test set)\n",
      "1 categories\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def size_mb(docs):\n",
    "    return sum(len(s.encode('utf-8')) for s in docs) / 1e6\n",
    "\n",
    "data_train_size_mb = size_mb(data_train['sentence'])\n",
    "data_test_size_mb = size_mb(data_test['sentence'])\n",
    "\n",
    "print(\"%d documents - %0.3fMB (training set)\" % (\n",
    "    len(data_train), data_train_size_mb))\n",
    "print(\"%d documents - %0.3fMB (test set)\" % (\n",
    "    len(data_test), data_test_size_mb))\n",
    "print(\"%d categories\" % len(categories))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7178x20204 sparse matrix of type '<type 'numpy.bool_'>'\n",
       "\twith 145024312 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.any(vectorizer.fit_transform(data_train.sentence.tolist()) == vectorizer.fit_transform(data_train['sentence']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max_df corresponds to < documents than min_df",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-3a959eb30c9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentence'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentence'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/antonio/.virtualenvs/lexisnexis/local/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1283\u001b[0m             \u001b[0mTf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m         \"\"\"\n\u001b[1;32m-> 1285\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1286\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1287\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/antonio/.virtualenvs/lexisnexis/local/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    819\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmax_doc_count\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mmin_doc_count\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m                 raise ValueError(\n\u001b[1;32m--> 821\u001b[1;33m                     \"max_df corresponds to < documents than min_df\")\n\u001b[0m\u001b[0;32m    822\u001b[0m             X, self.stop_words_ = self._limit_features(X, vocabulary,\n\u001b[0;32m    823\u001b[0m                                                        \u001b[0mmax_doc_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: max_df corresponds to < documents than min_df"
     ]
    }
   ],
   "source": [
    "vectorizer.fit_transform(data_train['sentence']) == vectorizer.fit_transform(data_train[['sentence']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from the training data using a sparse vectorizer\n",
      "  (0, 13906)\t0.280931966791\n",
      "  (0, 19579)\t0.258754235708\n",
      "  (0, 5814)\t0.23936049477\n",
      "  (0, 16228)\t0.183016452411\n",
      "  (0, 2998)\t0.179695401064\n",
      "  (0, 19066)\t0.177099732255\n",
      "  (0, 641)\t0.22360336359\n",
      "  (0, 1712)\t0.218671204275\n",
      "  (0, 1492)\t0.249549645659\n",
      "  (0, 7314)\t0.102164020077\n",
      "  (0, 16563)\t0.249549645659\n",
      "  (0, 13361)\t0.229436869908\n",
      "  (0, 7274)\t0.214398773542\n",
      "  (0, 14135)\t0.236576504625\n",
      "  (0, 2371)\t0.258754235708\n",
      "  (0, 15495)\t0.218671204275\n",
      "  (0, 16628)\t0.271727376742\n",
      "  (0, 11237)\t0.253822076392\n",
      "  (0, 12822)\t0.173562867311\n",
      "  (0, 11968)\t0.170716929997\n",
      "  (1, 11388)\t0.171541827176\n",
      "  (1, 3919)\t0.169621729789\n",
      "  (1, 8054)\t0.237998274652\n",
      "  (1, 3538)\t0.156759319167\n",
      "  (1, 6554)\t0.0596913408339\n",
      "  :\t:\n",
      "  (7176, 16893)\t0.142118600074\n",
      "  (7176, 10645)\t0.0957326757805\n",
      "  (7176, 3078)\t0.165865612074\n",
      "  (7176, 19904)\t0.177144468939\n",
      "  (7176, 12374)\t0.0868874065707\n",
      "  (7176, 11105)\t0.0784995308889\n",
      "  (7176, 4024)\t0.080406761479\n",
      "  (7176, 1341)\t0.0796093740444\n",
      "  (7176, 19065)\t0.13327135225\n",
      "  (7177, 3270)\t0.266030490035\n",
      "  (7177, 13514)\t0.27932177713\n",
      "  (7177, 13360)\t0.298462708632\n",
      "  (7177, 2273)\t0.307286197225\n",
      "  (7177, 6429)\t0.315302874364\n",
      "  (7177, 13945)\t0.258173413764\n",
      "  (7177, 4736)\t0.300508193282\n",
      "  (7177, 3947)\t0.219347211051\n",
      "  (7177, 2323)\t0.214058229468\n",
      "  (7177, 1112)\t0.218690930351\n",
      "  (7177, 222)\t0.27932177713\n",
      "  (7177, 3583)\t0.127380214124\n",
      "  (7177, 19575)\t0.291117342099\n",
      "  (7177, 12089)\t0.141173882847\n",
      "  (7177, 19528)\t0.245724354953\n",
      "  (7177, 6554)\t0.101299984826\n",
      "done in 0.324818s at 3.991MB/s\n",
      "n_samples: 7178, n_features: 20204\n",
      "\n",
      "Extracting features from the test data using the same vectorizer\n",
      "done in 0.029533s at 4.911MB/s\n",
      "n_samples: 798, n_features: 20204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split a training set and a test set\n",
    "y_train = train_labels\n",
    "y_test = test_labels\n",
    "\n",
    "print(\"Extracting features from the training data using a sparse vectorizer\")\n",
    "t0 = time()\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                             stop_words='english')\n",
    "X_train = vectorizer.fit_transform(data_train.sentence.tolist())\n",
    "print(X_train)\n",
    "duration = time() - t0\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, data_train_size_mb / duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % X_train.shape)\n",
    "print()\n",
    "\n",
    "print(\"Extracting features from the test data using the same vectorizer\")\n",
    "t0 = time()\n",
    "X_test = vectorizer.transform(data_test.sentence.tolist())\n",
    "duration = time() - t0\n",
    "print(\"done in %fs at %0.3fMB/s\" % (duration, data_test_size_mb / duration))\n",
    "print(\"n_samples: %d, n_features: %d\" % X_test.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting 100 best features by a chi-squared test\n",
      "done in 0.020813s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mapping from integer feature name to original token string\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "opts_select_chi2 = 100\n",
    "\n",
    "print(\"Extracting %d best features by a chi-squared test\" %\n",
    "      opts_select_chi2)\n",
    "t0 = time()\n",
    "ch2 = SelectKBest(chi2, k=opts_select_chi2)\n",
    "X_train = ch2.fit_transform(X_train2, y_train)\n",
    "X_test = ch2.transform(X_test2)\n",
    "if feature_names:\n",
    "    # keep selected feature names\n",
    "    feature_names = [feature_names[i] for i\n",
    "                     in ch2.get_support(indices=True)]\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "feature_names = np.asarray(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([u'accusations', u'accused', u'affected', u'alan', u'allegations',\n",
       "       u'anti', u'argument', u'arrested', u'asked', u'attacked',\n",
       "       u'authorities', u'ban', u'best', u'branch', u'bribes', u'case',\n",
       "       u'cases', u'charged', u'charges', u'civil', u'comment',\n",
       "       u'communication', u'continuing', u'controversy', u'corrupt',\n",
       "       u'corruption', u'creative', u'crimes', u'criminal', u'crisis',\n",
       "       u'cut', u'death', u'debate', u'debt', u'declined', u'demanding',\n",
       "       u'depuy', u'didn', u'difficult', u'emergency', u'entrepreneur',\n",
       "       u'error', u'evidence', u'fallen', u'fell', u'force', u'forced',\n",
       "       u'frustrated', u'gained', u'government', u'groom', u'guilty',\n",
       "       u'halt', u'ignored', u'include', u'information', u'invested',\n",
       "       u'investigation', u'irish', u'killed', u'killing', u'lawsuit',\n",
       "       u'lose', u'losing', u'losses', u'lost', u'mails', u'marriage',\n",
       "       u'mckesson', u'mobile', u'paid', u'plane', u'political',\n",
       "       u'problems', u'prosecutors', u'rate', u'regional', u'respond',\n",
       "       u'revelations', u'rifle', u'rival', u'rose', u'rowe', u'scrutiny',\n",
       "       u'seeking', u'shooting', u'shut', u'situation', u'space', u'split',\n",
       "       u'stack', u'sued', u'tax', u'tensions', u'told', u'trading',\n",
       "       u'troubled', u'warned', u'win', u'world'], \n",
       "      dtype='<U13')"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trim(s):\n",
    "    \"\"\"Trim string to fit on terminal (assuming 80-column display)\"\"\"\n",
    "    return s if len(s) < 80 else s[:75] + \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Benchmark classifiers\n",
    "def benchmark(clf):\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "#     print(X_train)\n",
    "#     print(y_train)\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "\n",
    "    if hasattr(clf, 'coef_'):\n",
    "        print(\"dimensionality: %d\" % clf.coef_.shape[1])\n",
    "        print(\"density: %f\" % density(clf.coef_))\n",
    "\n",
    "        print('qqq',clf.coef_)\n",
    "        \n",
    "#         if feature_names is not None:\n",
    "#             print(\"top 10 keywords per class:\")\n",
    "#             for i, category in enumerate(categories):\n",
    "#                 print('>>>',i, category)\n",
    "#                 print('www',np.argsort(clf.coef_[i]))\n",
    "#                 top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "#                 print(trim(\"%s: %s\"\n",
    "#                       % (category, \" \".join(feature_names[top10]))))\n",
    "#         print()\n",
    "\n",
    "#     print(\"classification report:\")\n",
    "#     print(metrics.classification_report(y_test, pred,\n",
    "#                                         target_names=categories))\n",
    "\n",
    "#     print(\"confusion matrix:\")\n",
    "#     print(metrics.confusion_matrix(y_test, pred))\n",
    "\n",
    "#     print()\n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    return clf_descr, score, train_time, test_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Perceptron\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
      "      n_iter=50, n_jobs=1, penalty=None, random_state=0, shuffle=True,\n",
      "      verbose=0, warm_start=False)\n",
      "train time: 0.015s\n",
      "test time:  0.000s\n",
      "accuracy:   0.444\n",
      "dimensionality: 100\n",
      "density: 1.000000\n",
      "qqq [[-0.5335122  -0.16511454  0.00374413  0.48295704 -0.1601605   0.13537801\n",
      "  -0.02309218 -0.55704241  0.08029599 -0.3415274  -0.04028362 -0.51794795\n",
      "   0.16182623 -0.18308933 -0.47915366  0.00272184 -0.44229246 -0.03284175\n",
      "   0.09967849 -0.13049082 -0.08857961 -0.59862664 -0.06061582 -0.26512979\n",
      "  -0.29895648 -0.23065064  0.10973617 -0.53932572 -0.1779774  -0.28218344\n",
      "  -0.17013736 -0.07778489 -0.04678277  0.03071914  0.01696396 -0.50072726\n",
      "  -0.25707113  0.14075555  0.22639308 -0.08603145  0.2474953  -0.20903327\n",
      "  -0.25046047 -0.33029966 -0.27934157  0.20697446 -0.30893246 -0.51634666\n",
      "   0.19834124 -0.03078869  0.28922023 -0.24178922 -0.48720787 -0.09278479\n",
      "   0.01430205  0.00515991  0.09630574 -0.17358165 -0.35031081 -0.44341692\n",
      "  -0.37480508 -0.32128729 -0.14539398  0.047395    0.02231356  0.22821498\n",
      "   0.05634859  0.1653762  -0.40196764 -0.11904994  0.13550692 -0.59837004\n",
      "  -0.08719124 -0.18268264 -0.45662964  0.02041625 -0.05599357 -0.28254726\n",
      "  -0.30629781 -0.26176554 -0.09628565  0.25667685 -0.29040098 -0.3455929\n",
      "  -0.1017149  -0.25781251 -0.52715729  0.0917351   0.2192905  -0.18858361\n",
      "  -0.60476668 -0.18159957 -0.15193851 -0.38017419 -0.09567245 -0.14982897\n",
      "  -0.09807826 -0.31042324  0.45786867  0.15961873]]\n",
      "================================================================================\n",
      "Passive-Aggressive\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "PassiveAggressiveClassifier(C=1.0, fit_intercept=True, loss='hinge',\n",
      "              n_iter=50, n_jobs=1, random_state=None, shuffle=True,\n",
      "              verbose=0, warm_start=False)\n",
      "train time: 0.012s\n",
      "test time:  0.000s\n",
      "accuracy:   0.644\n",
      "dimensionality: 100\n",
      "density: 1.000000\n",
      "qqq [[ -7.31873302  -8.54793191  -4.44018703   7.72362203  -5.40101573\n",
      "   -6.57059208  -6.74873933  -5.31183806  -3.26022989  -9.77712612\n",
      "   -7.00426813  -6.61280382   1.85752184  -7.47994893  -8.27010397\n",
      "   -0.92244849  -6.82742914  -7.22437391   0.27165795  -7.56460985\n",
      "   -4.95427164  -8.50351071  -5.65326231  -6.50968859 -10.45188146\n",
      "   -6.96674444   2.13499779  -5.54988621  -5.81569184  -2.88455546\n",
      "   -1.87710745  -4.25401892  -6.25040626  -6.17709987  -5.31989965\n",
      "   -6.68348744  -6.57488121  -4.86606718  -6.57049473  -6.32328207\n",
      "    2.49981395  -6.70415067  -8.07306881  -4.54649343  -6.18127468\n",
      "   -6.2583859   -7.22289357  -6.9642715    4.70926435   0.16859203\n",
      "    1.76054309  -6.49910653 -10.15265314  -9.01639149   2.49733723\n",
      "   -1.90172137   6.92759866  -7.54401891  -5.19644017  -8.54698713\n",
      "  -10.94366871  -7.84428445  -5.43184025  -6.28197941  -5.28192315\n",
      "   -6.47234651  -4.17986827  -7.06633323  -5.87089105   2.40219186\n",
      "   -4.76051162 -10.5297422   -7.2124307   -5.20750565  -6.38217834\n",
      "   -4.72559698  -7.56610639  -5.68743084  -7.68040113  -6.9101769\n",
      "   -6.03516744   2.07768176  -5.91550759  -4.99335967  -3.18929418\n",
      "   -5.39821788  -7.61385402  -4.84115332   2.20922588  -5.79227199\n",
      "   -5.48497166  -7.01461917  -6.36879243  -6.25352618  -6.89282255\n",
      "    0.43559293  -6.45901596  -6.13586139   2.82185256   2.48103153]]\n",
      "================================================================================\n",
      "kNN\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_neighbors=10, p=2, weights='uniform')\n",
      "train time: 0.001s\n",
      "test time:  0.094s\n",
      "accuracy:   0.645\n",
      "================================================================================\n",
      "Random forest\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "train time: 4.059s\n",
      "test time:  0.045s\n",
      "accuracy:   0.632\n",
      "================================================================================\n",
      "L2 penalty\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='l2', max_iter=1000, multi_class='ovr',\n",
      "     penalty='l2', random_state=None, tol=0.001, verbose=0)\n",
      "train time: 0.009s\n",
      "test time:  0.000s\n",
      "accuracy:   0.643\n",
      "dimensionality: 100\n",
      "density: 1.000000\n",
      "qqq [[-1.22986655 -2.8797962  -1.37997532  1.98329001 -1.69213103 -1.88584115\n",
      "  -1.62298298 -1.29652653 -1.33330799 -2.33932794 -2.27022985 -1.98069259\n",
      "   1.56307506 -1.99957265 -1.70066372 -1.10870363 -1.31444949 -1.76665247\n",
      "  -1.10141322 -1.91330641 -2.05569662 -1.90937253 -1.47014965 -1.64751456\n",
      "  -1.70414889 -1.82317901  1.60970192 -1.72182209 -1.43261379 -1.25061541\n",
      "  -1.35748834 -1.76401925 -1.7043403  -2.19192346 -1.47365121 -1.72462834\n",
      "  -1.41160892 -1.61662299 -2.87965559 -2.27845436  1.01955155 -1.87056861\n",
      "  -2.37987797 -1.7250652  -2.37257025 -1.8680382  -2.41723734 -1.96844352\n",
      "   1.53644854 -1.02002765  1.27558095 -1.67465736 -1.76885207 -2.30183206\n",
      "   1.15105238 -1.19498831  1.72090728 -2.42411818 -1.16877943 -2.65225109\n",
      "  -1.66177938 -1.95967368 -2.11227217 -1.80244051 -1.76580216 -1.92756889\n",
      "  -1.23920275 -2.00097864 -1.40933741  1.1251498  -1.5593961  -2.38920295\n",
      "  -2.03469734 -1.39210877 -1.18319196 -1.59892679 -1.83423257 -1.33781378\n",
      "  -1.6200614  -2.03662411 -2.04443999  1.33212855 -1.91981171 -1.28778148\n",
      "  -1.42792822 -1.80471603 -1.78013656 -1.58552673  1.6587165  -1.96582077\n",
      "  -1.5533382  -2.32527301 -2.15229023 -2.24807352 -2.37569127 -0.90752519\n",
      "  -1.82795793 -2.38797646  1.47958459  1.00428882]]\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=50, n_jobs=1,\n",
      "       penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "train time: 0.019s\n",
      "test time:  0.000s\n",
      "accuracy:   0.644\n",
      "dimensionality: 100\n",
      "density: 1.000000\n",
      "qqq [[ -1.44685942e+00  -4.10172158e+00  -1.43179655e+00   3.80643826e-01\n",
      "   -2.61661058e+00  -2.65229752e+00  -2.00854460e+00  -1.99198591e+00\n",
      "   -2.37324738e+00  -2.39721453e+00  -3.55715368e+00  -2.50250827e+00\n",
      "    9.28699269e-03  -3.78270112e+00  -2.01150354e+00  -2.26414376e+00\n",
      "   -1.87306387e+00  -2.70693594e+00  -2.68841281e+00  -3.07184832e+00\n",
      "   -4.14574590e+00  -1.90815604e+00  -1.90939198e+00  -2.11352296e+00\n",
      "   -1.95018804e+00  -2.72340238e+00   1.20942011e-02  -1.88602459e+00\n",
      "   -2.21177850e+00  -2.34612900e+00  -2.63135184e+00  -2.93340177e+00\n",
      "   -2.57165787e+00  -4.58396632e+00  -2.74201522e+00  -1.88327463e+00\n",
      "   -1.88344407e+00  -3.17511175e+00  -4.82617723e+00  -3.56240006e+00\n",
      "   -2.50811436e-03  -2.11952087e+00  -3.66522823e+00  -2.82456919e+00\n",
      "   -4.81862594e+00  -3.00067134e+00  -3.17773122e+00  -2.40379320e+00\n",
      "    5.12270187e-01  -1.93770050e+00   8.87941231e-03  -2.27868563e+00\n",
      "   -1.96903704e+00  -2.37676553e+00   1.65042690e-02  -2.35026310e+00\n",
      "    3.92204976e-01  -4.60897088e+00  -9.79222119e-01  -4.11033514e+00\n",
      "   -1.98559841e+00  -2.85985823e+00  -2.94801296e+00  -2.24409879e+00\n",
      "   -2.26489450e+00  -2.97463730e+00  -2.09904415e+00  -2.36218158e+00\n",
      "   -1.87724230e+00   1.10661783e-02  -3.48417629e+00  -3.03713979e+00\n",
      "   -3.09619165e+00  -2.64396261e+00  -2.08804392e+00  -2.73246426e+00\n",
      "   -2.40704090e+00  -2.02419509e+00  -2.21192182e+00  -2.33514964e+00\n",
      "   -3.55730893e+00   1.01118636e-02  -2.26444611e+00  -1.60415811e+00\n",
      "   -1.97167834e+00  -2.61349457e+00  -2.59073885e+00  -2.12659418e+00\n",
      "    1.14803089e-02  -2.61043793e+00  -1.96167618e+00  -2.82616748e+00\n",
      "   -4.79504685e+00  -2.75237714e+00  -6.17965062e+00  -1.07267761e+00\n",
      "   -2.14096486e+00  -3.67093065e+00   1.81687498e-02   2.54158270e-03]]\n",
      "================================================================================\n",
      "L1 penalty\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='l2', max_iter=1000, multi_class='ovr',\n",
      "     penalty='l1', random_state=None, tol=0.001, verbose=0)\n",
      "train time: 0.004s\n",
      "test time:  0.000s\n",
      "accuracy:   0.643\n",
      "dimensionality: 100\n",
      "density: 1.000000\n",
      "qqq [[-1.82619963 -4.07594293 -1.77401378  2.4001792  -2.11990254 -2.14433708\n",
      "  -2.01325133 -1.48151441 -1.39074163 -3.80028305 -2.92636616 -2.56747153\n",
      "   1.60662034 -2.26982168 -2.9391502  -1.0662005  -1.45521166 -2.44268569\n",
      "  -0.88879412 -2.26633662 -2.28441064 -3.41606268 -1.90681427 -2.19040799\n",
      "  -2.66797081 -2.29876702  1.70340905 -2.71118738 -1.72085498 -1.24831834\n",
      "  -1.37318304 -1.93397615 -2.02038761 -2.4974484  -1.52265264 -2.97243874\n",
      "  -2.00402628 -1.7485646  -3.62118326 -2.87395736  1.03289332 -2.71765268\n",
      "  -3.58181161 -2.00223006 -2.70718068 -2.14638335 -3.21495481 -3.09800793\n",
      "   1.68355602 -0.97572659  1.28097021 -2.0975463  -2.75488898 -3.80650315\n",
      "   1.09602217 -1.23029907  1.96427684 -2.77534078 -1.40085782 -3.72866026\n",
      "  -3.10903067 -2.49014317 -2.63381972 -2.28788127 -2.14305577 -2.22693762\n",
      "  -1.32795335 -2.71018737 -1.93313158  1.15892461 -1.61836105 -4.00032134\n",
      "  -2.39922702 -1.42573834 -1.13616679 -1.78884566 -2.6166148  -1.5367212\n",
      "  -2.81700987 -3.35560441 -2.33190774  1.34779205 -2.8357826  -1.55082737\n",
      "  -1.54774586 -1.92196751 -2.32312239 -1.90175318  1.88295213 -2.5936157\n",
      "  -2.11528393 -3.13153496 -2.38098709 -3.24463098 -2.58921615 -0.79793464\n",
      "  -2.99691978 -3.03400109  1.61664192  0.97957687]]\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=50, n_jobs=1,\n",
      "       penalty='l1', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "train time: 0.029s\n",
      "test time:  0.000s\n",
      "accuracy:   0.644\n",
      "dimensionality: 100\n",
      "density: 0.950000\n",
      "qqq [[ -5.98898906e-01  -8.79275132e+00  -1.34786615e+00   6.34496582e-03\n",
      "   -2.69122423e+00  -5.24579966e+00  -4.69587967e+00  -1.85574924e+00\n",
      "   -3.29957541e+00  -6.68863886e+00  -6.60923036e+00  -5.06088047e+00\n",
      "    3.73076484e-03  -7.24512381e+00  -3.49674323e+00  -1.86230213e+00\n",
      "   -2.89014508e+00  -6.30750098e+00  -2.60477411e+00  -5.10237377e+00\n",
      "   -6.37789881e+00  -3.64122369e+00  -2.02626401e+00  -4.59526337e+00\n",
      "   -3.51212260e+00  -4.21841696e+00   6.83631498e-03  -5.01554241e+00\n",
      "   -3.82605928e+00  -3.10025657e+00  -1.95302313e+00  -5.15968690e+00\n",
      "   -3.13892916e+00  -7.10318292e+00  -3.95228290e+00  -4.68369355e+00\n",
      "   -2.50907135e+00  -4.41621852e+00  -7.56320426e+00  -7.16908371e+00\n",
      "    0.00000000e+00  -4.10729025e+00  -7.62508192e+00  -4.48478668e+00\n",
      "   -7.18001021e+00  -5.81463531e+00  -7.06454296e+00  -5.84720565e+00\n",
      "    1.73390896e-03  -9.87377772e-03   2.08662735e-04  -5.59792037e+00\n",
      "   -4.35501212e+00  -5.93697856e+00   5.69960721e-03  -2.79395817e+00\n",
      "    3.86042167e-03  -8.07664950e+00   0.00000000e+00  -6.54782913e+00\n",
      "   -3.37872503e+00  -7.56148915e+00  -4.79523244e+00  -5.62591338e+00\n",
      "   -4.89805346e+00  -5.74343306e+00  -4.58511120e+00  -4.94550922e+00\n",
      "   -3.56106399e+00   6.84549998e-04  -5.08374563e+00  -8.00523674e+00\n",
      "   -6.87800342e+00  -3.74958007e+00  -2.06712571e+00  -4.10868542e+00\n",
      "   -6.15619908e+00  -1.93833097e+00  -2.46847097e+00  -6.16698110e+00\n",
      "   -6.37191167e+00   0.00000000e+00  -4.58315049e+00  -1.30905330e+00\n",
      "   -1.71921414e+00  -4.26555612e+00  -4.03969008e+00  -4.85513984e+00\n",
      "    5.42357037e-03  -5.48228828e+00  -4.05180353e+00  -6.75447114e+00\n",
      "   -6.16818216e+00  -6.28576103e+00  -7.82920526e+00   0.00000000e+00\n",
      "   -4.23930163e+00  -7.38652343e+00   1.13425200e-02   0.00000000e+00]]\n",
      "================================================================================\n",
      "Elastic-Net penalty\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=50, n_jobs=1,\n",
      "       penalty='elasticnet', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "train time: 0.031s\n",
      "test time:  0.000s\n",
      "accuracy:   0.649\n",
      "dimensionality: 100\n",
      "density: 0.990000\n",
      "qqq [[ -1.26405414e+00  -4.60772204e+00  -1.49978177e+00   2.82752006e-01\n",
      "   -2.76641960e+00  -2.92593806e+00  -2.17862209e+00  -1.91790670e+00\n",
      "   -2.53492934e+00  -2.63321007e+00  -3.58754067e+00  -2.75627905e+00\n",
      "    8.27468133e-03  -4.07189639e+00  -2.01913428e+00  -2.30922439e+00\n",
      "   -1.90536438e+00  -2.75227437e+00  -2.79932345e+00  -3.33818752e+00\n",
      "   -4.20524702e+00  -2.04981857e+00  -2.05668517e+00  -2.18479629e+00\n",
      "   -2.10575972e+00  -2.87008000e+00   1.01373406e-02  -2.04485057e+00\n",
      "   -2.40275029e+00  -2.55108400e+00  -2.57989699e+00  -3.26653941e+00\n",
      "   -2.82688468e+00  -4.74155966e+00  -2.71448259e+00  -2.02825981e+00\n",
      "   -2.02663707e+00  -3.35500265e+00  -5.29928680e+00  -3.80793643e+00\n",
      "    3.91483231e-03  -2.30681833e+00  -3.78841606e+00  -3.12679813e+00\n",
      "   -4.90756138e+00  -3.28768540e+00  -3.54580570e+00  -2.64011275e+00\n",
      "    4.09576536e-01  -1.90704903e+00   1.41490941e-02  -2.48993342e+00\n",
      "   -2.13090603e+00  -2.60992561e+00   0.00000000e+00  -2.23848285e+00\n",
      "    3.02230589e-01  -4.90312588e+00  -9.70337233e-01  -4.32810273e+00\n",
      "   -1.90722586e+00  -2.86889174e+00  -3.27225496e+00  -2.44431211e+00\n",
      "   -2.47637043e+00  -3.30095257e+00  -2.19132059e+00  -2.59095669e+00\n",
      "   -2.02882047e+00   1.01745456e-02  -3.61175950e+00  -3.15021276e+00\n",
      "   -3.39494315e+00  -2.68074592e+00  -2.14923240e+00  -2.77368320e+00\n",
      "   -2.42841045e+00  -2.01485639e+00  -2.11031910e+00  -2.55918810e+00\n",
      "   -3.80899501e+00   3.60406193e-03  -2.47831700e+00  -1.70479090e+00\n",
      "   -1.91807401e+00  -2.55057316e+00  -2.85967775e+00  -2.14432826e+00\n",
      "    4.75301496e-03  -2.82099671e+00  -2.12500546e+00  -3.14051968e+00\n",
      "   -5.04970939e+00  -3.05035551e+00  -6.50892941e+00  -9.80592262e-01\n",
      "   -2.33085784e+00  -3.86138684e+00   8.64806120e-03   2.74736001e-03]]\n",
      "================================================================================\n",
      "NearestCentroid (aka Rocchio classifier)\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "NearestCentroid(metric='euclidean', shrink_threshold=None)\n",
      "train time: 0.005s\n",
      "test time:  0.000s\n",
      "accuracy:   0.647\n",
      "================================================================================\n",
      "Naive Bayes\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "train time: 0.003s\n",
      "test time:  0.000s\n",
      "accuracy:   0.638\n",
      "dimensionality: 100\n",
      "density: 1.000000\n",
      "qqq [[-10.23692027  -7.09109556  -6.81809714  -4.15863724  -6.94871049\n",
      "   -5.51211214  -7.0097165  -10.23692027  -3.96972139 -10.23692027\n",
      "   -6.11351055  -6.30008148  -2.97439395  -4.88786165 -10.23692027\n",
      "   -3.86340352  -5.63162334  -6.99072096  -5.02865929  -5.89485979\n",
      "   -5.2979658  -10.23692027  -6.83214576  -6.99174091 -10.23692027\n",
      "   -7.07349394  -3.28667159 -10.23692027  -5.91789392  -4.75157221\n",
      "   -4.26221159  -5.25105629  -5.88426822  -5.3327699   -4.84190553\n",
      "  -10.23692027 -10.23692027  -4.64876526  -6.01913989  -6.03813683\n",
      "   -1.95924853  -7.21284861  -7.08526538  -5.9381538   -4.83299259\n",
      "   -4.9085527   -6.02296458 -10.23692027  -3.78983244  -3.2956474\n",
      "   -3.55971837  -6.70032274 -10.23692027  -7.13706005  -2.95768755\n",
      "   -3.72853609  -3.81734764  -5.35052011  -6.28386612 -10.23692027\n",
      "  -10.23692027  -5.45284199  -6.38528296  -5.80608756  -5.59433991\n",
      "   -4.82707115  -5.90976595  -6.86480495 -10.23692027  -2.59760249\n",
      "   -4.32538668 -10.23692027  -4.73671002  -4.10770244  -6.43599849\n",
      "   -5.28559379  -6.48433391  -6.42967831 -10.23692027 -10.23692027\n",
      "   -4.75905966  -3.22595843 -10.23692027 -10.23692027  -5.34873335\n",
      "  -10.23692027 -10.23692027  -5.42051862  -3.78605625  -6.82451646\n",
      "  -10.23692027  -6.39608928  -4.64215256 -10.23692027  -3.69297648\n",
      "   -3.81176102  -7.2338768   -6.18054389  -3.90072318  -2.45073446]]\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "BernoulliNB(alpha=0.01, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "train time: 0.003s\n",
      "test time:  0.000s\n",
      "accuracy:   0.644\n",
      "dimensionality: 100\n",
      "density: 1.000000\n",
      "qqq [[-13.02585688  -8.41073637  -8.41073637  -5.47572154  -8.41073637\n",
      "   -6.80925078  -8.41073637 -13.02585688  -5.28475779 -13.02585688\n",
      "   -7.31874662  -7.72255197  -4.20103194  -6.1171021  -13.02585688\n",
      "   -5.08812511  -7.03189545  -8.41073637  -6.62726195  -7.03189545\n",
      "   -6.62726195 -13.02585688  -8.41073637  -8.41073637 -13.02585688\n",
      "   -8.41073637  -4.57032635 -13.02585688  -7.31874662  -6.02188275\n",
      "   -5.64747317  -6.473349    -7.31874662  -6.80925078  -6.22235162\n",
      "  -13.02585688 -13.02585688  -6.02188275  -7.31874662  -7.31874662\n",
      "   -2.92347749  -8.41073637  -8.41073637  -7.72255197  -6.22235162\n",
      "   -6.22235162  -7.31874662 -13.02585688  -5.28475779  -4.39515645\n",
      "   -5.08812511  -8.41073637 -13.02585688  -8.41073637  -4.10306526\n",
      "   -4.95463834  -5.3291898   -6.62726195  -7.72255197 -13.02585688\n",
      "  -13.02585688  -6.62726195  -7.72255197  -7.31874662  -6.80925078\n",
      "   -6.22235162  -7.72255197  -8.41073637 -13.02585688  -3.76663111\n",
      "   -5.71197005 -13.02585688  -5.8549684   -5.42445455  -7.72255197\n",
      "   -6.473349    -7.72255197  -7.72255197 -13.02585688 -13.02585688\n",
      "   -6.02188275  -4.73155727 -13.02585688 -13.02585688  -6.473349\n",
      "  -13.02585688 -13.02585688  -6.80925078  -5.16220562  -8.41073637\n",
      "  -13.02585688  -7.72255197  -5.93494706 -13.02585688  -4.83689002\n",
      "   -5.08812511  -8.41073637  -7.72255197  -5.47572154  -3.50795828]]\n",
      "================================================================================\n",
      "LinearSVC with L1-based feature selection\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "Pipeline(steps=[('feature_selection', LinearSVC(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l1', random_state=None, tol=0.001,\n",
      "     verbose=0)), ('classification', LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0))])\n",
      "train time: 0.028s\n",
      "test time:  0.002s\n",
      "accuracy:   0.643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/antonio/.virtualenvs/lexisnexis/local/lib/python2.7/site-packages/sklearn/svm/classes.py:192: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
      "  DeprecationWarning)\n",
      "/home/antonio/.virtualenvs/lexisnexis/local/lib/python2.7/site-packages/sklearn/svm/classes.py:192: DeprecationWarning: loss='l2' has been deprecated in favor of loss='squared_hinge' as of 0.16. Backward compatibility for the loss='l2' will be removed in 1.0\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for clf, name in (\n",
    "        #(RidgeClassifier(tol=1e-2, solver=\"lsqr\"), \"Ridge Classifier\"),\n",
    "        (Perceptron(n_iter=50), \"Perceptron\"),\n",
    "        (PassiveAggressiveClassifier(n_iter=50), \"Passive-Aggressive\"),\n",
    "        (KNeighborsClassifier(n_neighbors=10), \"kNN\"),\n",
    "        (RandomForestClassifier(n_estimators=100), \"Random forest\")\n",
    "    ):\n",
    "    print('=' * 80)\n",
    "    print(name)\n",
    "    results.append(benchmark(clf))\n",
    "\n",
    "for penalty in [\"l2\", \"l1\"]:\n",
    "    print('=' * 80)\n",
    "    print(\"%s penalty\" % penalty.upper())\n",
    "    # Train Liblinear model\n",
    "    results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,\n",
    "                                            dual=False, tol=1e-3)))\n",
    "\n",
    "    # Train SGD model\n",
    "    results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,\n",
    "                                           penalty=penalty)))\n",
    "\n",
    "# Train SGD with Elastic Net penalty\n",
    "print('=' * 80)\n",
    "print(\"Elastic-Net penalty\")\n",
    "results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,\n",
    "                                       penalty=\"elasticnet\")))\n",
    "\n",
    "# Train NearestCentroid without threshold\n",
    "print('=' * 80)\n",
    "print(\"NearestCentroid (aka Rocchio classifier)\")\n",
    "results.append(benchmark(NearestCentroid()))\n",
    "\n",
    "# Train sparse Naive Bayes classifiers\n",
    "print('=' * 80)\n",
    "print(\"Naive Bayes\")\n",
    "results.append(benchmark(MultinomialNB(alpha=.01)))\n",
    "results.append(benchmark(BernoulliNB(alpha=.01)))\n",
    "\n",
    "print('=' * 80)\n",
    "print(\"LinearSVC with L1-based feature selection\")\n",
    "# The smaller C, the stronger the regularization.\n",
    "# The more regularization, the more sparsity.\n",
    "results.append(benchmark(Pipeline([\n",
    "  ('feature_selection', LinearSVC(penalty=\"l1\", dual=False, tol=1e-3)),\n",
    "  ('classification', LinearSVC())\n",
    "])))\n",
    "\n",
    "# make some plots\n",
    "\n",
    "indices = np.arange(len(results))\n",
    "\n",
    "results = [[x[i] for x in results] for i in range(4)]\n",
    "\n",
    "clf_names, score, training_time, test_time = results\n",
    "training_time = np.array(training_time) / np.max(training_time)\n",
    "test_time = np.array(test_time) / np.max(test_time)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(\"Score\")\n",
    "plt.barh(indices, score, .2, label=\"score\", color='r')\n",
    "plt.barh(indices + .3, training_time, .2, label=\"training time\", color='g')\n",
    "plt.barh(indices + .6, test_time, .2, label=\"test time\", color='b')\n",
    "plt.yticks(())\n",
    "plt.legend(loc='best')\n",
    "plt.subplots_adjust(left=.25)\n",
    "plt.subplots_adjust(top=.95)\n",
    "plt.subplots_adjust(bottom=.05)\n",
    "\n",
    "for i, c in zip(indices, clf_names):\n",
    "    plt.text(-.3, i, c)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
